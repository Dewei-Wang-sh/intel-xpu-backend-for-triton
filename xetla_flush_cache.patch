commit 361c42ffa2dd8c849f33b28f1028532cc9451e98
Author: Ling, Liyang <liyang.ling@intel.com>
Date:   Mon May 20 23:33:23 2024 -0700

    Flush L2 cache

diff --git a/tests/integration/gemm/bf16_stream_k/main.cpp b/tests/integration/gemm/bf16_stream_k/main.cpp
index b9f19201..c164756c 100644
--- a/tests/integration/gemm/bf16_stream_k/main.cpp
+++ b/tests/integration/gemm/bf16_stream_k/main.cpp
@@ -21,6 +21,8 @@ using namespace gpu::xetla;
 //The number of times the kernel is executed
 constexpr int ITER = 1;
 
+#define CACHE_FLUSH 1
+
 template <typename data_type_a, typename data_type_b, typename data_type_c,
         typename data_type_d, typename data_type_acc = float>
 int gemm_result_validate(data_type_a *A_device, data_type_b *B_device,
@@ -351,6 +353,19 @@ void stream_k_gemm_run(uint32_t iter) {
     long ops = 2 * static_cast<long>(matrix_m) * matrix_n * matrix_k;
     profiling_helper prof("stream_k_universal_gemm", ops, "gflops");
 
+#ifdef CACHE_FLUSH
+        auto size = 256*1024*1024;
+        auto host_ptr = static_cast<int8_t *>(
+                malloc(size * sizeof(int8_t)));
+
+        for (size_t i = 0; i < size; ++i) {
+            host_ptr[i] = 0;
+        }
+        auto device_ptr = static_cast<int8_t *>(
+                aligned_alloc_device(DEVICE_MEM_ALIGNMENT,
+                        size * sizeof(int8_t), device, context));
+#endif
+
     if constexpr (postop_enable) {
 
         // [ReLuBias] define the shape of matrix bias, which should be identitcal to C
@@ -378,6 +393,9 @@ void stream_k_gemm_run(uint32_t iter) {
         }
 
         for (uint32_t i = 0; i < iter + warmup; i++) {
+#ifdef CACHE_FLUSH
+            queue.memset((void *)(device_ptr), 0, size * sizeof(int8_t)).wait();
+#endif
             if (i >= warmup) { prof.cpu_start(); }
             auto gpu_event = queue.submit([&](handler &cgh) {
                 // GPU kernel
@@ -414,6 +432,9 @@ void stream_k_gemm_run(uint32_t iter) {
         cl::sycl::nd_range<3> NDRange = gemm_op_t::get_nd_range(gemm_arg);
 
         for (uint32_t i = 0; i < iter + warmup; i++) {
+#ifdef CACHE_FLUSH
+            queue.memset((void *)(device_ptr), 0, size * sizeof(int8_t)).wait();
+#endif
             if (i >= warmup) { prof.cpu_start(); }
             auto gpu_event = queue.submit([&](handler &cgh) {
                 // GPU kernel
@@ -432,6 +453,9 @@ void stream_k_gemm_run(uint32_t iter) {
             }
         }
     }
+#ifdef CACHE_FLUSH
+        free(host_ptr);
+#endif
 
     unsetenv("SYCL_PROGRAM_COMPILE_OPTIONS");
 
diff --git a/tests/utils/execution.hpp b/tests/utils/execution.hpp
index 260b503a..dc306e8c 100644
--- a/tests/utils/execution.hpp
+++ b/tests/utils/execution.hpp
@@ -75,11 +75,7 @@ void gemm_exec(const std::string &compile_str, size_t batch = 1) {
     using data_type_acc = typename Test::data_type_acc;
 
     int iter = 10, warmup = 10;
-#ifdef CACHE_FLUSH
-    batch = iter + warmup;
-#else
     batch = 1;
-#endif
     constexpr size_t matrix_m = Test::mat_m;
     constexpr size_t matrix_n = Test::mat_n;
     constexpr size_t matrix_k = Test::mat_k;
@@ -161,28 +157,33 @@ void gemm_exec(const std::string &compile_str, size_t batch = 1) {
         // }
         cl::sycl::nd_range<3> nd_range = gemm_op_t::get_nd_range(arg);
 
+#ifdef CACHE_FLUSH
+        auto size = 256*1024*1024;
+        auto host_ptr = static_cast<int8_t *>(
+                malloc(size * sizeof(int8_t)));
+
+        for (size_t i = 0; i < size; ++i) {
+            host_ptr[i] = 0;
+        }
+        auto device_ptr = static_cast<int8_t *>(
+                aligned_alloc_device(DEVICE_MEM_ALIGNMENT,
+                        size * sizeof(int8_t), device, context));
+#endif
+
         std::vector<float> event_times(iter + warmup);
         for (uint32_t j = 0; j < iter + warmup; j++) {
-
+#ifdef CACHE_FLUSH
+            queue.memset((void *)(device_ptr), 0, size * sizeof(int8_t)).wait();
+#endif
             auto e_esimd = queue.submit([&](handler &cgh) {
                 cgh.use_kernel_bundle(exeBundle);
                 cgh.parallel_for<Test>(
                         nd_range, [=](nd_item<3> item) KERNEL_MAIN {
-                // int batch_idx = item.get_workgroup(0);
-#ifdef CACHE_FLUSH
-                            int batch_idx = j;
-                            auto A_ptr = A + batch_idx * size_a;
-                            auto B_ptr = B + batch_idx * size_b;
-                            auto C_ptr = C + batch_idx * size_c;
-                            auto Acc_ptr = Acc + batch_idx * size_acc;
-                            auto Cnt_ptr = Cnt + batch_idx * size_cnt;
-#else
                             auto A_ptr = A;
                             auto B_ptr = B;
                             auto C_ptr = C;
                             auto Acc_ptr = Acc;
                             auto Cnt_ptr = Cnt;
-#endif
                             gpu::xetla::xetla_local_init<SLMSIZE>();
                             gpu::xetla::xetla_nbarrier_init<BARNUM>();
                             KERNEL::run(item, A_ptr, B_ptr, C_ptr, matrix_m,
@@ -193,6 +194,9 @@ void gemm_exec(const std::string &compile_str, size_t batch = 1) {
             e_esimd.wait();
             event_times[j] = time_event(e_esimd) / 1e9;
         }
+#ifdef CACHE_FLUSH
+        free(host_ptr);
+#endif
         auto best = 999.f;
         auto worst = 0.f;
         double average = 0.f;
