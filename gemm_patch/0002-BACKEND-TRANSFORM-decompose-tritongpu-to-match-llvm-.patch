From 068d20657ab6bf0e648149be3f5cfab573f00a5f Mon Sep 17 00:00:00 2001
From: Dewei Wang <dewei.wang@intel.com>
Date: Sun, 28 Jan 2024 23:41:48 -0800
Subject: [PATCH] [BACKEND][TRANSFORM] decompose tritongpu to match llvm/spirv
 target size

---
 .../Dialect/TritonGPU/Transforms/Passes.h     |   2 +
 .../Dialect/TritonGPU/Transforms/Passes.td    |  20 +
 .../TritonGPU/Transforms/CMakeLists.txt       |   1 +
 .../TritonGPU/Transforms/MatchTargetSize.cpp  | 489 ++++++++++++++++++
 test/TritonGPU/match-target-size.mlir         |  64 +++
 test/TritonGPU/match-target-size.output.mlir  | 186 +++++++
 6 files changed, 762 insertions(+)
 create mode 100644 lib/Dialect/TritonGPU/Transforms/MatchTargetSize.cpp
 create mode 100644 test/TritonGPU/match-target-size.mlir
 create mode 100644 test/TritonGPU/match-target-size.output.mlir

diff --git a/include/triton/Dialect/TritonGPU/Transforms/Passes.h b/include/triton/Dialect/TritonGPU/Transforms/Passes.h
index 2d745b8d3..129496b67 100644
--- a/include/triton/Dialect/TritonGPU/Transforms/Passes.h
+++ b/include/triton/Dialect/TritonGPU/Transforms/Passes.h
@@ -20,6 +20,8 @@ std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();
 
 std::unique_ptr<Pass> createTritonGPUDistributeToWarpsPass();
 
+std::unique_ptr<Pass> createTritonGPUMatchTargetSizePass();
+
 std::unique_ptr<Pass> createTritonGPUCoalescePass();
 
 std::unique_ptr<Pass> createTritonGPUReorderInstructionsPass();
diff --git a/include/triton/Dialect/TritonGPU/Transforms/Passes.td b/include/triton/Dialect/TritonGPU/Transforms/Passes.td
index c552c427e..0b8a055b1 100644
--- a/include/triton/Dialect/TritonGPU/Transforms/Passes.td
+++ b/include/triton/Dialect/TritonGPU/Transforms/Passes.td
@@ -64,6 +64,26 @@ def TritonGPUDistributeToWarps : Pass<"tritongpu-distribute-to-warps", "mlir::Mo
                            "mlir::arith::ArithDialect"];
 }
 
+def TritonGPUMatchTargetSize : Pass<"tritongpu-match-target-size", "mlir::ModuleOp"> {
+  let summary = "match the target size of specific op (dot, load, store)";
+
+  let description = [{
+    this pass should be run after tritongpu-distribute-to-warps
+  }];
+
+  let constructor = "mlir::createTritonGPUMatchTargetSizePass()";
+
+  let dependentDialects = ["mlir::triton::TritonDialect",
+                           "mlir::triton::gpu::TritonGPUDialect"];
+
+  let options = [
+    ListOption<"dotSize", "dot-size",
+           "int64_t", "target LLVM IR dot operation size">
+    // fixme : add operate-size load-size, store-size
+    // load-operate-store-size 256DW which is 16x16xf32, 32x16xf16
+  ];
+}
+
 def TritonGPUAccelerateMatmul : Pass<"tritongpu-accelerate-matmul", "mlir::ModuleOp"> {
   let summary = "accelerate matmul";
 
diff --git a/lib/Dialect/TritonGPU/Transforms/CMakeLists.txt b/lib/Dialect/TritonGPU/Transforms/CMakeLists.txt
index d34eb9b56..30416a00e 100644
--- a/lib/Dialect/TritonGPU/Transforms/CMakeLists.txt
+++ b/lib/Dialect/TritonGPU/Transforms/CMakeLists.txt
@@ -3,6 +3,7 @@ add_mlir_dialect_library(TritonGPUTransforms
   Coalesce.cpp
   DecomposeConversions.cpp
   DistributeToWarps.cpp
+  MatchTargetSize.cpp
   OptimizeDotOperands.cpp
   OptimizeEpilogue.cpp
   OptimizeThreadLocality.cpp
diff --git a/lib/Dialect/TritonGPU/Transforms/MatchTargetSize.cpp b/lib/Dialect/TritonGPU/Transforms/MatchTargetSize.cpp
new file mode 100644
index 000000000..3c3bf82f6
--- /dev/null
+++ b/lib/Dialect/TritonGPU/Transforms/MatchTargetSize.cpp
@@ -0,0 +1,489 @@
+#include "mlir/Dialect/Arith/IR/Arith.h"
+#include "mlir/Dialect/SCF/IR/SCF.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/IR/TypeUtilities.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "triton/Analysis/Utility.h"
+#include "triton/Dialect/Triton/IR/Dialect.h"
+#include "triton/Dialect/TritonGPU/IR/Dialect.h"
+#include "triton/Dialect/TritonGPU/Transforms/Passes.h"
+#include "triton/Dialect/TritonGPU/Transforms/Utility.h"
+#include "triton/Tools/Sys/GetEnv.hpp"
+#include "llvm/Support/Debug.h"
+#include <memory>
+
+using namespace mlir;
+namespace tt = mlir::triton;
+namespace ttg = mlir::triton::gpu;
+
+#define GEN_PASS_CLASSES
+#include "triton/Dialect/TritonGPU/Transforms/Passes.h.inc"
+
+class TritonGPUMatchTargetSizePass
+    : public TritonGPUMatchTargetSizeBase<TritonGPUMatchTargetSizePass> {
+public:
+  void runOnOperation() override {
+    auto *ctx = &getContext();
+    ModuleOp m = getOperation();
+    sizePerAttrMap.clear();
+    if (!dotSize.hasValue()) {
+      dotSize.addValue(8);
+      dotSize.addValue(16);
+      dotSize.addValue(16);
+    }
+    /// split op to match target llvm/spirv size
+    // try to use callback to handle different cases
+    m.walk<WalkOrder::PreOrder>([&](Operation *op) {
+      SmallVector<Type> types(op->getOperandTypes().begin(),
+                              op->getOperandTypes().end());
+      SmallVector<Type> resultTypes(op->getResultTypes().begin(),
+                                    op->getResultTypes().end());
+      types.append(resultTypes);
+      if (!llvm::any_of(types, [&](Type type) {
+            if (isa<RankedTensorType>(type)) {
+              return true;
+            } else if (auto ptrType = dyn_cast<tt::PointerType>(type)) {
+              auto pointeeType = ptrType.getPointeeType();
+              return isa<RankedTensorType>(pointeeType) ? true : false;
+            } else {
+              return false;
+            }
+          }))
+        ;
+      else if (isa<scf::ForOp, scf::YieldOp>(op))
+        ;
+      else if (auto cstOp = dyn_cast<arith::ConstantOp>(op)) {
+        recordRootSubSize(op->getResultTypes()[0]);
+        transformArithConstantOp(cstOp);
+      } else if (auto dot = dyn_cast<tt::DotOp>(op))
+        transformDotOp(dot);
+      else if (auto ptrOp = dyn_cast<tt::MakeTensorPtrOp>(op)) {
+        recordRootSubSize(op->getResultTypes()[0]);
+        transformMakeTensorPtrOp(ptrOp);
+      }
+      // arith,math,tt.advance,tt.load,tt.store,tt.prefetch
+      else
+        transformGenericOp(op);
+      return WalkResult::advance();
+    });
+
+    m.dump();
+    /// canonicalize ops(remove redundant tt.extract, tt.glue)
+    RewritePatternSet patterns(ctx);
+    patterns.add<ScfPattern>(ctx);
+    patterns.add<ExtractPattern>(ctx);
+    patterns.add<ArithRemPattern>(ctx);
+    if (failed(applyPatternsAndFoldGreedily(m, std::move(patterns))))
+      signalPassFailure();
+  }
+
+private:
+  DenseMap<Attribute, SmallVector<int64_t>> sizePerAttrMap;
+  void recordRootSubSize(Type type) {
+    if (auto tensorType = dyn_cast<RankedTensorType>(type)) {
+      auto layout = tensorType.getEncoding();
+      if (sizePerAttrMap.count(layout) == 0)
+        sizePerAttrMap[layout] = getSubOpSize(tensorType);
+    } else if (auto ptrType = dyn_cast<tt::PointerType>(type)) {
+      recordRootSubSize(ptrType.getPointeeType());
+    }
+  }
+  // assume
+  // all lsc size max is 256 DW
+  // lsc 2d limit is 32
+  //  bool isLsc = false)
+  SmallVector<int64_t> getSubOpSize(RankedTensorType type) {
+    auto mStep = dotSize[0];
+    auto nStep = dotSize[1];
+    auto kStep = dotSize[2];
+    auto layout = type.getEncoding();
+    int64_t colLimit = 0;
+    if (auto warpAttr = dyn_cast<ttg::WarpEncodingAttr>(layout)) {
+      colLimit = 32;
+      colLimit = 16;
+      // if (warpAttr.getIsDotC())
+      if (warpAttr.getSizePerThread()[1] == 64)
+        colLimit = nStep;
+    } else if (auto dotAttr = dyn_cast<ttg::DotOperandEncodingAttr>(layout)) {
+      colLimit = dotAttr.getOpIdx() == 0 ? kStep : nStep;
+    }
+    auto shape = type.getShape();
+    SmallVector<int64_t> subSize(shape.size());
+    auto sizeInByte = type.getElementTypeBitWidth() / 8;
+    if (shape.size() == 2) {
+      subSize[1] = (shape[1] > colLimit) ? colLimit : shape[1];
+      auto max = 256 * 4 / sizeInByte / subSize[1];
+      subSize[0] = std::min(max, shape[0]);
+    } else if (shape.size() == 1) {
+      int64_t max = 256 * 4 / sizeInByte;
+      subSize[0] = std::min(max, shape[0]);
+    }
+    return subSize;
+  }
+
+  std::tuple<SmallVector<int64_t>, Type, SmallVector<int64_t>>
+  getSubTypeAndShape(Type type) {
+    if (auto tensorType = dyn_cast<RankedTensorType>(type)) {
+      auto shape = llvm::to_vector(tensorType.getShape());
+      auto attr = tensorType.getEncoding();
+      auto subSize = sizePerAttrMap[attr];
+      auto subType = RankedTensorType::get(
+          subSize, tensorType.getElementType() /*no encoding*/);
+      return std::make_tuple(shape, subType, subSize);
+    } else if (auto ptrType = dyn_cast<tt::PointerType>(type)) {
+      auto pointeeType = ptrType.getPointeeType();
+      auto [shape, subType, subSize] = getSubTypeAndShape(pointeeType);
+      auto newType = tt::PointerType::get(subType, ptrType.getAddressSpace());
+      return std::make_tuple(shape, newType, subSize);
+    }
+    return {{0}, type, {0}};
+  }
+
+  void transformMakeTensorPtrOp(tt::MakeTensorPtrOp op) {
+    auto type = op.getType();
+    auto [shape, subType, subSize] = getSubTypeAndShape(type);
+    // // early return
+    // if (shape == subSize)
+    //   return;
+    auto dim = shape.size();
+    OpBuilder b(op);
+    auto loc = op.getLoc();
+    auto idx = 0;
+    SmallVector<Value> subOps;
+    for (auto i = 0; i < shape[dim - 1]; i += subSize[dim - 1]) {
+      if (dim == 2) {
+        for (auto j = 0; j < shape[0]; j += subSize[0]) {
+          auto offsets = op.getOffsets();
+          // auto newOffsets = offsets += [i, j]
+          SmallVector<Value> newOffsets(2);
+          if (j == 0)
+            newOffsets[0] = offsets[0];
+          else {
+            auto step = b.create<arith::ConstantIntOp>(loc, j, 32);
+            newOffsets[0] = b.create<arith::AddIOp>(loc, offsets[0], step);
+          }
+          if (i == 0)
+            newOffsets[1] = offsets[1];
+          else {
+            auto step = b.create<arith::ConstantIntOp>(loc, i, 32);
+            newOffsets[1] = b.create<arith::AddIOp>(loc, offsets[1], step);
+          }
+          SmallVector<int32_t> subShape;
+          for (auto sub : subSize)
+            subShape.push_back(sub);
+          auto subOp = b.create<tt::MakeTensorPtrOp>(
+              loc, op.getBase(), op.getShape(), op.getStrides(), newOffsets,
+              subShape, op.getOrder());
+          subOps.push_back(subOp);
+        }
+      }
+    }
+    auto glue = b.create<tt::GlueOp>(loc, type, subOps);
+    op->replaceAllUsesWith(glue->getResults());
+    op->erase();
+    return;
+  }
+
+  void transformArithConstantOp(arith::ConstantOp op) {
+    auto type = cast<RankedTensorType>(op.getResult().getType());
+    auto [shape, subType, subSize] = getSubTypeAndShape(type);
+    // // early return
+    // if (shape == subSize)
+    //   return;
+    auto dim = shape.size();
+    OpBuilder b(op);
+    auto loc = op.getLoc();
+    auto idx = 0;
+    // these 2 lines are specific
+    auto value = cast<DenseElementsAttr>(op.getValue());
+    value = value.resizeSplat(subType.cast<ShapedType>());
+    SmallVector<Value> subOps;
+    for (auto i = 0; i < shape[dim - 1]; i += subSize[dim - 1]) {
+      if (dim == 2) {
+        for (auto j = 0; j < shape[0]; j += subSize[0]) {
+          auto subOp = b.create<arith::ConstantOp>(loc, subType, value);
+          subOps.push_back(subOp);
+        }
+      }
+    }
+    auto glue = b.create<tt::GlueOp>(loc, type, subOps);
+    op->replaceAllUsesWith(glue->getResults());
+    op->erase();
+    return;
+  }
+
+  void transformGenericOp(Operation *op) {
+    auto numResults = op->getResults().size();
+    Type type;
+    // prefetch/store
+    if (numResults == 0)
+      type = op->getOperand(0).getType();
+    // arith/math/advanceOp
+    else {
+      assert(numResults == 1);
+      type = op->getResultTypes()[0];
+    }
+    auto [shape, subType, subSize] = getSubTypeAndShape(type);
+    // // early return
+    // if (shape == subSize)
+    //   return;
+    auto dim = shape.size();
+    OpBuilder b(op);
+    auto loc = op->getLoc();
+    auto idx = 0;
+    SmallVector<Value> subOps;
+    for (auto i = 0; i < shape[dim - 1]; i += subSize[dim - 1]) {
+      if (dim == 2) {
+        for (auto j = 0; j < shape[0]; j += subSize[0]) {
+          SmallVector<Value> newOperands;
+          llvm::transform(op->getOperands(), std::back_inserter(newOperands),
+                          [&](Value operand) {
+                            auto type = operand.getType();
+                            if (isa<tt::PointerType, RankedTensorType>(type)) {
+                              auto subOpndType =
+                                  std::get<1>(getSubTypeAndShape(type));
+                              Value newOp = b.create<tt::ExtractOp>(
+                                  loc, subOpndType, operand, idx);
+                              return newOp;
+                            } else
+                              return operand;
+                          });
+          // auto subOp = b.create<op->getName().getTypeID()>(loc, newOperands,
+          //                                                  op->getAttrs());
+          // subOps.push_back(subOp);
+          Operation *subOp;
+          if (numResults == 0)
+            subOp = b.create(loc, op->getName().getIdentifier(), newOperands,
+                             {}, op->getAttrs());
+          else {
+            subOp = b.create(loc, op->getName().getIdentifier(), newOperands,
+                             subType, op->getAttrs());
+            subOps.push_back(subOp->getResults()[0]);
+          }
+          idx++;
+        }
+      }
+    }
+    if (numResults == 1) {
+      auto glue = b.create<tt::GlueOp>(loc, type, subOps);
+      op->replaceAllUsesWith(glue);
+    }
+    op->erase();
+  }
+
+  // for dot split k, n to make the inner register contiguous
+  void transformDotOp(tt::DotOp dot) {
+    assert(dotSize.size() == 3 && "target-size should have m, n ,k");
+    auto aType = dot.getA().getType().cast<RankedTensorType>();
+    auto bType = dot.getB().getType().cast<RankedTensorType>();
+    auto cType = dot.getC().getType().cast<RankedTensorType>();
+    auto aShape = aType.getShape();
+    auto bShape = bType.getShape();
+    auto cShape = cType.getShape();
+    auto m = aShape[0];
+    auto n = bShape[1];
+    auto k = aShape[1];
+    auto mStep = dotSize[0];
+    auto nStep = dotSize[1];
+    auto kStep = dotSize[2];
+    OpBuilder b(dot);
+    auto loc = dot.getLoc();
+    auto packValues = [&](ArrayRef<int64_t> values) {
+      SmallVector<OpFoldResult> newValues = llvm::to_vector<4>(
+          llvm::map_range(values, [&](int64_t v) -> OpFoldResult {
+            return b.getI64IntegerAttr(v);
+          }));
+      return newValues;
+    };
+    auto getSubDotVal = [&](Value val, int64_t mm, int64_t kk, int64_t mStep,
+                            int64_t kStep) {
+      auto [shape, subType, subSize] = getSubTypeAndShape(val.getType());
+      auto subIdx =
+          (kk / subSize[1]) * (shape[0] / subSize[0]) + mm / subSize[0];
+      auto subVal = b.create<tt::ExtractOp>(loc, subType, val, subIdx);
+      auto subDotType = RankedTensorType::get(
+          {mStep, kStep},
+          val.getType().cast<RankedTensorType>().getElementType());
+      auto subDotIdx = ((kk % subSize[1]) / kStep) * (subSize[0] / mStep) +
+                       (mm % subSize[0]) / mStep;
+      auto subDotVal =
+          b.create<tt::ExtractOp>(loc, subDotType, subVal, subDotIdx);
+      return subDotVal;
+    };
+
+    // n first, so that we can use larger store
+    auto [shape, subType, subSize] = getSubTypeAndShape(cType);
+    SmallVector<Value> subCs;
+    SmallVector<Value> subOps;
+    for (auto nn = 0; nn < n; nn += nStep) {
+      for (auto mm = 0; mm < m; mm += mStep) {
+        if (mm % subSize[0] == 0) {
+          subOps.clear();
+        }
+        Value subDotC = getSubDotVal(dot.getC(), mm, nn, mStep, nStep);
+        for (auto kk = 0; kk < k; kk += kStep) {
+          auto subDotA = getSubDotVal(dot.getA(), mm, kk, mStep, kStep);
+          auto subDotB = getSubDotVal(dot.getB(), kk, nn, kStep, nStep);
+          subDotC = b.create<tt::DotOp>(loc, subDotA, subDotB, subDotC,
+                                        dot.getAllowTF32Attr(),
+                                        dot.getMaxNumImpreciseAccAttr());
+        }
+        subOps.push_back(subDotC);
+        if ((mm % subSize[0]) == (subSize[0] - mStep)) {
+          Value glue = b.create<tt::GlueOp>(loc, subType, subOps);
+          subCs.push_back(glue);
+        }
+      }
+    }
+    auto newC = b.create<tt::GlueOp>(loc, dot.getType(), subCs);
+    dot->replaceAllUsesWith(newC->getResults());
+    dot->erase();
+  }
+
+  class ArithRemPattern : public OpRewritePattern<arith::RemSIOp> {
+  public:
+    using OpRewritePattern<arith::RemSIOp>::OpRewritePattern;
+    LogicalResult matchAndRewrite(arith::RemSIOp op,
+                                  PatternRewriter &rewriter) const final {
+      auto loc = op.getLoc();
+      auto type = op.getType();
+      auto rhs = op.getRhs();
+      APInt value;
+      if (!matchPattern(rhs, m_ConstantInt(&value)))
+        return failure();
+      if (value.isOne()) {
+        auto attr = rewriter.getIntegerAttr(type, 0);
+        auto zero = rewriter.create<arith::ConstantOp>(loc, attr);
+        rewriter.replaceOp(op, zero);
+        return success();
+      } else if (value.popcount() == 1) {
+        auto attr = rewriter.getIntegerAttr(type, value.getSExtValue() - 1);
+        auto mask = rewriter.create<arith::ConstantOp>(loc, attr);
+        auto result = rewriter.create<arith::AndIOp>(loc, op.getLhs(), mask);
+        rewriter.replaceOp(op, result);
+        return success();
+      }
+      return failure();
+    }
+  };
+
+  // assume that no sideEffect op in between
+  class ExtractPattern : public OpRewritePattern<tt::ExtractOp> {
+  public:
+    using OpRewritePattern<tt::ExtractOp>::OpRewritePattern;
+    LogicalResult matchAndRewrite(tt::ExtractOp op,
+                                  PatternRewriter &rewriter) const final {
+      auto base = op.getBase();
+      if (auto def = base.getDefiningOp()) {
+        if (auto glue = dyn_cast<tt::GlueOp>(def)) {
+          auto sub = glue->getOperand(op.getIdx());
+          rewriter.replaceOp(op, sub);
+          return success();
+        }
+      }
+      if (base.getType() == op.getType() && op.getIdx() == 0) {
+        rewriter.replaceOp(op, base);
+        return success();
+      }
+      return failure();
+    }
+  };
+  class ScfPattern : public OpRewritePattern<scf::ForOp> {
+  public:
+    using OpRewritePattern<scf::ForOp>::OpRewritePattern;
+    LogicalResult matchAndRewrite(scf::ForOp op,
+                                  PatternRewriter &rewriter) const final {
+      SmallVector<Operation *> deleteList;
+      SmallVector<Value> newInits;
+      DenseMap<Value, int> userIndexMap;
+      auto idx = 0;
+      for (auto [arg, init] :
+           llvm::zip(op.getRegionIterArgs(), op.getInits())) {
+        auto glue = dyn_cast<tt::GlueOp>(init.getDefiningOp());
+        if (!glue) {
+          newInits.push_back(init);
+          userIndexMap[arg] = idx;
+          idx++;
+          continue;
+        }
+        auto numSplit = glue->getOperands().size();
+        for (auto i = 0; i < numSplit; i++) {
+          newInits.push_back(glue->getOperand(i));
+        }
+        for (auto user : arg.getUsers()) {
+          auto extract = dyn_cast<tt::ExtractOp>(user);
+          if (extract) {
+            userIndexMap[extract] = idx + extract.getIdx();
+            deleteList.push_back(extract.getOperation());
+          }
+        }
+        idx += numSplit;
+      }
+      if (newInits.size() == op.getInits().size())
+        return failure();
+      auto newOp = rewriter.create<scf::ForOp>(op.getLoc(), op.getLowerBound(),
+                                               op.getUpperBound(), op.getStep(),
+                                               newInits);
+
+      for (auto [user, idx] : userIndexMap)
+        user.replaceAllUsesWith(newOp.getRegionIterArgs()[idx]);
+      op.getInductionVar().replaceAllUsesWith(newOp.getInductionVar());
+      // splice operations
+      auto *body = newOp.getBody();
+      body->getOperations().splice(body->begin(),
+                                   op.getBody()->getOperations());
+      // yield op
+      auto yield = cast<scf::YieldOp>(body->getTerminator());
+      SmallVector<Value> newValues;
+      for (auto result : yield.getResults()) {
+        auto def = result.getDefiningOp();
+        if (def) {
+          if (auto glue = dyn_cast<tt::GlueOp>(def)) {
+            newValues.append(glue->getOperands().begin(),
+                             glue->getOperands().end());
+          } else {
+            newValues.push_back(result);
+          }
+        }
+      }
+      {
+        OpBuilder::InsertionGuard guard(rewriter);
+        rewriter.setInsertionPoint(yield);
+        rewriter.create<scf::YieldOp>(yield.getLoc(), newValues);
+        rewriter.eraseOp(yield);
+      }
+
+      // replaceResults
+      userIndexMap.clear();
+      idx = 0;
+      for (auto [result, init] : llvm::zip(op.getResults(), op.getInits())) {
+        auto glue = dyn_cast<tt::GlueOp>(init.getDefiningOp());
+        if (!glue) {
+          userIndexMap[result] = idx;
+          idx++;
+          continue;
+        }
+        for (auto user : result.getUsers()) {
+          auto extract = dyn_cast<tt::ExtractOp>(user);
+          if (extract) {
+            userIndexMap[extract] = idx + extract.getIdx();
+            deleteList.push_back(extract.getOperation());
+          }
+        }
+        idx += glue->getOperands().size();
+      }
+      for (auto [user, idx] : userIndexMap)
+        user.replaceAllUsesWith(newOp.getResults()[idx]);
+      // rewriter.replaceAllUsesWith()
+      for (auto op : deleteList)
+        rewriter.eraseOp(op);
+      rewriter.eraseOp(op);
+      return success();
+    }
+  };
+};
+
+std::unique_ptr<Pass> mlir::createTritonGPUMatchTargetSizePass() {
+  return std::make_unique<TritonGPUMatchTargetSizePass>();
+}
diff --git a/test/TritonGPU/match-target-size.mlir b/test/TritonGPU/match-target-size.mlir
new file mode 100644
index 000000000..709154e63
--- /dev/null
+++ b/test/TritonGPU/match-target-size.mlir
@@ -0,0 +1,64 @@
+#warp = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>
+#dot0_ = #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>
+#dot1_ = #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>
+module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
+  tt.func public @matmul_kernel_with_block_pointers_without_convertlayout(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg4: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg5: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg8: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {
+    %c64_i32 = arith.constant 64 : i32
+    %c4_i32 = arith.constant 4 : i32
+    %c0_i32 = arith.constant 0 : i32
+    %c32_i32 = arith.constant 32 : i32
+    %c127_i32 = arith.constant 127 : i32
+    %c1_i64 = arith.constant 1 : i64
+    %c128_i32 = arith.constant 128 : i32
+    %c8_i32 = arith.constant 8 : i32
+    %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #warp>
+    %0 = gpu.subgroup_id : index
+    %1 = arith.index_cast %0 : index to i32
+    %2 = tt.get_program_id x : i32
+    %3 = arith.addi %arg3, %c127_i32 : i32
+    %4 = arith.divsi %3, %c128_i32 : i32
+    %5 = arith.addi %arg4, %c127_i32 : i32
+    %6 = arith.divsi %5, %c128_i32 : i32
+    %7 = arith.muli %6, %c8_i32 : i32
+    %8 = arith.divsi %2, %7 : i32
+    %9 = arith.muli %8, %c8_i32 : i32
+    %10 = arith.subi %4, %9 : i32
+    %11 = arith.minsi %10, %c8_i32 : i32
+    %12 = arith.remsi %2, %11 : i32
+    %13 = arith.addi %9, %12 : i32
+    %14 = arith.remsi %2, %7 : i32
+    %15 = arith.divsi %14, %11 : i32
+    %16 = arith.muli %13, %c128_i32 : i32
+    %17 = arith.extsi %arg3 : i32 to i64
+    %18 = arith.extsi %arg5 : i32 to i64
+    %19 = arith.extsi %arg6 : i32 to i64
+    %20 = arith.divsi %1, %c4_i32 : i32
+    %21 = arith.remsi %20, %c8_i32 : i32
+    %22 = arith.muli %21, %c32_i32 : i32
+    %23 = arith.addi %22, %16 : i32
+    %24 = tt.make_tensor_ptr %arg0, [%17, %18], [%19, %c1_i64], [%23, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16, #dot0_>, 1>
+    %25 = arith.muli %15, %c128_i32 : i32
+    %26 = arith.extsi %arg4 : i32 to i64
+    %27 = arith.extsi %arg7 : i32 to i64
+    %28 = arith.remsi %1, %c4_i32 : i32
+    %29 = arith.remsi %28, %c4_i32 : i32
+    %30 = arith.muli %29, %c64_i32 : i32
+    %31 = arith.addi %30, %25 : i32
+    %32 = tt.make_tensor_ptr %arg1, [%18, %26], [%27, %c1_i64], [%c0_i32, %31] {order = array<i32: 1, 0>} : <tensor<32x64xf16, #dot1_>, 1>
+    %33:3 = scf.for %arg9 = %c0_i32 to %arg5 step %c32_i32 iter_args(%arg10 = %cst, %arg11 = %24, %arg12 = %32) -> (tensor<32x64xf32, #warp>, !tt.ptr<tensor<32x32xf16, #dot0_>, 1>, !tt.ptr<tensor<32x64xf16, #dot1_>, 1>)  : i32 {
+      %37 = tt.load %arg11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16, #dot0_>, 1> -> tensor<32x32xf16, #dot0_>
+      %38 = tt.load %arg12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x64xf16, #dot1_>, 1> -> tensor<32x64xf16, #dot1_>
+      %39 = tt.dot %37, %38, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<32x32xf16, #dot0_> * tensor<32x64xf16, #dot1_> -> tensor<32x64xf32, #warp>
+      %40 = tt.advance %arg11, [%c0_i32, %c32_i32] : <tensor<32x32xf16, #dot0_>, 1>
+      %41 = tt.advance %arg12, [%c32_i32, %c0_i32] : <tensor<32x64xf16, #dot1_>, 1>
+      scf.yield %39, %40, %41 : tensor<32x64xf32, #warp>, !tt.ptr<tensor<32x32xf16, #dot0_>, 1>, !tt.ptr<tensor<32x64xf16, #dot1_>, 1>
+    }
+    %34 = arith.truncf %33#0 : tensor<32x64xf32, #warp> to tensor<32x64xf16, #warp>
+    %35 = arith.extsi %arg8 : i32 to i64
+    %36 = tt.make_tensor_ptr %arg2, [%17, %26], [%35, %c1_i64], [%23, %31] {order = array<i32: 1, 0>} : <tensor<32x64xf16, #warp>, 1>
+    tt.store %36, %34 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<32x64xf16, #warp>, 1>, tensor<32x64xf16, #warp>
+    tt.return
+  }
+}
+
+
diff --git a/test/TritonGPU/match-target-size.output.mlir b/test/TritonGPU/match-target-size.output.mlir
new file mode 100644
index 000000000..390c6c7d1
--- /dev/null
+++ b/test/TritonGPU/match-target-size.output.mlir
@@ -0,0 +1,186 @@
+module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
+  tt.func public @matmul_kernel_with_block_pointers_without_convertlayout(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg4: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg5: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg8: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {
+    %c16_i32 = arith.constant 16 : i32
+    %c48_i32 = arith.constant 48 : i32
+    %c64_i32 = arith.constant 64 : i32
+    %c4_i32 = arith.constant 4 : i32
+    %c0_i32 = arith.constant 0 : i32
+    %c32_i32 = arith.constant 32 : i32
+    %c127_i32 = arith.constant 127 : i32
+    %c1_i64 = arith.constant 1 : i64
+    %c128_i32 = arith.constant 128 : i32
+    %c8_i32 = arith.constant 8 : i32
+    %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32>
+    %0 = gpu.subgroup_id : index
+    %1 = arith.index_cast %0 : index to i32
+    %2 = tt.get_program_id x : i32
+    %3 = arith.addi %arg3, %c127_i32 : i32
+    %4 = arith.divsi %3, %c128_i32 : i32
+    %5 = arith.addi %arg4, %c127_i32 : i32
+    %6 = arith.divsi %5, %c128_i32 : i32
+    %7 = arith.muli %6, %c8_i32 : i32
+    %8 = arith.divsi %2, %7 : i32
+    %9 = arith.muli %8, %c8_i32 : i32
+    %10 = arith.subi %4, %9 : i32
+    %11 = arith.minsi %10, %c8_i32 : i32
+    %12 = arith.remsi %2, %11 : i32
+    %13 = arith.addi %9, %12 : i32
+    %14 = arith.remsi %2, %7 : i32
+    %15 = arith.divsi %14, %11 : i32
+    %16 = arith.muli %13, %c128_i32 : i32
+    %17 = arith.extsi %arg3 : i32 to i64
+    %18 = arith.extsi %arg5 : i32 to i64
+    %19 = arith.extsi %arg6 : i32 to i64
+    %20 = arith.muli %1, %c8_i32 : i32
+    %21 = arith.addi %20, %16 : i32
+    %26 = arith.divsi %1, %c4_i32 : i32
+    %27 = arith.remsi %26, %c8_i32 : i32
+    %28 = arith.muli %27, %c32_i32 : i32
+    %29 = arith.addi %28, %16 : i32
+    %30 = tt.make_tensor_ptr %arg0, [%17, %18], [%19, %c1_i64], [%29, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x16xf16>, 1>
+    %31 = tt.make_tensor_ptr %arg0, [%17, %18], [%19, %c1_i64], [%29, %c16_i32] {order = array<i32: 1, 0>} : <tensor<32x16xf16>, 1>
+    %32 = arith.muli %15, %c128_i32 : i32
+    %33 = arith.extsi %arg4 : i32 to i64
+    %34 = arith.extsi %arg7 : i32 to i64
+    %35 = arith.divsi %1, %c8_i32 : i32
+    %36 = arith.remsi %35, %c4_i32 : i32
+    %37 = arith.muli %36, %c8_i32 : i32
+    %38 = arith.remsi %1, %c8_i32 : i32
+    %39 = arith.remsi %38, %c8_i32 : i32
+    %40 = arith.muli %39, %c32_i32 : i32
+    %41 = arith.addi %40, %32 : i32
+    %46 = arith.remsi %1, %c4_i32 : i32
+    %47 = arith.remsi %46, %c4_i32 : i32
+    %48 = arith.muli %47, %c64_i32 : i32
+    %49 = arith.addi %48, %32 : i32
+    %50 = tt.make_tensor_ptr %arg1, [%18, %33], [%34, %c1_i64], [%c0_i32, %49] {order = array<i32: 1, 0>} : <tensor<32x16xf16>, 1>
+    %51 = arith.addi %49, %c16_i32 : i32
+    %52 = tt.make_tensor_ptr %arg1, [%18, %33], [%34, %c1_i64], [%c0_i32, %51] {order = array<i32: 1, 0>} : <tensor<32x16xf16>, 1>
+    %53 = arith.addi %49, %c32_i32 : i32
+    %54 = tt.make_tensor_ptr %arg1, [%18, %33], [%34, %c1_i64], [%c0_i32, %53] {order = array<i32: 1, 0>} : <tensor<32x16xf16>, 1>
+    %55 = arith.addi %49, %c48_i32 : i32
+    %56 = tt.make_tensor_ptr %arg1, [%18, %33], [%34, %c1_i64], [%c0_i32, %55] {order = array<i32: 1, 0>} : <tensor<32x16xf16>, 1>
+    %57:16 = scf.for %arg9 = %c0_i32 to %arg5 step %c32_i32 iter_args(%arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %30, %arg19 = %31, %arg20 = %50, %arg21 = %52, %arg22 = %54, %arg23 = %56) -> (tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>)  : i32 {
+      %76 = tt.load %arg18 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x16xf16>, 1> -> tensor<32x16xf16>
+      %77 = tt.load %arg19 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x16xf16>, 1> -> tensor<32x16xf16>
+      %78 = tt.load %arg20 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x16xf16>, 1> -> tensor<32x16xf16>
+      %79 = tt.load %arg21 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x16xf16>, 1> -> tensor<32x16xf16>
+      %80 = tt.load %arg22 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x16xf16>, 1> -> tensor<32x16xf16>
+      %81 = tt.load %arg23 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x16xf16>, 1> -> tensor<32x16xf16>
+      %extracted_slice = tensor.extract_slice %arg10[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_0 = tensor.extract_slice %76[0, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %extracted_slice_1 = tensor.extract_slice %78[0, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %82 = tt.dot %extracted_slice_0, %extracted_slice_1, %extracted_slice {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_2 = tensor.extract_slice %77[0, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %extracted_slice_3 = tensor.extract_slice %78[16, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %83 = tt.dot %extracted_slice_2, %extracted_slice_3, %82 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_4 = tensor.extract_slice %arg10[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_5 = tensor.extract_slice %76[8, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %84 = tt.dot %extracted_slice_5, %extracted_slice_1, %extracted_slice_4 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_6 = tensor.extract_slice %77[8, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %85 = tt.dot %extracted_slice_6, %extracted_slice_3, %84 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %86 = tensor.empty() : tensor<16x16xf32>
+      %inserted_slice = tensor.insert_slice %83 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_7 = tensor.insert_slice %85 into %inserted_slice[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_8 = tensor.extract_slice %arg11[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_9 = tensor.extract_slice %76[16, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %87 = tt.dot %extracted_slice_9, %extracted_slice_1, %extracted_slice_8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_10 = tensor.extract_slice %77[16, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %88 = tt.dot %extracted_slice_10, %extracted_slice_3, %87 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_11 = tensor.extract_slice %arg11[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_12 = tensor.extract_slice %76[24, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %89 = tt.dot %extracted_slice_12, %extracted_slice_1, %extracted_slice_11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_13 = tensor.extract_slice %77[24, 0] [8, 16] [1, 1] : tensor<32x16xf16> to tensor<8x16xf16>
+      %90 = tt.dot %extracted_slice_13, %extracted_slice_3, %89 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_14 = tensor.insert_slice %88 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_15 = tensor.insert_slice %90 into %inserted_slice_14[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_16 = tensor.extract_slice %arg12[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_17 = tensor.extract_slice %79[0, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %91 = tt.dot %extracted_slice_0, %extracted_slice_17, %extracted_slice_16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_18 = tensor.extract_slice %79[16, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %92 = tt.dot %extracted_slice_2, %extracted_slice_18, %91 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_19 = tensor.extract_slice %arg12[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %93 = tt.dot %extracted_slice_5, %extracted_slice_17, %extracted_slice_19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %94 = tt.dot %extracted_slice_6, %extracted_slice_18, %93 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_20 = tensor.insert_slice %92 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_21 = tensor.insert_slice %94 into %inserted_slice_20[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_22 = tensor.extract_slice %arg13[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %95 = tt.dot %extracted_slice_9, %extracted_slice_17, %extracted_slice_22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %96 = tt.dot %extracted_slice_10, %extracted_slice_18, %95 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_23 = tensor.extract_slice %arg13[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %97 = tt.dot %extracted_slice_12, %extracted_slice_17, %extracted_slice_23 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %98 = tt.dot %extracted_slice_13, %extracted_slice_18, %97 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_24 = tensor.insert_slice %96 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_25 = tensor.insert_slice %98 into %inserted_slice_24[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_26 = tensor.extract_slice %arg14[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_27 = tensor.extract_slice %80[0, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %99 = tt.dot %extracted_slice_0, %extracted_slice_27, %extracted_slice_26 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_28 = tensor.extract_slice %80[16, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %100 = tt.dot %extracted_slice_2, %extracted_slice_28, %99 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_29 = tensor.extract_slice %arg14[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %101 = tt.dot %extracted_slice_5, %extracted_slice_27, %extracted_slice_29 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %102 = tt.dot %extracted_slice_6, %extracted_slice_28, %101 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_30 = tensor.insert_slice %100 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_31 = tensor.insert_slice %102 into %inserted_slice_30[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_32 = tensor.extract_slice %arg15[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %103 = tt.dot %extracted_slice_9, %extracted_slice_27, %extracted_slice_32 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %104 = tt.dot %extracted_slice_10, %extracted_slice_28, %103 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_33 = tensor.extract_slice %arg15[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %105 = tt.dot %extracted_slice_12, %extracted_slice_27, %extracted_slice_33 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %106 = tt.dot %extracted_slice_13, %extracted_slice_28, %105 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_34 = tensor.insert_slice %104 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_35 = tensor.insert_slice %106 into %inserted_slice_34[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_36 = tensor.extract_slice %arg16[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %extracted_slice_37 = tensor.extract_slice %81[0, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %107 = tt.dot %extracted_slice_0, %extracted_slice_37, %extracted_slice_36 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_38 = tensor.extract_slice %81[16, 0] [16, 16] [1, 1] : tensor<32x16xf16> to tensor<16x16xf16>
+      %108 = tt.dot %extracted_slice_2, %extracted_slice_38, %107 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_39 = tensor.extract_slice %arg16[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %109 = tt.dot %extracted_slice_5, %extracted_slice_37, %extracted_slice_39 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %110 = tt.dot %extracted_slice_6, %extracted_slice_38, %109 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_40 = tensor.insert_slice %108 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_41 = tensor.insert_slice %110 into %inserted_slice_40[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %extracted_slice_42 = tensor.extract_slice %arg17[0, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %111 = tt.dot %extracted_slice_9, %extracted_slice_37, %extracted_slice_42 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %112 = tt.dot %extracted_slice_10, %extracted_slice_38, %111 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %extracted_slice_43 = tensor.extract_slice %arg17[8, 0] [8, 16] [1, 1] : tensor<16x16xf32> to tensor<8x16xf32>
+      %113 = tt.dot %extracted_slice_12, %extracted_slice_37, %extracted_slice_43 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %114 = tt.dot %extracted_slice_13, %extracted_slice_38, %113 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
+      %inserted_slice_44 = tensor.insert_slice %112 into %86[0, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %inserted_slice_45 = tensor.insert_slice %114 into %inserted_slice_44[8, 0] [8, 16] [1, 1] : tensor<8x16xf32> into tensor<16x16xf32>
+      %116 = tt.advance %arg18, [%c0_i32, %c32_i32] : <tensor<32x16xf16>, 1>
+      %117 = tt.advance %arg19, [%c0_i32, %c32_i32] : <tensor<32x16xf16>, 1>
+      %119 = tt.advance %arg20, [%c32_i32, %c0_i32] : <tensor<32x16xf16>, 1>
+      %120 = tt.advance %arg21, [%c32_i32, %c0_i32] : <tensor<32x16xf16>, 1>
+      %121 = tt.advance %arg22, [%c32_i32, %c0_i32] : <tensor<32x16xf16>, 1>
+      %122 = tt.advance %arg23, [%c32_i32, %c0_i32] : <tensor<32x16xf16>, 1>
+      scf.yield %inserted_slice_7, %inserted_slice_15, %inserted_slice_21, %inserted_slice_25, %inserted_slice_31, %inserted_slice_35, %inserted_slice_41, %inserted_slice_45, %116, %117, %119, %120, %121, %122, %115, %118 : tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, tensor<16x16xf32>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>, !tt.ptr<tensor<32x16xf16>, 1>}
+    %58 = arith.truncf %57#0 : tensor<16x16xf32> to tensor<16x16xf16>
+    %59 = arith.truncf %57#1 : tensor<16x16xf32> to tensor<16x16xf16>
+    %60 = arith.truncf %57#2 : tensor<16x16xf32> to tensor<16x16xf16>
+    %61 = arith.truncf %57#3 : tensor<16x16xf32> to tensor<16x16xf16>
+    %62 = arith.truncf %57#4 : tensor<16x16xf32> to tensor<16x16xf16>
+    %63 = arith.truncf %57#5 : tensor<16x16xf32> to tensor<16x16xf16>
+    %64 = arith.truncf %57#6 : tensor<16x16xf32> to tensor<16x16xf16>
+    %65 = arith.truncf %57#7 : tensor<16x16xf32> to tensor<16x16xf16>
+    %66 = arith.extsi %arg8 : i32 to i64
+    %67 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%29, %49] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %68 = arith.addi %29, %c16_i32 : i32
+    %69 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%68, %49] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %70 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%29, %51] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %71 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%68, %51] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %72 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%29, %53] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %73 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%68, %53] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %74 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%29, %55] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    %75 = tt.make_tensor_ptr %arg2, [%17, %33], [%66, %c1_i64], [%68, %55] {order = array<i32: 1, 0>} : <tensor<16x16xf16>, 1>
+    tt.store %67, %58 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %69, %59 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %70, %60 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %71, %61 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %72, %62 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %73, %63 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %74, %64 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.store %75, %65 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<16x16xf16>, 1>, tensor<16x16xf16>
+    tt.return
+  }
+}
-- 
2.34.1

