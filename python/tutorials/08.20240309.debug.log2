// -----// IR Dump Before Inliner (inline) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %2 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %3 = arith.muli %2, %c4_i32 : i32 loc(#loc)
    %4 = arith.divsi %0, %3 : i32 loc(#loc)
    %c4_i32_0 = arith.constant 4 : i32 loc(#loc)
    %5 = arith.muli %4, %c4_i32_0 : i32 loc(#loc)
    %6 = arith.subi %1, %5 : i32 loc(#loc)
    %7 = tt.call @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%6) : (i32) -> i32 loc(#loc)
    %8 = arith.remsi %0, %7 : i32 loc(#loc)
    %9 = arith.addi %5, %8 : i32 loc(#loc)
    %10 = arith.remsi %0, %3 : i32 loc(#loc)
    %11 = arith.divsi %10, %7 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %12 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_1 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_2 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %13 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64_1], [%c4096_i64_2, %c1_i64], [%12, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %c256_i32_3 = arith.constant 256 : i32 loc(#loc)
    %14 = arith.muli %11, %c256_i32_3 : i32 loc(#loc)
    %c4096_i64_4 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_5 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_6 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_7 = arith.constant 1 : i64 loc(#loc)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc)
    %15 = tt.make_tensor_ptr %arg1, [%c4096_i64_4, %c4096_i64_5], [%c4096_i64_6, %c1_i64_7], [%c0_i32_8, %14] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %16 = tt.call @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() : () -> tensor<256x256xf32> loc(#loc)
    %c0_i32_9 = arith.constant 0 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %17 = arith.bitcast %c0_i32_9 : i32 to i32 loc(#loc)
    %18 = arith.bitcast %c4096_i32 : i32 to i32 loc(#loc)
    %19 = arith.bitcast %c32_i32 : i32 to i32 loc(#loc)
    %20 = llvm.mlir.undef : i32 loc(#loc)
    %21:3 = scf.for %arg6 = %17 to %18 step %19 iter_args(%arg7 = %16, %arg8 = %13, %arg9 = %15) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %25 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %26 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
      %cst_16 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
      %27 = tt.dot %25, %26, %cst_16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %28 = arith.addf %arg7, %27 : tensor<256x256xf32> loc(#loc)
      %c0_i32_17 = arith.constant 0 : i32 loc(#loc)
      %c32_i32_18 = arith.constant 32 : i32 loc(#loc)
      %29 = tt.advance %arg8, [%c0_i32_17, %c32_i32_18] : <tensor<256x32xf16>, 1> loc(#loc)
      %c32_i32_19 = arith.constant 32 : i32 loc(#loc)
      %c0_i32_20 = arith.constant 0 : i32 loc(#loc)
      %30 = tt.advance %arg9, [%c32_i32_19, %c0_i32_20] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %28, %29, %30 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %c256_i32_10 = arith.constant 256 : i32 loc(#loc)
    %22 = arith.muli %9, %c256_i32_10 : i32 loc(#loc)
    %c256_i32_11 = arith.constant 256 : i32 loc(#loc)
    %23 = arith.muli %11, %c256_i32_11 : i32 loc(#loc)
    %c4096_i64_12 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_13 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_14 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_15 = arith.constant 1 : i64 loc(#loc)
    %24 = tt.make_tensor_ptr %arg2, [%c4096_i64_12, %c4096_i64_13], [%c4096_i64_14, %c1_i64_15], [%22, %23] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %24, %21#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
  tt.func private @cdiv____0cconstexpr_4096__1cconstexpr_256_() -> i32 attributes {noinline = false} {
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    tt.return %c16_i32 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%arg0: i32 loc(unknown)) -> i32 attributes {noinline = false} {
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = arith.minsi %arg0, %c4_i32 : i32 loc(#loc)
    tt.return %0 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() -> tensor<256x256xf32> attributes {noinline = false} {
    %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    tt.return %cst_0 : tensor<256x256xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('tt.func' operation: @cdiv____0cconstexpr_4096__1cconstexpr_256_) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %2 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %3 = arith.muli %2, %c4_i32 : i32 loc(#loc)
    %4 = arith.divsi %0, %3 : i32 loc(#loc)
    %c4_i32_0 = arith.constant 4 : i32 loc(#loc)
    %5 = arith.muli %4, %c4_i32_0 : i32 loc(#loc)
    %6 = arith.subi %1, %5 : i32 loc(#loc)
    %7 = tt.call @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%6) : (i32) -> i32 loc(#loc)
    %8 = arith.remsi %0, %7 : i32 loc(#loc)
    %9 = arith.addi %5, %8 : i32 loc(#loc)
    %10 = arith.remsi %0, %3 : i32 loc(#loc)
    %11 = arith.divsi %10, %7 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %12 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_1 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_2 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %13 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64_1], [%c4096_i64_2, %c1_i64], [%12, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %c256_i32_3 = arith.constant 256 : i32 loc(#loc)
    %14 = arith.muli %11, %c256_i32_3 : i32 loc(#loc)
    %c4096_i64_4 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_5 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_6 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_7 = arith.constant 1 : i64 loc(#loc)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc)
    %15 = tt.make_tensor_ptr %arg1, [%c4096_i64_4, %c4096_i64_5], [%c4096_i64_6, %c1_i64_7], [%c0_i32_8, %14] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %16 = tt.call @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() : () -> tensor<256x256xf32> loc(#loc)
    %c0_i32_9 = arith.constant 0 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %17 = arith.bitcast %c0_i32_9 : i32 to i32 loc(#loc)
    %18 = arith.bitcast %c4096_i32 : i32 to i32 loc(#loc)
    %19 = arith.bitcast %c32_i32 : i32 to i32 loc(#loc)
    %20 = llvm.mlir.undef : i32 loc(#loc)
    %21:3 = scf.for %arg6 = %17 to %18 step %19 iter_args(%arg7 = %16, %arg8 = %13, %arg9 = %15) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %25 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %26 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
      %cst_16 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
      %27 = tt.dot %25, %26, %cst_16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %28 = arith.addf %arg7, %27 : tensor<256x256xf32> loc(#loc)
      %c0_i32_17 = arith.constant 0 : i32 loc(#loc)
      %c32_i32_18 = arith.constant 32 : i32 loc(#loc)
      %29 = tt.advance %arg8, [%c0_i32_17, %c32_i32_18] : <tensor<256x32xf16>, 1> loc(#loc)
      %c32_i32_19 = arith.constant 32 : i32 loc(#loc)
      %c0_i32_20 = arith.constant 0 : i32 loc(#loc)
      %30 = tt.advance %arg9, [%c32_i32_19, %c0_i32_20] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %28, %29, %30 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %c256_i32_10 = arith.constant 256 : i32 loc(#loc)
    %22 = arith.muli %9, %c256_i32_10 : i32 loc(#loc)
    %c256_i32_11 = arith.constant 256 : i32 loc(#loc)
    %23 = arith.muli %11, %c256_i32_11 : i32 loc(#loc)
    %c4096_i64_12 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_13 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_14 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_15 = arith.constant 1 : i64 loc(#loc)
    %24 = tt.make_tensor_ptr %arg2, [%c4096_i64_12, %c4096_i64_13], [%c4096_i64_14, %c1_i64_15], [%22, %23] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %24, %21#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
  tt.func private @cdiv____0cconstexpr_4096__1cconstexpr_256_() -> i32 attributes {noinline = false} {
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    tt.return %c16_i32 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%arg0: i32 loc(unknown)) -> i32 attributes {noinline = false} {
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = arith.minsi %arg0, %c4_i32 : i32 loc(#loc)
    tt.return %0 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() -> tensor<256x256xf32> attributes {noinline = false} {
    %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    tt.return %cst_0 : tensor<256x256xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('tt.func' operation: @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %2 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %3 = arith.muli %2, %c4_i32 : i32 loc(#loc)
    %4 = arith.divsi %0, %3 : i32 loc(#loc)
    %c4_i32_0 = arith.constant 4 : i32 loc(#loc)
    %5 = arith.muli %4, %c4_i32_0 : i32 loc(#loc)
    %6 = arith.subi %1, %5 : i32 loc(#loc)
    %7 = tt.call @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%6) : (i32) -> i32 loc(#loc)
    %8 = arith.remsi %0, %7 : i32 loc(#loc)
    %9 = arith.addi %5, %8 : i32 loc(#loc)
    %10 = arith.remsi %0, %3 : i32 loc(#loc)
    %11 = arith.divsi %10, %7 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %12 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_1 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_2 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %13 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64_1], [%c4096_i64_2, %c1_i64], [%12, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %c256_i32_3 = arith.constant 256 : i32 loc(#loc)
    %14 = arith.muli %11, %c256_i32_3 : i32 loc(#loc)
    %c4096_i64_4 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_5 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_6 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_7 = arith.constant 1 : i64 loc(#loc)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc)
    %15 = tt.make_tensor_ptr %arg1, [%c4096_i64_4, %c4096_i64_5], [%c4096_i64_6, %c1_i64_7], [%c0_i32_8, %14] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %16 = tt.call @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() : () -> tensor<256x256xf32> loc(#loc)
    %c0_i32_9 = arith.constant 0 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %17 = arith.bitcast %c0_i32_9 : i32 to i32 loc(#loc)
    %18 = arith.bitcast %c4096_i32 : i32 to i32 loc(#loc)
    %19 = arith.bitcast %c32_i32 : i32 to i32 loc(#loc)
    %20 = llvm.mlir.undef : i32 loc(#loc)
    %21:3 = scf.for %arg6 = %17 to %18 step %19 iter_args(%arg7 = %16, %arg8 = %13, %arg9 = %15) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %25 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %26 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
      %cst_16 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
      %27 = tt.dot %25, %26, %cst_16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %28 = arith.addf %arg7, %27 : tensor<256x256xf32> loc(#loc)
      %c0_i32_17 = arith.constant 0 : i32 loc(#loc)
      %c32_i32_18 = arith.constant 32 : i32 loc(#loc)
      %29 = tt.advance %arg8, [%c0_i32_17, %c32_i32_18] : <tensor<256x32xf16>, 1> loc(#loc)
      %c32_i32_19 = arith.constant 32 : i32 loc(#loc)
      %c0_i32_20 = arith.constant 0 : i32 loc(#loc)
      %30 = tt.advance %arg9, [%c32_i32_19, %c0_i32_20] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %28, %29, %30 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %c256_i32_10 = arith.constant 256 : i32 loc(#loc)
    %22 = arith.muli %9, %c256_i32_10 : i32 loc(#loc)
    %c256_i32_11 = arith.constant 256 : i32 loc(#loc)
    %23 = arith.muli %11, %c256_i32_11 : i32 loc(#loc)
    %c4096_i64_12 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_13 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_14 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_15 = arith.constant 1 : i64 loc(#loc)
    %24 = tt.make_tensor_ptr %arg2, [%c4096_i64_12, %c4096_i64_13], [%c4096_i64_14, %c1_i64_15], [%22, %23] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %24, %21#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
  tt.func private @cdiv____0cconstexpr_4096__1cconstexpr_256_() -> i32 attributes {noinline = false} {
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    tt.return %c16_i32 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%arg0: i32 loc(unknown)) -> i32 attributes {noinline = false} {
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = arith.minsi %arg0, %c4_i32 : i32 loc(#loc)
    tt.return %0 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() -> tensor<256x256xf32> attributes {noinline = false} {
    %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    tt.return %cst_0 : tensor<256x256xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('tt.func' operation: @zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %2 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %3 = arith.muli %2, %c4_i32 : i32 loc(#loc)
    %4 = arith.divsi %0, %3 : i32 loc(#loc)
    %c4_i32_0 = arith.constant 4 : i32 loc(#loc)
    %5 = arith.muli %4, %c4_i32_0 : i32 loc(#loc)
    %6 = arith.subi %1, %5 : i32 loc(#loc)
    %7 = tt.call @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%6) : (i32) -> i32 loc(#loc)
    %8 = arith.remsi %0, %7 : i32 loc(#loc)
    %9 = arith.addi %5, %8 : i32 loc(#loc)
    %10 = arith.remsi %0, %3 : i32 loc(#loc)
    %11 = arith.divsi %10, %7 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %12 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_1 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_2 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %13 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64_1], [%c4096_i64_2, %c1_i64], [%12, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %c256_i32_3 = arith.constant 256 : i32 loc(#loc)
    %14 = arith.muli %11, %c256_i32_3 : i32 loc(#loc)
    %c4096_i64_4 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_5 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_6 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_7 = arith.constant 1 : i64 loc(#loc)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc)
    %15 = tt.make_tensor_ptr %arg1, [%c4096_i64_4, %c4096_i64_5], [%c4096_i64_6, %c1_i64_7], [%c0_i32_8, %14] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %16 = tt.call @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() : () -> tensor<256x256xf32> loc(#loc)
    %c0_i32_9 = arith.constant 0 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %17 = arith.bitcast %c0_i32_9 : i32 to i32 loc(#loc)
    %18 = arith.bitcast %c4096_i32 : i32 to i32 loc(#loc)
    %19 = arith.bitcast %c32_i32 : i32 to i32 loc(#loc)
    %20 = llvm.mlir.undef : i32 loc(#loc)
    %21:3 = scf.for %arg6 = %17 to %18 step %19 iter_args(%arg7 = %16, %arg8 = %13, %arg9 = %15) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %25 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %26 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
      %cst_16 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
      %27 = tt.dot %25, %26, %cst_16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %28 = arith.addf %arg7, %27 : tensor<256x256xf32> loc(#loc)
      %c0_i32_17 = arith.constant 0 : i32 loc(#loc)
      %c32_i32_18 = arith.constant 32 : i32 loc(#loc)
      %29 = tt.advance %arg8, [%c0_i32_17, %c32_i32_18] : <tensor<256x32xf16>, 1> loc(#loc)
      %c32_i32_19 = arith.constant 32 : i32 loc(#loc)
      %c0_i32_20 = arith.constant 0 : i32 loc(#loc)
      %30 = tt.advance %arg9, [%c32_i32_19, %c0_i32_20] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %28, %29, %30 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %c256_i32_10 = arith.constant 256 : i32 loc(#loc)
    %22 = arith.muli %9, %c256_i32_10 : i32 loc(#loc)
    %c256_i32_11 = arith.constant 256 : i32 loc(#loc)
    %23 = arith.muli %11, %c256_i32_11 : i32 loc(#loc)
    %c4096_i64_12 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_13 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_14 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_15 = arith.constant 1 : i64 loc(#loc)
    %24 = tt.make_tensor_ptr %arg2, [%c4096_i64_12, %c4096_i64_13], [%c4096_i64_14, %c1_i64_15], [%22, %23] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %24, %21#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
  tt.func private @cdiv____0cconstexpr_4096__1cconstexpr_256_() -> i32 attributes {noinline = false} {
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    tt.return %c16_i32 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%arg0: i32 loc(unknown)) -> i32 attributes {noinline = false} {
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = arith.minsi %arg0, %c4_i32 : i32 loc(#loc)
    tt.return %0 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() -> tensor<256x256xf32> attributes {noinline = false} {
    %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    tt.return %cst_0 : tensor<256x256xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('tt.func' operation: @matmul_kernel_with_block_pointers_0d1d2d3d4d5d) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %2 = tt.call @cdiv____0cconstexpr_4096__1cconstexpr_256_() : () -> i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %3 = arith.muli %2, %c4_i32 : i32 loc(#loc)
    %4 = arith.divsi %0, %3 : i32 loc(#loc)
    %c4_i32_0 = arith.constant 4 : i32 loc(#loc)
    %5 = arith.muli %4, %c4_i32_0 : i32 loc(#loc)
    %6 = arith.subi %1, %5 : i32 loc(#loc)
    %7 = tt.call @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%6) : (i32) -> i32 loc(#loc)
    %8 = arith.remsi %0, %7 : i32 loc(#loc)
    %9 = arith.addi %5, %8 : i32 loc(#loc)
    %10 = arith.remsi %0, %3 : i32 loc(#loc)
    %11 = arith.divsi %10, %7 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %12 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_1 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_2 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %13 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64_1], [%c4096_i64_2, %c1_i64], [%12, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %c256_i32_3 = arith.constant 256 : i32 loc(#loc)
    %14 = arith.muli %11, %c256_i32_3 : i32 loc(#loc)
    %c4096_i64_4 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_5 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_6 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_7 = arith.constant 1 : i64 loc(#loc)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc)
    %15 = tt.make_tensor_ptr %arg1, [%c4096_i64_4, %c4096_i64_5], [%c4096_i64_6, %c1_i64_7], [%c0_i32_8, %14] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %16 = tt.call @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() : () -> tensor<256x256xf32> loc(#loc)
    %c0_i32_9 = arith.constant 0 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %17 = arith.bitcast %c0_i32_9 : i32 to i32 loc(#loc)
    %18 = arith.bitcast %c4096_i32 : i32 to i32 loc(#loc)
    %19 = arith.bitcast %c32_i32 : i32 to i32 loc(#loc)
    %20 = llvm.mlir.undef : i32 loc(#loc)
    %21:3 = scf.for %arg6 = %17 to %18 step %19 iter_args(%arg7 = %16, %arg8 = %13, %arg9 = %15) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %25 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %26 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc)
      %cst_16 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
      %27 = tt.dot %25, %26, %cst_16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %28 = arith.addf %arg7, %27 : tensor<256x256xf32> loc(#loc)
      %c0_i32_17 = arith.constant 0 : i32 loc(#loc)
      %c32_i32_18 = arith.constant 32 : i32 loc(#loc)
      %29 = tt.advance %arg8, [%c0_i32_17, %c32_i32_18] : <tensor<256x32xf16>, 1> loc(#loc)
      %c32_i32_19 = arith.constant 32 : i32 loc(#loc)
      %c0_i32_20 = arith.constant 0 : i32 loc(#loc)
      %30 = tt.advance %arg9, [%c32_i32_19, %c0_i32_20] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %28, %29, %30 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %c256_i32_10 = arith.constant 256 : i32 loc(#loc)
    %22 = arith.muli %9, %c256_i32_10 : i32 loc(#loc)
    %c256_i32_11 = arith.constant 256 : i32 loc(#loc)
    %23 = arith.muli %11, %c256_i32_11 : i32 loc(#loc)
    %c4096_i64_12 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_13 = arith.constant 4096 : i64 loc(#loc)
    %c4096_i64_14 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64_15 = arith.constant 1 : i64 loc(#loc)
    %24 = tt.make_tensor_ptr %arg2, [%c4096_i64_12, %c4096_i64_13], [%c4096_i64_14, %c1_i64_15], [%22, %23] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %24, %21#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
  tt.func private @cdiv____0cconstexpr_4096__1cconstexpr_256_() -> i32 attributes {noinline = false} {
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    tt.return %c16_i32 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(%arg0: i32 loc(unknown)) -> i32 attributes {noinline = false} {
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = arith.minsi %arg0, %c4_i32 : i32 loc(#loc)
    tt.return %0 : i32 loc(#loc)
  } loc(#loc)
  tt.func private @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() -> tensor<256x256xf32> attributes {noinline = false} {
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    tt.return %cst : tensor<256x256xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('tt.func' operation: @matmul_kernel_with_block_pointers_0d1d2d3d4d5d) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c16_i32_0 = arith.constant 16 : i32 loc(#loc)
    %1 = arith.muli %c16_i32_0, %c4_i32 : i32 loc(#loc)
    %2 = arith.divsi %0, %1 : i32 loc(#loc)
    %3 = arith.muli %2, %c4_i32 : i32 loc(#loc)
    %4 = arith.subi %c16_i32, %3 : i32 loc(#loc)
    %c4_i32_1 = arith.constant 4 : i32 loc(#loc)
    %5 = arith.minsi %4, %c4_i32_1 : i32 loc(#loc)
    %6 = arith.remsi %0, %5 : i32 loc(#loc)
    %7 = arith.addi %3, %6 : i32 loc(#loc)
    %8 = arith.remsi %0, %1 : i32 loc(#loc)
    %9 = arith.divsi %8, %5 : i32 loc(#loc)
    %10 = arith.muli %7, %c256_i32 : i32 loc(#loc)
    %11 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%10, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %12 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %13 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %12] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %14:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst_2, %arg8 = %11, %arg9 = %13) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %18 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %19 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %20 = tt.dot %18, %19, %cst {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %21 = arith.addf %arg7, %20 : tensor<256x256xf32> loc(#loc)
      %22 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %23 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %21, %22, %23 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %15 = arith.muli %7, %c256_i32 : i32 loc(#loc)
    %16 = arith.muli %9, %c256_i32 : i32 loc(#loc)
    %17 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %16] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %17, %14#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
  tt.func private @cdiv____0cconstexpr_4096__1cconstexpr_256_() -> i32 attributes {noinline = false} loc(#loc)
  tt.func private @minimum__i32__1cconstexpr_4__2cconstexpr_PROPAGATE_NAN_d_NONE_(i32) -> i32 attributes {noinline = false} loc(#loc)
  tt.func private @"zeros____0cconstexpr_(constexpr_256_, constexpr_256_)__1cconstexpr_fp32_"() -> tensor<256x256xf32> attributes {noinline = false} loc(#loc)
} loc(#loc)


// -----// IR Dump Before TritonCombineOps (triton-combine) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %17 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %18 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %19 = tt.dot %17, %18, %cst {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %20 = arith.addf %arg7, %19 : tensor<256x256xf32> loc(#loc)
      %21 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %22 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %20, %21, %22 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %15 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%14, %15] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %16, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %17 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %18 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %19 = tt.dot %17, %18, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %20 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %21 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %19, %20, %21 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %15 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%14, %15] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %16, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before TritonReorderBroadcast (triton-reorder-broadcast) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %17 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %18 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %19 = tt.dot %17, %18, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %20 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %21 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %19, %20, %21 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %15 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%14, %15] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %16, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %17 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %18 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %19 = tt.dot %17, %18, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %20 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %21 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %19, %20, %21 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %15 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%14, %15] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %16, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before LoopInvariantCodeMotion (loop-invariant-code-motion) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %15 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %16 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %17 = tt.dot %15, %16, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %18 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %19 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %17, %18, %19 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %11] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %14, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before SymbolDCE (symbol-dce) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %15 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %16 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %17 = tt.dot %15, %16, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %18 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %19 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %17, %18, %19 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %11] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %14, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before ConvertTritonToTritonGPUWarp (convert-triton-to-tritongpu-warp) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1>)  : i32 {
      %15 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16> loc(#loc)
      %16 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16> loc(#loc)
      %17 = tt.dot %15, %16, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32> loc(#loc)
      %18 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1> loc(#loc)
      %19 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1> loc(#loc)
      scf.yield %17, %18, %19 : tensor<256x256xf32>, !tt.ptr<tensor<256x32xf16>, 1>, !tt.ptr<tensor<32x256xf16>, 1> loc(#loc)
    } loc(#loc)
    %14 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %11] {order = array<i32: 1, 0>} : <tensor<256x256xf32>, 1> loc(#loc)
    tt.store %14, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32>, 1>, tensor<256x256xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


%17 = tt.dot %15, %16, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32>
%15 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16>
<block argument> of type '!tt.ptr<tensor<256x32xf16>, 1>' at index: 2
%10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16>, 1>
%15 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16>, 1> -> tensor<256x32xf16>
%18 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16>, 1>
%16 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16>
<block argument> of type '!tt.ptr<tensor<32x256xf16>, 1>' at index: 3
%12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16>, 1>
%16 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16>, 1> -> tensor<32x256xf16>
%19 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16>, 1>
%cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32>
<block argument> of type 'tensor<256x256xf32>' at index: 1
%17 = tt.dot %15, %16, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16> * tensor<32x256xf16> -> tensor<256x256xf32>
// -----// IR Dump Before TritonGPUDistributeToWarps (tritongpu-distribute-to-warps) ('builtin.module' operation) //----- //
#blocked = #triton_gpu.blocked<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], warpsPerCTA = [8, 4], order = [1, 0]}>
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x256xf32, #blocked> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %0 = tt.get_program_id x : i32 loc(#loc)
    %1 = arith.divsi %0, %c64_i32 : i32 loc(#loc)
    %2 = arith.muli %1, %c4_i32 : i32 loc(#loc)
    %3 = arith.subi %c16_i32, %2 : i32 loc(#loc)
    %4 = arith.minsi %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.remsi %0, %4 : i32 loc(#loc)
    %6 = arith.addi %2, %5 : i32 loc(#loc)
    %7 = arith.remsi %0, %c64_i32 : i32 loc(#loc)
    %8 = arith.divsi %7, %4 : i32 loc(#loc)
    %9 = arith.muli %6, %c256_i32 : i32 loc(#loc)
    %10 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %c0_i32] {order = array<i32: 1, 0>} : <tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>, 1> loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %11] {order = array<i32: 1, 0>} : <tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>, 1> loc(#loc)
    %13:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %10, %arg9 = %12) -> (tensor<256x256xf32, #blocked>, !tt.ptr<tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>, 1>, !tt.ptr<tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>, 1>)  : i32 {
      %15 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>, 1> -> tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> loc(#loc)
      %16 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>, 1> -> tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> loc(#loc)
      %17 = tt.dot %15, %16, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<256x256xf32, #blocked> loc(#loc)
      %18 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>, 1> loc(#loc)
      %19 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>, 1> loc(#loc)
      scf.yield %17, %18, %19 : tensor<256x256xf32, #blocked>, !tt.ptr<tensor<256x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>, 1>, !tt.ptr<tensor<32x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>, 1> loc(#loc)
    } {triton_gpu.workload = 3 : i32} loc(#loc)
    %14 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%9, %11] {order = array<i32: 1, 0>} : <tensor<256x256xf32, #blocked>, 1> loc(#loc)
    tt.store %14, %13#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<256x256xf32, #blocked>, 1>, tensor<256x256xf32, #blocked> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before TritonGPUMatchTargetSize (tritongpu-match-target-size) ('builtin.module' operation) //----- //
#loc = loc(unknown)
#warp = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #warp> loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.remsi %2, %c64_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %c4_i32_0 = arith.constant 4 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32_0 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %13 = arith.remsi %12, %c8_i32 : i32 loc(#loc)
    %c32_i32_1 = arith.constant 32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32_1 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %c4_i32_2 = arith.constant 4 : i32 loc(#loc)
    %16 = arith.remsi %1, %c4_i32_2 : i32 loc(#loc)
    %17 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>>, 1> loc(#loc)
    %18 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %c4_i32_3 = arith.constant 4 : i32 loc(#loc)
    %19 = arith.divsi %1, %c4_i32_3 : i32 loc(#loc)
    %c4_i32_4 = arith.constant 4 : i32 loc(#loc)
    %20 = arith.remsi %1, %c4_i32_4 : i32 loc(#loc)
    %c4_i32_5 = arith.constant 4 : i32 loc(#loc)
    %21 = arith.remsi %20, %c4_i32_5 : i32 loc(#loc)
    %c64_i32_6 = arith.constant 64 : i32 loc(#loc)
    %22 = arith.muli %21, %c64_i32_6 : i32 loc(#loc)
    %23 = arith.addi %22, %18 : i32 loc(#loc)
    %24 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %23] {order = array<i32: 1, 0>} : <tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>>, 1> loc(#loc)
    %25:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %17, %arg9 = %24) -> (tensor<32x64xf32, #warp>, !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>>, 1>, !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>>, 1>)  : i32 {
      %35 = tt.load %arg8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>>, 1> -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>> loc(#loc)
      %36 = tt.load %arg9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>>, 1> -> tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>> loc(#loc)
      %37 = tt.dot %35, %36, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>> * tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>> -> tensor<32x64xf32, #warp> loc(#loc)
      %38 = tt.advance %arg8, [%c0_i32, %c32_i32] : <tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>>, 1> loc(#loc)
      %39 = tt.advance %arg9, [%c32_i32, %c0_i32] : <tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>>, 1> loc(#loc)
      scf.yield %37, %38, %39 : tensor<32x64xf32, #warp>, !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #warp}>>, 1>, !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #warp}>>, 1> loc(#loc)
    } {triton_gpu.workload = 3 : i32} loc(#loc)
    %c4_i32_7 = arith.constant 4 : i32 loc(#loc)
    %26 = arith.divsi %1, %c4_i32_7 : i32 loc(#loc)
    %c8_i32_8 = arith.constant 8 : i32 loc(#loc)
    %27 = arith.remsi %26, %c8_i32_8 : i32 loc(#loc)
    %c32_i32_9 = arith.constant 32 : i32 loc(#loc)
    %28 = arith.muli %27, %c32_i32_9 : i32 loc(#loc)
    %29 = arith.addi %28, %11 : i32 loc(#loc)
    %c4_i32_10 = arith.constant 4 : i32 loc(#loc)
    %30 = arith.remsi %1, %c4_i32_10 : i32 loc(#loc)
    %c4_i32_11 = arith.constant 4 : i32 loc(#loc)
    %31 = arith.remsi %30, %c4_i32_11 : i32 loc(#loc)
    %c64_i32_12 = arith.constant 64 : i32 loc(#loc)
    %32 = arith.muli %31, %c64_i32_12 : i32 loc(#loc)
    %33 = arith.addi %32, %18 : i32 loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%29, %33] {order = array<i32: 1, 0>} : <tensor<32x64xf32, #warp>, 1> loc(#loc)
    tt.store %34, %25#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<32x64xf32, #warp>, 1>, tensor<32x64xf32, #warp> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %0 = gpu.subgroup_id : index
    %1 = arith.index_cast %0 : index to i32
    %c64_i32 = arith.constant 64 : i32
    %c16_i32 = arith.constant 16 : i32
    %c4096_i32 = arith.constant 4096 : i32
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_6 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_7 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_8 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_9 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_11 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_12 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_13 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %cst_14 = arith.constant dense<0.000000e+00> : tensor<8x16xf32>
    %2 = tt.glue %cst, %cst_0, %cst_1, %cst_2, %cst_3, %cst_4, %cst_5, %cst_6, %cst_7, %cst_8, %cst_9, %cst_10, %cst_11, %cst_12, %cst_13, %cst_14 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32> -> tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i64 = arith.constant 1 : i64
    %c4096_i64 = arith.constant 4096 : i64
    %c256_i32 = arith.constant 256 : i32
    %c4_i32 = arith.constant 4 : i32
    %3 = tt.get_program_id x : i32
    %4 = arith.divsi %3, %c64_i32 : i32
    %5 = arith.muli %4, %c4_i32 : i32
    %6 = arith.subi %c16_i32, %5 : i32
    %7 = arith.minsi %6, %c4_i32 : i32
    %8 = arith.remsi %3, %7 : i32
    %9 = arith.addi %5, %8 : i32
    %10 = arith.remsi %3, %c64_i32 : i32
    %11 = arith.divsi %10, %7 : i32
    %12 = arith.muli %9, %c256_i32 : i32
    %c4_i32_15 = arith.constant 4 : i32
    %13 = arith.divsi %1, %c4_i32_15 : i32
    %c8_i32 = arith.constant 8 : i32
    %14 = arith.remsi %13, %c8_i32 : i32
    %c32_i32_16 = arith.constant 32 : i32
    %15 = arith.muli %14, %c32_i32_16 : i32
    %16 = arith.addi %15, %12 : i32
    %c4_i32_17 = arith.constant 4 : i32
    %17 = arith.remsi %1, %c4_i32_17 : i32
    %18 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%16, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1>
    %19 = tt.glue %18 : !tt.ptr<tensor<32x32xf16>, 1> -> !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>
    %20 = arith.muli %11, %c256_i32 : i32
    %c4_i32_18 = arith.constant 4 : i32
    %21 = arith.divsi %1, %c4_i32_18 : i32
    %c4_i32_19 = arith.constant 4 : i32
    %22 = arith.remsi %1, %c4_i32_19 : i32
    %c4_i32_20 = arith.constant 4 : i32
    %23 = arith.remsi %22, %c4_i32_20 : i32
    %c64_i32_21 = arith.constant 64 : i32
    %24 = arith.muli %23, %c64_i32_21 : i32
    %25 = arith.addi %24, %20 : i32
    %26 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %25] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1>
    %c32_i32_22 = arith.constant 32 : i32
    %27 = arith.addi %25, %c32_i32_22 : i32
    %28 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %27] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1>
    %29 = tt.glue %26, %28 : !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> -> !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>
    %30:3 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %2, %arg8 = %19, %arg9 = %29) -> (tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>, !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>)  : i32 {
      %112 = tt.extract %arg8, 0 : !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1> -> !tt.ptr<tensor<32x32xf16>, 1>
      %113 = tt.load %112 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16>
      %114 = tt.glue %113 : tensor<32x32xf16> -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>
      %115 = tt.extract %arg9, 0 : !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1> -> !tt.ptr<tensor<32x32xf16>, 1>
      %116 = tt.load %115 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16>
      %117 = tt.extract %arg9, 1 : !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1> -> !tt.ptr<tensor<32x32xf16>, 1>
      %118 = tt.load %117 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16>
      %119 = tt.glue %116, %118 : tensor<32x32xf16>, tensor<32x32xf16> -> tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>
      %120 = tt.extract %arg7, 0 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %121 = tt.extract %120, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %122 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %123 = tt.extract %122, 0 : tensor<32x32xf16> -> tensor<8x16xf16>
      %124 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %125 = tt.extract %124, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %126 = tt.dot %123, %125, %121 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %127 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %128 = tt.extract %127, 4 : tensor<32x32xf16> -> tensor<8x16xf16>
      %129 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %130 = tt.extract %129, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %131 = tt.dot %128, %130, %126 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %132 = tt.glue %131 : tensor<8x16xf32> -> tensor<8x16xf32>
      %133 = tt.extract %arg7, 1 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %134 = tt.extract %133, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %135 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %136 = tt.extract %135, 1 : tensor<32x32xf16> -> tensor<8x16xf16>
      %137 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %138 = tt.extract %137, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %139 = tt.dot %136, %138, %134 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %140 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %141 = tt.extract %140, 5 : tensor<32x32xf16> -> tensor<8x16xf16>
      %142 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %143 = tt.extract %142, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %144 = tt.dot %141, %143, %139 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %145 = tt.glue %144 : tensor<8x16xf32> -> tensor<8x16xf32>
      %146 = tt.extract %arg7, 2 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %147 = tt.extract %146, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %148 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %149 = tt.extract %148, 2 : tensor<32x32xf16> -> tensor<8x16xf16>
      %150 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %151 = tt.extract %150, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %152 = tt.dot %149, %151, %147 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %153 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %154 = tt.extract %153, 6 : tensor<32x32xf16> -> tensor<8x16xf16>
      %155 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %156 = tt.extract %155, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %157 = tt.dot %154, %156, %152 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %158 = tt.glue %157 : tensor<8x16xf32> -> tensor<8x16xf32>
      %159 = tt.extract %arg7, 3 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %160 = tt.extract %159, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %161 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %162 = tt.extract %161, 3 : tensor<32x32xf16> -> tensor<8x16xf16>
      %163 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %164 = tt.extract %163, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %165 = tt.dot %162, %164, %160 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %166 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %167 = tt.extract %166, 7 : tensor<32x32xf16> -> tensor<8x16xf16>
      %168 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %169 = tt.extract %168, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %170 = tt.dot %167, %169, %165 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %171 = tt.glue %170 : tensor<8x16xf32> -> tensor<8x16xf32>
      %172 = tt.extract %arg7, 4 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %173 = tt.extract %172, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %174 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %175 = tt.extract %174, 0 : tensor<32x32xf16> -> tensor<8x16xf16>
      %176 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %177 = tt.extract %176, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %178 = tt.dot %175, %177, %173 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %179 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %180 = tt.extract %179, 4 : tensor<32x32xf16> -> tensor<8x16xf16>
      %181 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %182 = tt.extract %181, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %183 = tt.dot %180, %182, %178 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %184 = tt.glue %183 : tensor<8x16xf32> -> tensor<8x16xf32>
      %185 = tt.extract %arg7, 5 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %186 = tt.extract %185, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %187 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %188 = tt.extract %187, 1 : tensor<32x32xf16> -> tensor<8x16xf16>
      %189 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %190 = tt.extract %189, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %191 = tt.dot %188, %190, %186 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %192 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %193 = tt.extract %192, 5 : tensor<32x32xf16> -> tensor<8x16xf16>
      %194 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %195 = tt.extract %194, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %196 = tt.dot %193, %195, %191 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %197 = tt.glue %196 : tensor<8x16xf32> -> tensor<8x16xf32>
      %198 = tt.extract %arg7, 6 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %199 = tt.extract %198, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %200 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %201 = tt.extract %200, 2 : tensor<32x32xf16> -> tensor<8x16xf16>
      %202 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %203 = tt.extract %202, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %204 = tt.dot %201, %203, %199 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %205 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %206 = tt.extract %205, 6 : tensor<32x32xf16> -> tensor<8x16xf16>
      %207 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %208 = tt.extract %207, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %209 = tt.dot %206, %208, %204 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %210 = tt.glue %209 : tensor<8x16xf32> -> tensor<8x16xf32>
      %211 = tt.extract %arg7, 7 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %212 = tt.extract %211, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %213 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %214 = tt.extract %213, 3 : tensor<32x32xf16> -> tensor<8x16xf16>
      %215 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %216 = tt.extract %215, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %217 = tt.dot %214, %216, %212 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %218 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %219 = tt.extract %218, 7 : tensor<32x32xf16> -> tensor<8x16xf16>
      %220 = tt.extract %119, 0 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %221 = tt.extract %220, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %222 = tt.dot %219, %221, %217 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %223 = tt.glue %222 : tensor<8x16xf32> -> tensor<8x16xf32>
      %224 = tt.extract %arg7, 8 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %225 = tt.extract %224, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %226 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %227 = tt.extract %226, 0 : tensor<32x32xf16> -> tensor<8x16xf16>
      %228 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %229 = tt.extract %228, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %230 = tt.dot %227, %229, %225 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %231 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %232 = tt.extract %231, 4 : tensor<32x32xf16> -> tensor<8x16xf16>
      %233 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %234 = tt.extract %233, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %235 = tt.dot %232, %234, %230 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %236 = tt.glue %235 : tensor<8x16xf32> -> tensor<8x16xf32>
      %237 = tt.extract %arg7, 9 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %238 = tt.extract %237, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %239 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %240 = tt.extract %239, 1 : tensor<32x32xf16> -> tensor<8x16xf16>
      %241 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %242 = tt.extract %241, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %243 = tt.dot %240, %242, %238 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %244 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %245 = tt.extract %244, 5 : tensor<32x32xf16> -> tensor<8x16xf16>
      %246 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %247 = tt.extract %246, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %248 = tt.dot %245, %247, %243 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %249 = tt.glue %248 : tensor<8x16xf32> -> tensor<8x16xf32>
      %250 = tt.extract %arg7, 10 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %251 = tt.extract %250, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %252 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %253 = tt.extract %252, 2 : tensor<32x32xf16> -> tensor<8x16xf16>
      %254 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %255 = tt.extract %254, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %256 = tt.dot %253, %255, %251 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %257 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %258 = tt.extract %257, 6 : tensor<32x32xf16> -> tensor<8x16xf16>
      %259 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %260 = tt.extract %259, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %261 = tt.dot %258, %260, %256 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %262 = tt.glue %261 : tensor<8x16xf32> -> tensor<8x16xf32>
      %263 = tt.extract %arg7, 11 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %264 = tt.extract %263, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %265 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %266 = tt.extract %265, 3 : tensor<32x32xf16> -> tensor<8x16xf16>
      %267 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %268 = tt.extract %267, 0 : tensor<32x32xf16> -> tensor<16x16xf16>
      %269 = tt.dot %266, %268, %264 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %270 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %271 = tt.extract %270, 7 : tensor<32x32xf16> -> tensor<8x16xf16>
      %272 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %273 = tt.extract %272, 1 : tensor<32x32xf16> -> tensor<16x16xf16>
      %274 = tt.dot %271, %273, %269 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %275 = tt.glue %274 : tensor<8x16xf32> -> tensor<8x16xf32>
      %276 = tt.extract %arg7, 12 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %277 = tt.extract %276, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %278 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %279 = tt.extract %278, 0 : tensor<32x32xf16> -> tensor<8x16xf16>
      %280 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %281 = tt.extract %280, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %282 = tt.dot %279, %281, %277 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %283 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %284 = tt.extract %283, 4 : tensor<32x32xf16> -> tensor<8x16xf16>
      %285 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %286 = tt.extract %285, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %287 = tt.dot %284, %286, %282 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %288 = tt.glue %287 : tensor<8x16xf32> -> tensor<8x16xf32>
      %289 = tt.extract %arg7, 13 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %290 = tt.extract %289, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %291 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %292 = tt.extract %291, 1 : tensor<32x32xf16> -> tensor<8x16xf16>
      %293 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %294 = tt.extract %293, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %295 = tt.dot %292, %294, %290 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %296 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %297 = tt.extract %296, 5 : tensor<32x32xf16> -> tensor<8x16xf16>
      %298 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %299 = tt.extract %298, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %300 = tt.dot %297, %299, %295 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %301 = tt.glue %300 : tensor<8x16xf32> -> tensor<8x16xf32>
      %302 = tt.extract %arg7, 14 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %303 = tt.extract %302, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %304 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %305 = tt.extract %304, 2 : tensor<32x32xf16> -> tensor<8x16xf16>
      %306 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %307 = tt.extract %306, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %308 = tt.dot %305, %307, %303 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %309 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %310 = tt.extract %309, 6 : tensor<32x32xf16> -> tensor<8x16xf16>
      %311 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %312 = tt.extract %311, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %313 = tt.dot %310, %312, %308 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %314 = tt.glue %313 : tensor<8x16xf32> -> tensor<8x16xf32>
      %315 = tt.extract %arg7, 15 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
      %316 = tt.extract %315, 0 : tensor<8x16xf32> -> tensor<8x16xf32>
      %317 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %318 = tt.extract %317, 3 : tensor<32x32xf16> -> tensor<8x16xf16>
      %319 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %320 = tt.extract %319, 2 : tensor<32x32xf16> -> tensor<16x16xf16>
      %321 = tt.dot %318, %320, %316 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %322 = tt.extract %114, 0 : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %323 = tt.extract %322, 7 : tensor<32x32xf16> -> tensor<8x16xf16>
      %324 = tt.extract %119, 1 : tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>> -> tensor<32x32xf16>
      %325 = tt.extract %324, 3 : tensor<32x32xf16> -> tensor<16x16xf16>
      %326 = tt.dot %323, %325, %321 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32>
      %327 = tt.glue %326 : tensor<8x16xf32> -> tensor<8x16xf32>
      %328 = tt.glue %132, %145, %158, %171, %184, %197, %210, %223, %236, %249, %262, %275, %288, %301, %314, %327 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32> -> tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>
      %329 = tt.extract %arg8, 0 : !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1> -> !tt.ptr<tensor<32x32xf16>, 1>
      %330 = tt.advance %329, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1>
      %331 = tt.glue %330 : !tt.ptr<tensor<32x32xf16>, 1> -> !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>
      %332 = tt.extract %arg9, 0 : !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1> -> !tt.ptr<tensor<32x32xf16>, 1>
      %333 = tt.advance %332, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1>
      %334 = tt.extract %arg9, 1 : !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1> -> !tt.ptr<tensor<32x32xf16>, 1>
      %335 = tt.advance %334, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1>
      %336 = tt.glue %333, %335 : !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> -> !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>
      scf.yield %328, %331, %336 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, !tt.ptr<tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>, !tt.ptr<tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>}>>, 1>
    } {triton_gpu.workload = 3 : i32}
    %c4_i32_23 = arith.constant 4 : i32
    %31 = arith.divsi %1, %c4_i32_23 : i32
    %c8_i32_24 = arith.constant 8 : i32
    %32 = arith.remsi %31, %c8_i32_24 : i32
    %c32_i32_25 = arith.constant 32 : i32
    %33 = arith.muli %32, %c32_i32_25 : i32
    %34 = arith.addi %33, %12 : i32
    %c4_i32_26 = arith.constant 4 : i32
    %35 = arith.remsi %1, %c4_i32_26 : i32
    %c4_i32_27 = arith.constant 4 : i32
    %36 = arith.remsi %35, %c4_i32_27 : i32
    %c64_i32_28 = arith.constant 64 : i32
    %37 = arith.muli %36, %c64_i32_28 : i32
    %38 = arith.addi %37, %20 : i32
    %39 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%34, %38] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c8_i32_29 = arith.constant 8 : i32
    %40 = arith.addi %34, %c8_i32_29 : i32
    %41 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%40, %38] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c16_i32_30 = arith.constant 16 : i32
    %42 = arith.addi %34, %c16_i32_30 : i32
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%42, %38] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c24_i32 = arith.constant 24 : i32
    %44 = arith.addi %34, %c24_i32 : i32
    %45 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%44, %38] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c16_i32_31 = arith.constant 16 : i32
    %46 = arith.addi %38, %c16_i32_31 : i32
    %47 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%34, %46] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c8_i32_32 = arith.constant 8 : i32
    %48 = arith.addi %34, %c8_i32_32 : i32
    %c16_i32_33 = arith.constant 16 : i32
    %49 = arith.addi %38, %c16_i32_33 : i32
    %50 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%48, %49] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c16_i32_34 = arith.constant 16 : i32
    %51 = arith.addi %34, %c16_i32_34 : i32
    %c16_i32_35 = arith.constant 16 : i32
    %52 = arith.addi %38, %c16_i32_35 : i32
    %53 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%51, %52] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c24_i32_36 = arith.constant 24 : i32
    %54 = arith.addi %34, %c24_i32_36 : i32
    %c16_i32_37 = arith.constant 16 : i32
    %55 = arith.addi %38, %c16_i32_37 : i32
    %56 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%54, %55] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c32_i32_38 = arith.constant 32 : i32
    %57 = arith.addi %38, %c32_i32_38 : i32
    %58 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%34, %57] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c8_i32_39 = arith.constant 8 : i32
    %59 = arith.addi %34, %c8_i32_39 : i32
    %c32_i32_40 = arith.constant 32 : i32
    %60 = arith.addi %38, %c32_i32_40 : i32
    %61 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%59, %60] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c16_i32_41 = arith.constant 16 : i32
    %62 = arith.addi %34, %c16_i32_41 : i32
    %c32_i32_42 = arith.constant 32 : i32
    %63 = arith.addi %38, %c32_i32_42 : i32
    %64 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%62, %63] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c24_i32_43 = arith.constant 24 : i32
    %65 = arith.addi %34, %c24_i32_43 : i32
    %c32_i32_44 = arith.constant 32 : i32
    %66 = arith.addi %38, %c32_i32_44 : i32
    %67 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%65, %66] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c48_i32 = arith.constant 48 : i32
    %68 = arith.addi %38, %c48_i32 : i32
    %69 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%34, %68] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c8_i32_45 = arith.constant 8 : i32
    %70 = arith.addi %34, %c8_i32_45 : i32
    %c48_i32_46 = arith.constant 48 : i32
    %71 = arith.addi %38, %c48_i32_46 : i32
    %72 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%70, %71] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c16_i32_47 = arith.constant 16 : i32
    %73 = arith.addi %34, %c16_i32_47 : i32
    %c48_i32_48 = arith.constant 48 : i32
    %74 = arith.addi %38, %c48_i32_48 : i32
    %75 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%73, %74] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %c24_i32_49 = arith.constant 24 : i32
    %76 = arith.addi %34, %c24_i32_49 : i32
    %c48_i32_50 = arith.constant 48 : i32
    %77 = arith.addi %38, %c48_i32_50 : i32
    %78 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%76, %77] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1>
    %79 = tt.glue %39, %41, %43, %45, %47, %50, %53, %56, %58, %61, %64, %67, %69, %72, %75, %78 : !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1>, !tt.ptr<tensor<8x16xf32>, 1> -> !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1>
    %80 = tt.extract %79, 0 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %81 = tt.extract %30#0, 0 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %80, %81 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %82 = tt.extract %79, 1 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %83 = tt.extract %30#0, 1 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %82, %83 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %84 = tt.extract %79, 2 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %85 = tt.extract %30#0, 2 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %84, %85 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %86 = tt.extract %79, 3 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %87 = tt.extract %30#0, 3 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %86, %87 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %88 = tt.extract %79, 4 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %89 = tt.extract %30#0, 4 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %88, %89 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %90 = tt.extract %79, 5 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %91 = tt.extract %30#0, 5 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %90, %91 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %92 = tt.extract %79, 6 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %93 = tt.extract %30#0, 6 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %92, %93 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %94 = tt.extract %79, 7 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %95 = tt.extract %30#0, 7 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %94, %95 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %96 = tt.extract %79, 8 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %97 = tt.extract %30#0, 8 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %96, %97 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %98 = tt.extract %79, 9 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %99 = tt.extract %30#0, 9 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %98, %99 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %100 = tt.extract %79, 10 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %101 = tt.extract %30#0, 10 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %100, %101 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %102 = tt.extract %79, 11 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %103 = tt.extract %30#0, 11 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %102, %103 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %104 = tt.extract %79, 12 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %105 = tt.extract %30#0, 12 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %104, %105 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %106 = tt.extract %79, 13 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %107 = tt.extract %30#0, 13 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %106, %107 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %108 = tt.extract %79, 14 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %109 = tt.extract %30#0, 14 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %108, %109 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    %110 = tt.extract %79, 15 : !tt.ptr<tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>>, 1> -> !tt.ptr<tensor<8x16xf32>, 1>
    %111 = tt.extract %30#0, 15 : tensor<32x64xf32, #triton_gpu.warp<{sizePerThread = [32, 64], threadsPerWarp = [1, 1], order = [1, 0]}>> -> tensor<8x16xf32>
    tt.store %110, %111 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>
    tt.return
  }
}
// -----// IR Dump Before TritonGPUPrepareGenxLsc (tritongpu-prepare-genxlsc) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %24:19 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %cst, %arg9 = %cst, %arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %cst, %arg19 = %cst, %arg20 = %cst, %arg21 = %cst, %arg22 = %cst, %arg23 = %16, %arg24 = %21, %arg25 = %23) -> (tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>)  : i32 {
      %72 = tt.load %arg23 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %73 = tt.load %arg24 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %74 = tt.load %arg25 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %75 = tt.extract %72, 0 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %76 = tt.extract %73, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %77 = tt.dot %75, %76, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %78 = tt.extract %72, 4 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %79 = tt.extract %73, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %80 = tt.dot %78, %79, %77 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %81 = tt.glue %80 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %82 = tt.extract %72, 1 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %83 = tt.extract %73, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %84 = tt.dot %82, %83, %arg8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %85 = tt.extract %72, 5 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %86 = tt.extract %73, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %87 = tt.dot %85, %86, %84 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %88 = tt.glue %87 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %89 = tt.extract %72, 2 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %90 = tt.extract %73, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %91 = tt.dot %89, %90, %arg9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %92 = tt.extract %72, 6 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %93 = tt.extract %73, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %94 = tt.dot %92, %93, %91 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %95 = tt.glue %94 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %96 = tt.extract %72, 3 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %97 = tt.extract %73, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %98 = tt.dot %96, %97, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %99 = tt.extract %72, 7 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %100 = tt.extract %73, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %101 = tt.dot %99, %100, %98 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %102 = tt.glue %101 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %103 = tt.extract %72, 0 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %104 = tt.extract %73, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %105 = tt.dot %103, %104, %arg11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %106 = tt.extract %72, 4 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %107 = tt.extract %73, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %108 = tt.dot %106, %107, %105 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %109 = tt.glue %108 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %110 = tt.extract %72, 1 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %111 = tt.extract %73, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %112 = tt.dot %110, %111, %arg12 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %113 = tt.extract %72, 5 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %114 = tt.extract %73, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %115 = tt.dot %113, %114, %112 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %116 = tt.glue %115 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %117 = tt.extract %72, 2 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %118 = tt.extract %73, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %119 = tt.dot %117, %118, %arg13 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %120 = tt.extract %72, 6 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %121 = tt.extract %73, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %122 = tt.dot %120, %121, %119 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %123 = tt.glue %122 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %124 = tt.extract %72, 3 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %125 = tt.extract %73, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %126 = tt.dot %124, %125, %arg14 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %127 = tt.extract %72, 7 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %128 = tt.extract %73, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %129 = tt.dot %127, %128, %126 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %130 = tt.glue %129 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %131 = tt.extract %72, 0 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %132 = tt.extract %74, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %133 = tt.dot %131, %132, %arg15 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %134 = tt.extract %72, 4 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %135 = tt.extract %74, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %136 = tt.dot %134, %135, %133 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %137 = tt.glue %136 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %138 = tt.extract %72, 1 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %139 = tt.extract %74, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %140 = tt.dot %138, %139, %arg16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %141 = tt.extract %72, 5 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %142 = tt.extract %74, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %143 = tt.dot %141, %142, %140 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %144 = tt.glue %143 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %145 = tt.extract %72, 2 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %146 = tt.extract %74, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %147 = tt.dot %145, %146, %arg17 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %148 = tt.extract %72, 6 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %149 = tt.extract %74, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %150 = tt.dot %148, %149, %147 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %151 = tt.glue %150 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %152 = tt.extract %72, 3 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %153 = tt.extract %74, 0 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %154 = tt.dot %152, %153, %arg18 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %155 = tt.extract %72, 7 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %156 = tt.extract %74, 1 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %157 = tt.dot %155, %156, %154 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %158 = tt.glue %157 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %159 = tt.extract %72, 0 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %160 = tt.extract %74, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %161 = tt.dot %159, %160, %arg19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %162 = tt.extract %72, 4 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %163 = tt.extract %74, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %164 = tt.dot %162, %163, %161 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %165 = tt.glue %164 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %166 = tt.extract %72, 1 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %167 = tt.extract %74, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %168 = tt.dot %166, %167, %arg20 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %169 = tt.extract %72, 5 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %170 = tt.extract %74, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %171 = tt.dot %169, %170, %168 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %172 = tt.glue %171 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %173 = tt.extract %72, 2 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %174 = tt.extract %74, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %175 = tt.dot %173, %174, %arg21 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %176 = tt.extract %72, 6 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %177 = tt.extract %74, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %178 = tt.dot %176, %177, %175 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %179 = tt.glue %178 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %180 = tt.extract %72, 3 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %181 = tt.extract %74, 2 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %182 = tt.dot %180, %181, %arg22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %183 = tt.extract %72, 7 : tensor<32x32xf16> -> tensor<8x16xf16> loc(#loc)
      %184 = tt.extract %74, 3 : tensor<32x32xf16> -> tensor<16x16xf16> loc(#loc)
      %185 = tt.dot %183, %184, %182 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %186 = tt.glue %185 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %187 = tt.advance %arg23, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %188 = tt.advance %arg24, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %189 = tt.advance %arg25, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      scf.yield %81, %88, %95, %102, %109, %116, %123, %130, %137, %144, %151, %158, %165, %172, %179, %186, %187, %188, %189 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    } loc(#loc)
    %25 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %26 = arith.andi %25, %c7_i32 : i32 loc(#loc)
    %27 = arith.muli %26, %c32_i32 : i32 loc(#loc)
    %28 = arith.addi %27, %11 : i32 loc(#loc)
    %29 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %30 = arith.muli %29, %c64_i32 : i32 loc(#loc)
    %31 = arith.addi %30, %17 : i32 loc(#loc)
    %32 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %33 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%33, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %35 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %36 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%35, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %37 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %38 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%37, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %39 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %40 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %39] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %41 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %42 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%41, %42] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %44 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %45 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %46 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%44, %45] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %47 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %48 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %49 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%47, %48] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %50 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %51 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %50] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %52 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %53 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %54 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%52, %53] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %55 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %56 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %57 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%55, %56] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %58 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %59 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %60 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%58, %59] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %61 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %62 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %61] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %63 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %64 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %65 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%63, %64] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %66 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %67 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %68 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%66, %67] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %69 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %70 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %71 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%69, %70] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %32, %24#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %34, %24#1 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %36, %24#2 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %38, %24#3 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %40, %24#4 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %43, %24#5 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %46, %24#6 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %49, %24#7 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %51, %24#8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %54, %24#9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %57, %24#10 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %60, %24#11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %62, %24#12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %65, %24#13 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %68, %24#14 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %71, %24#15 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %24:19 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %cst, %arg9 = %cst, %arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %cst, %arg19 = %cst, %arg20 = %cst, %arg21 = %cst, %arg22 = %cst, %arg23 = %16, %arg24 = %21, %arg25 = %23) -> (tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>)  : i32 {
      %72 = tt.load %arg23 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %73 = tt.load %arg24 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %74 = tt.load %arg25 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %75 = tt.cast %72 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
      %76 = tt.extract %75, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %77 = tt.cast %76 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %78 = tt.cast %73 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %79 = tt.extract %78, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %80 = tt.cast %79 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %81 = tt.dot %77, %80, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %82 = tt.extract %75, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %83 = tt.cast %82 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %84 = tt.extract %78, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %85 = tt.cast %84 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %86 = tt.dot %83, %85, %81 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %87 = tt.glue %86 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %88 = tt.extract %75, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %89 = tt.cast %88 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %90 = tt.extract %78, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %91 = tt.cast %90 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %92 = tt.dot %89, %91, %arg8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %93 = tt.extract %75, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %94 = tt.cast %93 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %95 = tt.extract %78, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %96 = tt.cast %95 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %97 = tt.dot %94, %96, %92 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %98 = tt.glue %97 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %99 = tt.extract %75, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %100 = tt.cast %99 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %101 = tt.extract %78, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %102 = tt.cast %101 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %103 = tt.dot %100, %102, %arg9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %104 = tt.extract %75, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %105 = tt.cast %104 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %106 = tt.extract %78, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %107 = tt.cast %106 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %108 = tt.dot %105, %107, %103 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %109 = tt.glue %108 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %110 = tt.extract %75, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %111 = tt.cast %110 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %112 = tt.extract %78, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %113 = tt.cast %112 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %114 = tt.dot %111, %113, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %115 = tt.extract %75, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %116 = tt.cast %115 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %117 = tt.extract %78, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %118 = tt.cast %117 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %119 = tt.dot %116, %118, %114 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %120 = tt.glue %119 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %121 = tt.extract %75, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %122 = tt.cast %121 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %123 = tt.extract %78, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %124 = tt.cast %123 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %125 = tt.dot %122, %124, %arg11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %126 = tt.extract %75, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %127 = tt.cast %126 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %128 = tt.extract %78, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %129 = tt.cast %128 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %130 = tt.dot %127, %129, %125 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %131 = tt.glue %130 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %132 = tt.extract %75, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %133 = tt.cast %132 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %134 = tt.extract %78, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %135 = tt.cast %134 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %136 = tt.dot %133, %135, %arg12 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %137 = tt.extract %75, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %138 = tt.cast %137 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %139 = tt.extract %78, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %140 = tt.cast %139 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %141 = tt.dot %138, %140, %136 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %142 = tt.glue %141 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %143 = tt.extract %75, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %144 = tt.cast %143 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %145 = tt.extract %78, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %146 = tt.cast %145 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %147 = tt.dot %144, %146, %arg13 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %148 = tt.extract %75, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %149 = tt.cast %148 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %150 = tt.extract %78, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %151 = tt.cast %150 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %152 = tt.dot %149, %151, %147 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %153 = tt.glue %152 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %154 = tt.extract %75, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %155 = tt.cast %154 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %156 = tt.extract %78, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %157 = tt.cast %156 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %158 = tt.dot %155, %157, %arg14 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %159 = tt.extract %75, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %160 = tt.cast %159 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %161 = tt.extract %78, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %162 = tt.cast %161 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %163 = tt.dot %160, %162, %158 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %164 = tt.glue %163 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %165 = tt.extract %75, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %166 = tt.cast %165 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %167 = tt.cast %74 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %168 = tt.extract %167, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %169 = tt.cast %168 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %170 = tt.dot %166, %169, %arg15 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %171 = tt.extract %75, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %172 = tt.cast %171 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %173 = tt.extract %167, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %174 = tt.cast %173 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %175 = tt.dot %172, %174, %170 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %176 = tt.glue %175 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %177 = tt.extract %75, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %178 = tt.cast %177 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %179 = tt.extract %167, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %180 = tt.cast %179 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %181 = tt.dot %178, %180, %arg16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %182 = tt.extract %75, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %183 = tt.cast %182 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %184 = tt.extract %167, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %185 = tt.cast %184 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %186 = tt.dot %183, %185, %181 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %187 = tt.glue %186 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %188 = tt.extract %75, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %189 = tt.cast %188 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %190 = tt.extract %167, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %191 = tt.cast %190 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %192 = tt.dot %189, %191, %arg17 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %193 = tt.extract %75, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %194 = tt.cast %193 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %195 = tt.extract %167, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %196 = tt.cast %195 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %197 = tt.dot %194, %196, %192 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %198 = tt.glue %197 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %199 = tt.extract %75, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %200 = tt.cast %199 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %201 = tt.extract %167, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %202 = tt.cast %201 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %203 = tt.dot %200, %202, %arg18 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %204 = tt.extract %75, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %205 = tt.cast %204 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %206 = tt.extract %167, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %207 = tt.cast %206 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %208 = tt.dot %205, %207, %203 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %209 = tt.glue %208 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %210 = tt.extract %75, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %211 = tt.cast %210 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %212 = tt.extract %167, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %213 = tt.cast %212 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %214 = tt.dot %211, %213, %arg19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %215 = tt.extract %75, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %216 = tt.cast %215 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %217 = tt.extract %167, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %218 = tt.cast %217 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %219 = tt.dot %216, %218, %214 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %220 = tt.glue %219 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %221 = tt.extract %75, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %222 = tt.cast %221 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %223 = tt.extract %167, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %224 = tt.cast %223 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %225 = tt.dot %222, %224, %arg20 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %226 = tt.extract %75, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %227 = tt.cast %226 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %228 = tt.extract %167, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %229 = tt.cast %228 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %230 = tt.dot %227, %229, %225 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %231 = tt.glue %230 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %232 = tt.extract %75, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %233 = tt.cast %232 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %234 = tt.extract %167, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %235 = tt.cast %234 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %236 = tt.dot %233, %235, %arg21 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %237 = tt.extract %75, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %238 = tt.cast %237 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %239 = tt.extract %167, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %240 = tt.cast %239 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %241 = tt.dot %238, %240, %236 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %242 = tt.glue %241 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %243 = tt.extract %75, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %244 = tt.cast %243 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %245 = tt.extract %167, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %246 = tt.cast %245 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %247 = tt.dot %244, %246, %arg22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %248 = tt.extract %75, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %249 = tt.cast %248 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %250 = tt.extract %167, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %251 = tt.cast %250 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %252 = tt.dot %249, %251, %247 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %253 = tt.glue %252 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %254 = tt.advance %arg23, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %255 = tt.advance %arg24, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %256 = tt.advance %arg25, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      scf.yield %87, %98, %109, %120, %131, %142, %153, %164, %176, %187, %198, %209, %220, %231, %242, %253, %254, %255, %256 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    } loc(#loc)
    %25 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %26 = arith.andi %25, %c7_i32 : i32 loc(#loc)
    %27 = arith.muli %26, %c32_i32 : i32 loc(#loc)
    %28 = arith.addi %27, %11 : i32 loc(#loc)
    %29 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %30 = arith.muli %29, %c64_i32 : i32 loc(#loc)
    %31 = arith.addi %30, %17 : i32 loc(#loc)
    %32 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %33 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%33, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %35 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %36 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%35, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %37 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %38 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%37, %31] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %39 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %40 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %39] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %41 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %42 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%41, %42] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %44 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %45 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %46 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%44, %45] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %47 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %48 = arith.addi %31, %c16_i32 : i32 loc(#loc)
    %49 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%47, %48] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %50 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %51 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %50] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %52 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %53 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %54 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%52, %53] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %55 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %56 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %57 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%55, %56] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %58 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %59 = arith.addi %31, %c32_i32 : i32 loc(#loc)
    %60 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%58, %59] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %61 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %62 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %61] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %63 = arith.addi %28, %c8_i32 : i32 loc(#loc)
    %64 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %65 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%63, %64] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %66 = arith.addi %28, %c16_i32 : i32 loc(#loc)
    %67 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %68 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%66, %67] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %69 = arith.addi %28, %c24_i32 : i32 loc(#loc)
    %70 = arith.addi %31, %c48_i32 : i32 loc(#loc)
    %71 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%69, %70] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %32, %24#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %34, %24#1 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %36, %24#2 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %38, %24#3 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %40, %24#4 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %43, %24#5 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %46, %24#6 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %49, %24#7 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %51, %24#8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %54, %24#9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %57, %24#10 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %60, %24#11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %62, %24#12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %65, %24#13 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %68, %24#14 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %71, %24#15 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %24:19 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %cst, %arg9 = %cst, %arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %cst, %arg19 = %cst, %arg20 = %cst, %arg21 = %cst, %arg22 = %cst, %arg23 = %16, %arg24 = %21, %arg25 = %23) -> (tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>)  : i32 {
      %46 = tt.load %arg23 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %47 = tt.load %arg24 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %48 = tt.load %arg25 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %49 = tt.cast %46 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
      %50 = tt.extract %49, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %51 = tt.cast %50 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %52 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %53 = tt.extract %52, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %54 = tt.cast %53 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %55 = tt.dot %51, %54, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %56 = tt.extract %49, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %57 = tt.cast %56 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %58 = tt.extract %52, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %59 = tt.cast %58 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %60 = tt.dot %57, %59, %55 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %61 = tt.glue %60 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %62 = tt.extract %49, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %63 = tt.cast %62 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %64 = tt.dot %63, %54, %arg8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %65 = tt.extract %49, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %66 = tt.cast %65 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %67 = tt.dot %66, %59, %64 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %68 = tt.glue %67 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %69 = tt.extract %49, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %70 = tt.cast %69 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %71 = tt.dot %70, %54, %arg9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %72 = tt.extract %49, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %73 = tt.cast %72 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %74 = tt.dot %73, %59, %71 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %75 = tt.glue %74 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %76 = tt.extract %49, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %77 = tt.cast %76 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %78 = tt.dot %77, %54, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %79 = tt.extract %49, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %80 = tt.cast %79 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %81 = tt.dot %80, %59, %78 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %82 = tt.glue %81 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %83 = tt.extract %52, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %84 = tt.cast %83 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %85 = tt.dot %51, %84, %arg11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %86 = tt.extract %52, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %87 = tt.cast %86 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %88 = tt.dot %57, %87, %85 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %89 = tt.glue %88 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %90 = tt.dot %63, %84, %arg12 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %91 = tt.dot %66, %87, %90 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %92 = tt.glue %91 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %93 = tt.dot %70, %84, %arg13 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %94 = tt.dot %73, %87, %93 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %95 = tt.glue %94 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %96 = tt.dot %77, %84, %arg14 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %97 = tt.dot %80, %87, %96 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %98 = tt.glue %97 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %99 = tt.cast %48 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %100 = tt.extract %99, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %101 = tt.cast %100 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %102 = tt.dot %51, %101, %arg15 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %103 = tt.extract %99, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %104 = tt.cast %103 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %105 = tt.dot %57, %104, %102 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %106 = tt.glue %105 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %107 = tt.dot %63, %101, %arg16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %108 = tt.dot %66, %104, %107 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %109 = tt.glue %108 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %110 = tt.dot %70, %101, %arg17 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %111 = tt.dot %73, %104, %110 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %112 = tt.glue %111 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %113 = tt.dot %77, %101, %arg18 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %114 = tt.dot %80, %104, %113 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %115 = tt.glue %114 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %116 = tt.extract %99, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %117 = tt.cast %116 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %118 = tt.dot %51, %117, %arg19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %119 = tt.extract %99, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %120 = tt.cast %119 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %121 = tt.dot %57, %120, %118 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %122 = tt.glue %121 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %123 = tt.dot %63, %117, %arg20 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %124 = tt.dot %66, %120, %123 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %125 = tt.glue %124 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %126 = tt.dot %70, %117, %arg21 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %127 = tt.dot %73, %120, %126 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %128 = tt.glue %127 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %129 = tt.dot %77, %117, %arg22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %130 = tt.dot %80, %120, %129 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %131 = tt.glue %130 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %132 = tt.advance %arg23, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %133 = tt.advance %arg24, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %134 = tt.advance %arg25, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      scf.yield %61, %68, %75, %82, %89, %92, %95, %98, %106, %109, %112, %115, %122, %125, %128, %131, %132, %133, %134 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    } loc(#loc)
    %25 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %26 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %27 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %28 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %29 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %30 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %31 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %32 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %33 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %35 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %36 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %37 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %38 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %39 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %40 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %41 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %42 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %44 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %45 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %25, %24#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %27, %24#1 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %29, %24#2 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %31, %24#3 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %33, %24#4 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %34, %24#5 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %35, %24#6 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %36, %24#7 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %37, %24#8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %38, %24#9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %39, %24#10 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %40, %24#11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %42, %24#12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %43, %24#13 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %44, %24#14 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %45, %24#15 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before SymbolDCE (symbol-dce) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %24:19 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %cst, %arg9 = %cst, %arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %cst, %arg19 = %cst, %arg20 = %cst, %arg21 = %cst, %arg22 = %cst, %arg23 = %16, %arg24 = %21, %arg25 = %23) -> (tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>)  : i32 {
      %46 = tt.load %arg23 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %47 = tt.load %arg24 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %48 = tt.load %arg25 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %49 = tt.cast %46 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
      %50 = tt.extract %49, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %51 = tt.cast %50 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %52 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %53 = tt.extract %52, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %54 = tt.cast %53 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %55 = tt.dot %51, %54, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %56 = tt.extract %49, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %57 = tt.cast %56 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %58 = tt.extract %52, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %59 = tt.cast %58 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %60 = tt.dot %57, %59, %55 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %61 = tt.glue %60 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %62 = tt.extract %49, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %63 = tt.cast %62 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %64 = tt.dot %63, %54, %arg8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %65 = tt.extract %49, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %66 = tt.cast %65 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %67 = tt.dot %66, %59, %64 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %68 = tt.glue %67 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %69 = tt.extract %49, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %70 = tt.cast %69 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %71 = tt.dot %70, %54, %arg9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %72 = tt.extract %49, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %73 = tt.cast %72 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %74 = tt.dot %73, %59, %71 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %75 = tt.glue %74 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %76 = tt.extract %49, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %77 = tt.cast %76 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %78 = tt.dot %77, %54, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %79 = tt.extract %49, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %80 = tt.cast %79 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %81 = tt.dot %80, %59, %78 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %82 = tt.glue %81 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %83 = tt.extract %52, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %84 = tt.cast %83 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %85 = tt.dot %51, %84, %arg11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %86 = tt.extract %52, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %87 = tt.cast %86 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %88 = tt.dot %57, %87, %85 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %89 = tt.glue %88 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %90 = tt.dot %63, %84, %arg12 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %91 = tt.dot %66, %87, %90 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %92 = tt.glue %91 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %93 = tt.dot %70, %84, %arg13 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %94 = tt.dot %73, %87, %93 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %95 = tt.glue %94 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %96 = tt.dot %77, %84, %arg14 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %97 = tt.dot %80, %87, %96 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %98 = tt.glue %97 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %99 = tt.cast %48 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %100 = tt.extract %99, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %101 = tt.cast %100 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %102 = tt.dot %51, %101, %arg15 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %103 = tt.extract %99, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %104 = tt.cast %103 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %105 = tt.dot %57, %104, %102 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %106 = tt.glue %105 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %107 = tt.dot %63, %101, %arg16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %108 = tt.dot %66, %104, %107 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %109 = tt.glue %108 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %110 = tt.dot %70, %101, %arg17 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %111 = tt.dot %73, %104, %110 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %112 = tt.glue %111 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %113 = tt.dot %77, %101, %arg18 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %114 = tt.dot %80, %104, %113 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %115 = tt.glue %114 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %116 = tt.extract %99, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %117 = tt.cast %116 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %118 = tt.dot %51, %117, %arg19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %119 = tt.extract %99, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %120 = tt.cast %119 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %121 = tt.dot %57, %120, %118 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %122 = tt.glue %121 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %123 = tt.dot %63, %117, %arg20 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %124 = tt.dot %66, %120, %123 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %125 = tt.glue %124 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %126 = tt.dot %70, %117, %arg21 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %127 = tt.dot %73, %120, %126 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %128 = tt.glue %127 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %129 = tt.dot %77, %117, %arg22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %130 = tt.dot %80, %120, %129 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %131 = tt.glue %130 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %132 = tt.advance %arg23, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %133 = tt.advance %arg24, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %134 = tt.advance %arg25, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      scf.yield %61, %68, %75, %82, %89, %92, %95, %98, %106, %109, %112, %115, %122, %125, %128, %131, %132, %133, %134 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    } loc(#loc)
    %25 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %26 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %27 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %28 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %29 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %30 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %31 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %32 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %33 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %35 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %36 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %37 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %38 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %39 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %40 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %41 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %42 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %44 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %45 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %25, %24#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %27, %24#1 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %29, %24#2 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %31, %24#3 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %33, %24#4 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %34, %24#5 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %35, %24#6 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %36, %24#7 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %37, %24#8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %38, %24#9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %39, %24#10 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %40, %24#11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %42, %24#12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %43, %24#13 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %44, %24#14 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %45, %24#15 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before DecomposeUnsupportedConversions (decompose-unsupported-conversions) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %24:19 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %cst, %arg9 = %cst, %arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %cst, %arg19 = %cst, %arg20 = %cst, %arg21 = %cst, %arg22 = %cst, %arg23 = %16, %arg24 = %21, %arg25 = %23) -> (tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>)  : i32 {
      %46 = tt.load %arg23 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %47 = tt.load %arg24 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %48 = tt.load %arg25 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %49 = tt.cast %46 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
      %50 = tt.extract %49, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %51 = tt.cast %50 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %52 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %53 = tt.extract %52, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %54 = tt.cast %53 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %55 = tt.dot %51, %54, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %56 = tt.extract %49, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %57 = tt.cast %56 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %58 = tt.extract %52, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %59 = tt.cast %58 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %60 = tt.dot %57, %59, %55 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %61 = tt.glue %60 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %62 = tt.extract %49, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %63 = tt.cast %62 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %64 = tt.dot %63, %54, %arg8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %65 = tt.extract %49, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %66 = tt.cast %65 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %67 = tt.dot %66, %59, %64 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %68 = tt.glue %67 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %69 = tt.extract %49, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %70 = tt.cast %69 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %71 = tt.dot %70, %54, %arg9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %72 = tt.extract %49, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %73 = tt.cast %72 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %74 = tt.dot %73, %59, %71 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %75 = tt.glue %74 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %76 = tt.extract %49, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %77 = tt.cast %76 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %78 = tt.dot %77, %54, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %79 = tt.extract %49, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %80 = tt.cast %79 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %81 = tt.dot %80, %59, %78 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %82 = tt.glue %81 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %83 = tt.extract %52, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %84 = tt.cast %83 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %85 = tt.dot %51, %84, %arg11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %86 = tt.extract %52, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %87 = tt.cast %86 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %88 = tt.dot %57, %87, %85 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %89 = tt.glue %88 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %90 = tt.dot %63, %84, %arg12 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %91 = tt.dot %66, %87, %90 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %92 = tt.glue %91 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %93 = tt.dot %70, %84, %arg13 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %94 = tt.dot %73, %87, %93 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %95 = tt.glue %94 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %96 = tt.dot %77, %84, %arg14 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %97 = tt.dot %80, %87, %96 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %98 = tt.glue %97 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %99 = tt.cast %48 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %100 = tt.extract %99, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %101 = tt.cast %100 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %102 = tt.dot %51, %101, %arg15 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %103 = tt.extract %99, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %104 = tt.cast %103 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %105 = tt.dot %57, %104, %102 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %106 = tt.glue %105 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %107 = tt.dot %63, %101, %arg16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %108 = tt.dot %66, %104, %107 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %109 = tt.glue %108 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %110 = tt.dot %70, %101, %arg17 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %111 = tt.dot %73, %104, %110 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %112 = tt.glue %111 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %113 = tt.dot %77, %101, %arg18 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %114 = tt.dot %80, %104, %113 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %115 = tt.glue %114 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %116 = tt.extract %99, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %117 = tt.cast %116 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %118 = tt.dot %51, %117, %arg19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %119 = tt.extract %99, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %120 = tt.cast %119 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %121 = tt.dot %57, %120, %118 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %122 = tt.glue %121 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %123 = tt.dot %63, %117, %arg20 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %124 = tt.dot %66, %120, %123 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %125 = tt.glue %124 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %126 = tt.dot %70, %117, %arg21 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %127 = tt.dot %73, %120, %126 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %128 = tt.glue %127 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %129 = tt.dot %77, %117, %arg22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %130 = tt.dot %80, %120, %129 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %131 = tt.glue %130 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %132 = tt.advance %arg23, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %133 = tt.advance %arg24, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %134 = tt.advance %arg25, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      scf.yield %61, %68, %75, %82, %89, %92, %95, %98, %106, %109, %112, %115, %122, %125, %128, %131, %132, %133, %134 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    } loc(#loc)
    %25 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %26 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %27 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %28 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %29 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %30 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %31 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %32 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %33 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %35 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %36 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %37 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %38 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %39 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %40 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %41 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %42 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %44 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %45 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %25, %24#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %27, %24#1 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %29, %24#2 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %31, %24#3 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %33, %24#4 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %34, %24#5 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %35, %24#6 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %36, %24#7 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %37, %24#8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %38, %24#9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %39, %24#10 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %40, %24#11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %42, %24#12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %43, %24#13 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %44, %24#14 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %45, %24#15 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before SCFToControlFlow (convert-scf-to-cf) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %24:19 = scf.for %arg6 = %c0_i32 to %c4096_i32 step %c32_i32 iter_args(%arg7 = %cst, %arg8 = %cst, %arg9 = %cst, %arg10 = %cst, %arg11 = %cst, %arg12 = %cst, %arg13 = %cst, %arg14 = %cst, %arg15 = %cst, %arg16 = %cst, %arg17 = %cst, %arg18 = %cst, %arg19 = %cst, %arg20 = %cst, %arg21 = %cst, %arg22 = %cst, %arg23 = %16, %arg24 = %21, %arg25 = %23) -> (tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>)  : i32 {
      %46 = tt.load %arg23 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %47 = tt.load %arg24 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %48 = tt.load %arg25 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
      %49 = tt.cast %46 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
      %50 = tt.extract %49, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %51 = tt.cast %50 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %52 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %53 = tt.extract %52, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %54 = tt.cast %53 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %55 = tt.dot %51, %54, %arg7 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %56 = tt.extract %49, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %57 = tt.cast %56 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %58 = tt.extract %52, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %59 = tt.cast %58 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %60 = tt.dot %57, %59, %55 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %61 = tt.glue %60 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %62 = tt.extract %49, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %63 = tt.cast %62 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %64 = tt.dot %63, %54, %arg8 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %65 = tt.extract %49, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %66 = tt.cast %65 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %67 = tt.dot %66, %59, %64 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %68 = tt.glue %67 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %69 = tt.extract %49, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %70 = tt.cast %69 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %71 = tt.dot %70, %54, %arg9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %72 = tt.extract %49, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %73 = tt.cast %72 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %74 = tt.dot %73, %59, %71 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %75 = tt.glue %74 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %76 = tt.extract %49, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %77 = tt.cast %76 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %78 = tt.dot %77, %54, %arg10 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %79 = tt.extract %49, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
      %80 = tt.cast %79 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
      %81 = tt.dot %80, %59, %78 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %82 = tt.glue %81 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %83 = tt.extract %52, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %84 = tt.cast %83 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %85 = tt.dot %51, %84, %arg11 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %86 = tt.extract %52, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %87 = tt.cast %86 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %88 = tt.dot %57, %87, %85 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %89 = tt.glue %88 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %90 = tt.dot %63, %84, %arg12 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %91 = tt.dot %66, %87, %90 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %92 = tt.glue %91 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %93 = tt.dot %70, %84, %arg13 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %94 = tt.dot %73, %87, %93 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %95 = tt.glue %94 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %96 = tt.dot %77, %84, %arg14 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %97 = tt.dot %80, %87, %96 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %98 = tt.glue %97 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %99 = tt.cast %48 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
      %100 = tt.extract %99, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %101 = tt.cast %100 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %102 = tt.dot %51, %101, %arg15 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %103 = tt.extract %99, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %104 = tt.cast %103 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %105 = tt.dot %57, %104, %102 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %106 = tt.glue %105 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %107 = tt.dot %63, %101, %arg16 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %108 = tt.dot %66, %104, %107 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %109 = tt.glue %108 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %110 = tt.dot %70, %101, %arg17 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %111 = tt.dot %73, %104, %110 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %112 = tt.glue %111 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %113 = tt.dot %77, %101, %arg18 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %114 = tt.dot %80, %104, %113 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %115 = tt.glue %114 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %116 = tt.extract %99, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %117 = tt.cast %116 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %118 = tt.dot %51, %117, %arg19 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %119 = tt.extract %99, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
      %120 = tt.cast %119 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
      %121 = tt.dot %57, %120, %118 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %122 = tt.glue %121 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %123 = tt.dot %63, %117, %arg20 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %124 = tt.dot %66, %120, %123 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %125 = tt.glue %124 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %126 = tt.dot %70, %117, %arg21 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %127 = tt.dot %73, %120, %126 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %128 = tt.glue %127 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %129 = tt.dot %77, %117, %arg22 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %130 = tt.dot %80, %120, %129 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
      %131 = tt.glue %130 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
      %132 = tt.advance %arg23, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %133 = tt.advance %arg24, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      %134 = tt.advance %arg25, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
      scf.yield %61, %68, %75, %82, %89, %92, %95, %98, %106, %109, %112, %115, %122, %125, %128, %131, %132, %133, %134 : tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    } loc(#loc)
    %25 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %26 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %27 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %28 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %29 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %30 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %31 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %32 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %33 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %34 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %35 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %36 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %32] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %37 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %38 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %39 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %40 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %41 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %42 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %43 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%26, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %44 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%28, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %45 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%30, %41] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %25, %24#0 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %27, %24#1 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %29, %24#2 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %31, %24#3 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %33, %24#4 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %34, %24#5 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %35, %24#6 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %36, %24#7 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %37, %24#8 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %38, %24#9 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %39, %24#10 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %40, %24#11 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %42, %24#12 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %43, %24#13 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %44, %24#14 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %45, %24#15 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before ConvertIndexToLLVMPass (convert-index-to-llvm) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    cf.br ^bb1(%c0_i32, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %16, %21, %23 : i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) loc(#loc)
  ^bb1(%24: i32 loc(unknown), %25: tensor<8x16xf32> loc(unknown), %26: tensor<8x16xf32> loc(unknown), %27: tensor<8x16xf32> loc(unknown), %28: tensor<8x16xf32> loc(unknown), %29: tensor<8x16xf32> loc(unknown), %30: tensor<8x16xf32> loc(unknown), %31: tensor<8x16xf32> loc(unknown), %32: tensor<8x16xf32> loc(unknown), %33: tensor<8x16xf32> loc(unknown), %34: tensor<8x16xf32> loc(unknown), %35: tensor<8x16xf32> loc(unknown), %36: tensor<8x16xf32> loc(unknown), %37: tensor<8x16xf32> loc(unknown), %38: tensor<8x16xf32> loc(unknown), %39: tensor<8x16xf32> loc(unknown), %40: tensor<8x16xf32> loc(unknown), %41: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown), %42: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown), %43: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %44 = arith.cmpi slt, %24, %c4096_i32 : i32 loc(#loc)
    cf.cond_br %44, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %45 = tt.load %41 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %46 = tt.load %42 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %47 = tt.load %43 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %48 = tt.cast %45 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
    %49 = tt.extract %48, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %50 = tt.cast %49 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %51 = tt.cast %46 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
    %52 = tt.extract %51, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %53 = tt.cast %52 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %54 = tt.dot %50, %53, %25 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %55 = tt.extract %48, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %56 = tt.cast %55 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %57 = tt.extract %51, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %58 = tt.cast %57 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %59 = tt.dot %56, %58, %54 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %60 = tt.glue %59 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %61 = tt.extract %48, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %62 = tt.cast %61 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %63 = tt.dot %62, %53, %26 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %64 = tt.extract %48, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %65 = tt.cast %64 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %66 = tt.dot %65, %58, %63 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %67 = tt.glue %66 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %68 = tt.extract %48, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %69 = tt.cast %68 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %70 = tt.dot %69, %53, %27 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %71 = tt.extract %48, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %72 = tt.cast %71 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %73 = tt.dot %72, %58, %70 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %74 = tt.glue %73 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %75 = tt.extract %48, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %76 = tt.cast %75 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %77 = tt.dot %76, %53, %28 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %78 = tt.extract %48, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %79 = tt.cast %78 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %80 = tt.dot %79, %58, %77 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %81 = tt.glue %80 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %82 = tt.extract %51, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %83 = tt.cast %82 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %84 = tt.dot %50, %83, %29 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %85 = tt.extract %51, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %86 = tt.cast %85 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %87 = tt.dot %56, %86, %84 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %88 = tt.glue %87 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %89 = tt.dot %62, %83, %30 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %90 = tt.dot %65, %86, %89 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %91 = tt.glue %90 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %92 = tt.dot %69, %83, %31 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %93 = tt.dot %72, %86, %92 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %94 = tt.glue %93 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %95 = tt.dot %76, %83, %32 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %96 = tt.dot %79, %86, %95 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %97 = tt.glue %96 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %98 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
    %99 = tt.extract %98, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %100 = tt.cast %99 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %101 = tt.dot %50, %100, %33 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %102 = tt.extract %98, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %103 = tt.cast %102 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %104 = tt.dot %56, %103, %101 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %105 = tt.glue %104 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %106 = tt.dot %62, %100, %34 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %107 = tt.dot %65, %103, %106 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %108 = tt.glue %107 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %109 = tt.dot %69, %100, %35 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %110 = tt.dot %72, %103, %109 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %111 = tt.glue %110 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %112 = tt.dot %76, %100, %36 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %113 = tt.dot %79, %103, %112 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %114 = tt.glue %113 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %115 = tt.extract %98, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %116 = tt.cast %115 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %117 = tt.dot %50, %116, %37 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %118 = tt.extract %98, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %119 = tt.cast %118 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %120 = tt.dot %56, %119, %117 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %121 = tt.glue %120 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %122 = tt.dot %62, %116, %38 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %123 = tt.dot %65, %119, %122 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %124 = tt.glue %123 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %125 = tt.dot %69, %116, %39 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %126 = tt.dot %72, %119, %125 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %127 = tt.glue %126 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %128 = tt.dot %76, %116, %40 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %129 = tt.dot %79, %119, %128 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %130 = tt.glue %129 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %131 = tt.advance %41, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %132 = tt.advance %42, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %133 = tt.advance %43, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %134 = arith.addi %24, %c32_i32 : i32 loc(#loc)
    cf.br ^bb1(%134, %60, %67, %74, %81, %88, %91, %94, %97, %105, %108, %111, %114, %121, %124, %127, %130, %131, %132, %133 : i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %135 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %136 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %137 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %138 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %139 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %140 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %141 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %142 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %143 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %144 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %145 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %146 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %147 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %148 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %149 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %150 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %151 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %152 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %153 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %154 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %155 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %135, %25 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %137, %26 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %139, %27 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %141, %28 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %143, %29 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %144, %30 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %145, %31 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %146, %32 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %147, %33 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %148, %34 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %149, %35 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %150, %36 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %152, %37 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %153, %38 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %154, %39 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %155, %40 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before AllocateSharedMemory (allocate-shared-memory) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    cf.br ^bb1(%c0_i32, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %16, %21, %23 : i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) loc(#loc)
  ^bb1(%24: i32 loc(unknown), %25: tensor<8x16xf32> loc(unknown), %26: tensor<8x16xf32> loc(unknown), %27: tensor<8x16xf32> loc(unknown), %28: tensor<8x16xf32> loc(unknown), %29: tensor<8x16xf32> loc(unknown), %30: tensor<8x16xf32> loc(unknown), %31: tensor<8x16xf32> loc(unknown), %32: tensor<8x16xf32> loc(unknown), %33: tensor<8x16xf32> loc(unknown), %34: tensor<8x16xf32> loc(unknown), %35: tensor<8x16xf32> loc(unknown), %36: tensor<8x16xf32> loc(unknown), %37: tensor<8x16xf32> loc(unknown), %38: tensor<8x16xf32> loc(unknown), %39: tensor<8x16xf32> loc(unknown), %40: tensor<8x16xf32> loc(unknown), %41: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown), %42: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown), %43: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %44 = arith.cmpi slt, %24, %c4096_i32 : i32 loc(#loc)
    cf.cond_br %44, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %45 = tt.load %41 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %46 = tt.load %42 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %47 = tt.load %43 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %48 = tt.cast %45 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
    %49 = tt.extract %48, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %50 = tt.cast %49 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %51 = tt.cast %46 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
    %52 = tt.extract %51, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %53 = tt.cast %52 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %54 = tt.dot %50, %53, %25 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %55 = tt.extract %48, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %56 = tt.cast %55 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %57 = tt.extract %51, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %58 = tt.cast %57 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %59 = tt.dot %56, %58, %54 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %60 = tt.glue %59 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %61 = tt.extract %48, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %62 = tt.cast %61 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %63 = tt.dot %62, %53, %26 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %64 = tt.extract %48, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %65 = tt.cast %64 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %66 = tt.dot %65, %58, %63 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %67 = tt.glue %66 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %68 = tt.extract %48, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %69 = tt.cast %68 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %70 = tt.dot %69, %53, %27 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %71 = tt.extract %48, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %72 = tt.cast %71 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %73 = tt.dot %72, %58, %70 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %74 = tt.glue %73 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %75 = tt.extract %48, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %76 = tt.cast %75 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %77 = tt.dot %76, %53, %28 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %78 = tt.extract %48, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %79 = tt.cast %78 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %80 = tt.dot %79, %58, %77 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %81 = tt.glue %80 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %82 = tt.extract %51, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %83 = tt.cast %82 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %84 = tt.dot %50, %83, %29 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %85 = tt.extract %51, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %86 = tt.cast %85 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %87 = tt.dot %56, %86, %84 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %88 = tt.glue %87 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %89 = tt.dot %62, %83, %30 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %90 = tt.dot %65, %86, %89 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %91 = tt.glue %90 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %92 = tt.dot %69, %83, %31 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %93 = tt.dot %72, %86, %92 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %94 = tt.glue %93 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %95 = tt.dot %76, %83, %32 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %96 = tt.dot %79, %86, %95 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %97 = tt.glue %96 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %98 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
    %99 = tt.extract %98, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %100 = tt.cast %99 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %101 = tt.dot %50, %100, %33 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %102 = tt.extract %98, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %103 = tt.cast %102 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %104 = tt.dot %56, %103, %101 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %105 = tt.glue %104 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %106 = tt.dot %62, %100, %34 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %107 = tt.dot %65, %103, %106 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %108 = tt.glue %107 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %109 = tt.dot %69, %100, %35 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %110 = tt.dot %72, %103, %109 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %111 = tt.glue %110 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %112 = tt.dot %76, %100, %36 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %113 = tt.dot %79, %103, %112 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %114 = tt.glue %113 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %115 = tt.extract %98, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %116 = tt.cast %115 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %117 = tt.dot %50, %116, %37 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %118 = tt.extract %98, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %119 = tt.cast %118 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %120 = tt.dot %56, %119, %117 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %121 = tt.glue %120 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %122 = tt.dot %62, %116, %38 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %123 = tt.dot %65, %119, %122 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %124 = tt.glue %123 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %125 = tt.dot %69, %116, %39 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %126 = tt.dot %72, %119, %125 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %127 = tt.glue %126 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %128 = tt.dot %76, %116, %40 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %129 = tt.dot %79, %119, %128 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %130 = tt.glue %129 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %131 = tt.advance %41, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %132 = tt.advance %42, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %133 = tt.advance %43, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %134 = arith.addi %24, %c32_i32 : i32 loc(#loc)
    cf.br ^bb1(%134, %60, %67, %74, %81, %88, %91, %94, %97, %105, %108, %111, %114, %121, %124, %127, %130, %131, %132, %133 : i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %135 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %136 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %137 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %138 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %139 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %140 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %141 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %142 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %143 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %144 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %145 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %146 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %147 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %148 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %149 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %150 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %151 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %152 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %153 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %154 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %155 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %135, %25 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %137, %26 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %139, %27 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %141, %28 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %143, %29 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %144, %30 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %145, %31 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %146, %32 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %147, %33 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %148, %34 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %149, %35 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %150, %36 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %152, %37 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %153, %38 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %154, %39 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %155, %40 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before ConvertTritonGPUToLLVM (convert-triton-gpu-to-llvm) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 1 : i32} {
  tt.func public @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {noinline = false} {
    %c3_i32 = arith.constant 3 : i32 loc(#loc)
    %c7_i32 = arith.constant 7 : i32 loc(#loc)
    %c63_i32 = arith.constant 63 : i32 loc(#loc)
    %c48_i32 = arith.constant 48 : i32 loc(#loc)
    %c24_i32 = arith.constant 24 : i32 loc(#loc)
    %c8_i32 = arith.constant 8 : i32 loc(#loc)
    %c4_i32 = arith.constant 4 : i32 loc(#loc)
    %c256_i32 = arith.constant 256 : i32 loc(#loc)
    %c4096_i64 = arith.constant 4096 : i64 loc(#loc)
    %c1_i64 = arith.constant 1 : i64 loc(#loc)
    %c0_i32 = arith.constant 0 : i32 loc(#loc)
    %c32_i32 = arith.constant 32 : i32 loc(#loc)
    %cst = arith.constant dense<0.000000e+00> : tensor<8x16xf32> loc(#loc)
    %c4096_i32 = arith.constant 4096 : i32 loc(#loc)
    %c16_i32 = arith.constant 16 : i32 loc(#loc)
    %c64_i32 = arith.constant 64 : i32 loc(#loc)
    %0 = gpu.subgroup_id : index loc(#loc)
    %1 = arith.index_cast %0 : index to i32 loc(#loc)
    %2 = tt.get_program_id x : i32 loc(#loc)
    %3 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %4 = arith.muli %3, %c4_i32 : i32 loc(#loc)
    %5 = arith.subi %c16_i32, %4 : i32 loc(#loc)
    %6 = arith.minsi %5, %c4_i32 : i32 loc(#loc)
    %7 = arith.remsi %2, %6 : i32 loc(#loc)
    %8 = arith.addi %4, %7 : i32 loc(#loc)
    %9 = arith.andi %2, %c63_i32 : i32 loc(#loc)
    %10 = arith.divsi %9, %6 : i32 loc(#loc)
    %11 = arith.muli %8, %c256_i32 : i32 loc(#loc)
    %12 = arith.divsi %1, %c4_i32 : i32 loc(#loc)
    %13 = arith.andi %12, %c7_i32 : i32 loc(#loc)
    %14 = arith.muli %13, %c32_i32 : i32 loc(#loc)
    %15 = arith.addi %14, %11 : i32 loc(#loc)
    %16 = tt.make_tensor_ptr %arg0, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %c0_i32] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %17 = arith.muli %10, %c256_i32 : i32 loc(#loc)
    %18 = arith.andi %1, %c3_i32 : i32 loc(#loc)
    %19 = arith.muli %18, %c64_i32 : i32 loc(#loc)
    %20 = arith.addi %19, %17 : i32 loc(#loc)
    %21 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %20] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    %22 = arith.addi %20, %c32_i32 : i32 loc(#loc)
    %23 = tt.make_tensor_ptr %arg1, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%c0_i32, %22] {order = array<i32: 1, 0>} : <tensor<32x32xf16>, 1> loc(#loc)
    cf.br ^bb1(%c0_i32, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %cst, %16, %21, %23 : i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) loc(#loc)
  ^bb1(%24: i32 loc(unknown), %25: tensor<8x16xf32> loc(unknown), %26: tensor<8x16xf32> loc(unknown), %27: tensor<8x16xf32> loc(unknown), %28: tensor<8x16xf32> loc(unknown), %29: tensor<8x16xf32> loc(unknown), %30: tensor<8x16xf32> loc(unknown), %31: tensor<8x16xf32> loc(unknown), %32: tensor<8x16xf32> loc(unknown), %33: tensor<8x16xf32> loc(unknown), %34: tensor<8x16xf32> loc(unknown), %35: tensor<8x16xf32> loc(unknown), %36: tensor<8x16xf32> loc(unknown), %37: tensor<8x16xf32> loc(unknown), %38: tensor<8x16xf32> loc(unknown), %39: tensor<8x16xf32> loc(unknown), %40: tensor<8x16xf32> loc(unknown), %41: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown), %42: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown), %43: !tt.ptr<tensor<32x32xf16>, 1> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %44 = arith.cmpi slt, %24, %c4096_i32 : i32 loc(#loc)
    cf.cond_br %44, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %45 = tt.load %41 {DotIdx = 0 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %46 = tt.load %42 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %47 = tt.load %43 {DotIdx = 1 : i32, boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<32x32xf16>, 1> -> tensor<32x32xf16> loc(#loc)
    %48 = tt.cast %45 : tensor<32x32xf16> to tensor<32x32xi16> loc(#loc)
    %49 = tt.extract %48, 0 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %50 = tt.cast %49 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %51 = tt.cast %46 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
    %52 = tt.extract %51, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %53 = tt.cast %52 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %54 = tt.dot %50, %53, %25 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %55 = tt.extract %48, 4 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %56 = tt.cast %55 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %57 = tt.extract %51, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %58 = tt.cast %57 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %59 = tt.dot %56, %58, %54 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %60 = tt.glue %59 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %61 = tt.extract %48, 1 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %62 = tt.cast %61 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %63 = tt.dot %62, %53, %26 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %64 = tt.extract %48, 5 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %65 = tt.cast %64 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %66 = tt.dot %65, %58, %63 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %67 = tt.glue %66 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %68 = tt.extract %48, 2 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %69 = tt.cast %68 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %70 = tt.dot %69, %53, %27 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %71 = tt.extract %48, 6 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %72 = tt.cast %71 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %73 = tt.dot %72, %58, %70 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %74 = tt.glue %73 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %75 = tt.extract %48, 3 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %76 = tt.cast %75 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %77 = tt.dot %76, %53, %28 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %78 = tt.extract %48, 7 : tensor<32x32xi16> -> tensor<8x16xi16> loc(#loc)
    %79 = tt.cast %78 : tensor<8x16xi16> to tensor<8x16xf16> loc(#loc)
    %80 = tt.dot %79, %58, %77 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %81 = tt.glue %80 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %82 = tt.extract %51, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %83 = tt.cast %82 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %84 = tt.dot %50, %83, %29 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %85 = tt.extract %51, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %86 = tt.cast %85 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %87 = tt.dot %56, %86, %84 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %88 = tt.glue %87 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %89 = tt.dot %62, %83, %30 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %90 = tt.dot %65, %86, %89 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %91 = tt.glue %90 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %92 = tt.dot %69, %83, %31 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %93 = tt.dot %72, %86, %92 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %94 = tt.glue %93 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %95 = tt.dot %76, %83, %32 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %96 = tt.dot %79, %86, %95 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %97 = tt.glue %96 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %98 = tt.cast %47 : tensor<32x32xf16> to tensor<16x32xi32> loc(#loc)
    %99 = tt.extract %98, 0 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %100 = tt.cast %99 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %101 = tt.dot %50, %100, %33 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %102 = tt.extract %98, 1 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %103 = tt.cast %102 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %104 = tt.dot %56, %103, %101 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %105 = tt.glue %104 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %106 = tt.dot %62, %100, %34 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %107 = tt.dot %65, %103, %106 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %108 = tt.glue %107 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %109 = tt.dot %69, %100, %35 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %110 = tt.dot %72, %103, %109 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %111 = tt.glue %110 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %112 = tt.dot %76, %100, %36 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %113 = tt.dot %79, %103, %112 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %114 = tt.glue %113 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %115 = tt.extract %98, 2 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %116 = tt.cast %115 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %117 = tt.dot %50, %116, %37 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %118 = tt.extract %98, 3 : tensor<16x32xi32> -> tensor<8x16xi32> loc(#loc)
    %119 = tt.cast %118 : tensor<8x16xi32> to tensor<16x16xf16> loc(#loc)
    %120 = tt.dot %56, %119, %117 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %121 = tt.glue %120 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %122 = tt.dot %62, %116, %38 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %123 = tt.dot %65, %119, %122 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %124 = tt.glue %123 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %125 = tt.dot %69, %116, %39 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %126 = tt.dot %72, %119, %125 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %127 = tt.glue %126 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %128 = tt.dot %76, %116, %40 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %129 = tt.dot %79, %119, %128 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<8x16xf16> * tensor<16x16xf16> -> tensor<8x16xf32> loc(#loc)
    %130 = tt.glue %129 : tensor<8x16xf32> -> tensor<8x16xf32> loc(#loc)
    %131 = tt.advance %41, [%c0_i32, %c32_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %132 = tt.advance %42, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %133 = tt.advance %43, [%c32_i32, %c0_i32] : <tensor<32x32xf16>, 1> loc(#loc)
    %134 = arith.addi %24, %c32_i32 : i32 loc(#loc)
    cf.br ^bb1(%134, %60, %67, %74, %81, %88, %91, %94, %97, %105, %108, %111, %114, %121, %124, %127, %130, %131, %132, %133 : i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %135 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %136 = arith.addi %15, %c8_i32 : i32 loc(#loc)
    %137 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %138 = arith.addi %15, %c16_i32 : i32 loc(#loc)
    %139 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %140 = arith.addi %15, %c24_i32 : i32 loc(#loc)
    %141 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %20] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %142 = arith.addi %20, %c16_i32 : i32 loc(#loc)
    %143 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %144 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %145 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %146 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %142] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %147 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %148 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %149 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %150 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %22] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %151 = arith.addi %20, %c48_i32 : i32 loc(#loc)
    %152 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%15, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %153 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%136, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %154 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%138, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    %155 = tt.make_tensor_ptr %arg2, [%c4096_i64, %c4096_i64], [%c4096_i64, %c1_i64], [%140, %151] {order = array<i32: 1, 0>} : <tensor<8x16xf32>, 1> loc(#loc)
    tt.store %135, %25 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %137, %26 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %139, %27 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %141, %28 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %143, %29 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %144, %30 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %145, %31 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %146, %32 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %147, %33 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %148, %34 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %149, %35 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %150, %36 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %152, %37 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %153, %38 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %154, %39 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.store %155, %40 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32} : !tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32> loc(#loc)
    tt.return loc(#loc)
  } loc(#loc)
} loc(#loc)


"builtin.module"() ({
  "llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = !llvm.func<void (ptr<1>, ptr<1>, ptr<1>, i32, i32, i32)>, linkage = #llvm.linkage<external>, sym_name = "matmul_kernel_with_block_pointers_0d1d2d3d4d5d", visibility_ = 0 : i64}> ({
  ^bb0(%arg0: !llvm.ptr<1>, %arg1: !llvm.ptr<1>, %arg2: !llvm.ptr<1>, %arg3: i32, %arg4: i32, %arg5: i32):
    %0 = "builtin.unrealized_conversion_cast"(%arg2) : (!llvm.ptr<1>) -> !tt.ptr<f32, 1>
    %1 = "builtin.unrealized_conversion_cast"(%arg1) : (!llvm.ptr<1>) -> !tt.ptr<f16, 1>
    %2 = "builtin.unrealized_conversion_cast"(%arg0) : (!llvm.ptr<1>) -> !tt.ptr<f16, 1>
    %3 = "arith.constant"() <{value = 3 : i32}> : () -> i32
    %4 = "arith.constant"() <{value = 7 : i32}> : () -> i32
    %5 = "arith.constant"() <{value = 63 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 48 : i32}> : () -> i32
    %7 = "arith.constant"() <{value = 24 : i32}> : () -> i32
    %8 = "arith.constant"() <{value = 8 : i32}> : () -> i32
    %9 = "arith.constant"() <{value = 4 : i32}> : () -> i32
    %10 = "arith.constant"() <{value = 256 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 4096 : i64}> : () -> i64
    %12 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %13 = "arith.constant"() <{value = 0 : i32}> : () -> i32
    %14 = "arith.constant"() <{value = 32 : i32}> : () -> i32
    %15 = "arith.constant"() <{value = dense<0.000000e+00> : tensor<8x16xf32>}> : () -> tensor<8x16xf32>
    %16 = "arith.constant"() <{value = 4096 : i32}> : () -> i32
    %17 = "arith.constant"() <{value = 16 : i32}> : () -> i32
    %18 = "arith.constant"() <{value = 64 : i32}> : () -> i32
    %19 = "gpu.subgroup_id"() : () -> index
    %20 = "arith.index_cast"(%19) : (index) -> i32
    %21 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
    %22 = "arith.divsi"(%21, %18) : (i32, i32) -> i32
    %23 = "arith.muli"(%22, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %24 = "arith.subi"(%17, %23) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %25 = "arith.minsi"(%24, %9) : (i32, i32) -> i32
    %26 = "arith.remsi"(%21, %25) : (i32, i32) -> i32
    %27 = "arith.addi"(%23, %26) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %28 = "arith.andi"(%21, %5) : (i32, i32) -> i32
    %29 = "arith.divsi"(%28, %25) : (i32, i32) -> i32
    %30 = "arith.muli"(%27, %10) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %31 = "arith.divsi"(%20, %9) : (i32, i32) -> i32
    %32 = "arith.andi"(%31, %4) : (i32, i32) -> i32
    %33 = "arith.muli"(%32, %14) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %34 = "arith.addi"(%33, %30) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %35 = "tt.make_tensor_ptr"(%2, %11, %11, %11, %12, %34, %13) <{order = array<i32: 1, 0>}> : (!tt.ptr<f16, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
    %36 = "arith.muli"(%29, %10) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %37 = "arith.andi"(%20, %3) : (i32, i32) -> i32
    %38 = "arith.muli"(%37, %18) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %39 = "arith.addi"(%38, %36) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %40 = "tt.make_tensor_ptr"(%1, %11, %11, %11, %12, %13, %39) <{order = array<i32: 1, 0>}> : (!tt.ptr<f16, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
    %41 = "arith.addi"(%39, %14) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %42 = "tt.make_tensor_ptr"(%1, %11, %11, %11, %12, %13, %41) <{order = array<i32: 1, 0>}> : (!tt.ptr<f16, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
    "cf.br"(%13, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %35, %40, %42)[^bb1] : (i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) -> ()
  ^bb1(%43: i32, %44: vector<8xf32>, %45: vector<8xf32>, %46: vector<8xf32>, %47: vector<8xf32>, %48: vector<8xf32>, %49: vector<8xf32>, %50: vector<8xf32>, %51: vector<8xf32>, %52: vector<8xf32>, %53: vector<8xf32>, %54: vector<8xf32>, %55: vector<8xf32>, %56: vector<8xf32>, %57: vector<8xf32>, %58: vector<8xf32>, %59: vector<8xf32>, %60: vector<2xi32>, %61: vector<2xi32>, %62: vector<2xi32>):  // 2 preds: ^bb0, ^bb2
    %63 = "builtin.unrealized_conversion_cast"(%62) : (vector<2xi32>) -> !tt.ptr<tensor<32x32xf16>, 1>
    %64 = "builtin.unrealized_conversion_cast"(%61) : (vector<2xi32>) -> !tt.ptr<tensor<32x32xf16>, 1>
    %65 = "builtin.unrealized_conversion_cast"(%60) : (vector<2xi32>) -> !tt.ptr<tensor<32x32xf16>, 1>
    %66 = "builtin.unrealized_conversion_cast"(%59) : (vector<8xf32>) -> tensor<8x16xf32>
    %67 = "builtin.unrealized_conversion_cast"(%58) : (vector<8xf32>) -> tensor<8x16xf32>
    %68 = "builtin.unrealized_conversion_cast"(%57) : (vector<8xf32>) -> tensor<8x16xf32>
    %69 = "builtin.unrealized_conversion_cast"(%56) : (vector<8xf32>) -> tensor<8x16xf32>
    %70 = "builtin.unrealized_conversion_cast"(%55) : (vector<8xf32>) -> tensor<8x16xf32>
    %71 = "builtin.unrealized_conversion_cast"(%54) : (vector<8xf32>) -> tensor<8x16xf32>
    %72 = "builtin.unrealized_conversion_cast"(%53) : (vector<8xf32>) -> tensor<8x16xf32>
    %73 = "builtin.unrealized_conversion_cast"(%52) : (vector<8xf32>) -> tensor<8x16xf32>
    %74 = "builtin.unrealized_conversion_cast"(%51) : (vector<8xf32>) -> tensor<8x16xf32>
    %75 = "builtin.unrealized_conversion_cast"(%50) : (vector<8xf32>) -> tensor<8x16xf32>
    %76 = "builtin.unrealized_conversion_cast"(%49) : (vector<8xf32>) -> tensor<8x16xf32>
    %77 = "builtin.unrealized_conversion_cast"(%48) : (vector<8xf32>) -> tensor<8x16xf32>
    %78 = "builtin.unrealized_conversion_cast"(%47) : (vector<8xf32>) -> tensor<8x16xf32>
    %79 = "builtin.unrealized_conversion_cast"(%46) : (vector<8xf32>) -> tensor<8x16xf32>
    %80 = "builtin.unrealized_conversion_cast"(%45) : (vector<8xf32>) -> tensor<8x16xf32>
    %81 = "builtin.unrealized_conversion_cast"(%44) : (vector<8xf32>) -> tensor<8x16xf32>
    %82 = "arith.cmpi"(%43, %16) <{predicate = 2 : i64}> : (i32, i32) -> i1
    "cf.cond_br"(%82)[^bb2, ^bb3] <{operandSegmentSizes = array<i32: 1, 0, 0>}> : (i1) -> ()
  ^bb2:  // pred: ^bb1
    %83 = "tt.load"(%65) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DotIdx = 0 : i32} : (!tt.ptr<tensor<32x32xf16>, 1>) -> tensor<32x32xf16>
    %84 = "tt.load"(%64) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DotIdx = 1 : i32} : (!tt.ptr<tensor<32x32xf16>, 1>) -> tensor<32x32xf16>
    %85 = "tt.load"(%63) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DotIdx = 1 : i32} : (!tt.ptr<tensor<32x32xf16>, 1>) -> tensor<32x32xf16>
    %86 = "tt.cast"(%83) : (tensor<32x32xf16>) -> tensor<32x32xi16>
    %87 = "tt.extract"(%86) <{idx = 0 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %88 = "tt.cast"(%87) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %89 = "tt.cast"(%84) : (tensor<32x32xf16>) -> tensor<16x32xi32>
    %90 = "tt.extract"(%89) <{idx = 0 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %91 = "tt.cast"(%90) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %92 = "tt.dot"(%88, %91, %81) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %93 = "tt.extract"(%86) <{idx = 4 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %94 = "tt.cast"(%93) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %95 = "tt.extract"(%89) <{idx = 1 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %96 = "tt.cast"(%95) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %97 = "tt.dot"(%94, %96, %92) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %98 = "tt.glue"(%97) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %99 = "tt.extract"(%86) <{idx = 1 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %100 = "tt.cast"(%99) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %101 = "tt.dot"(%100, %91, %80) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %102 = "tt.extract"(%86) <{idx = 5 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %103 = "tt.cast"(%102) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %104 = "tt.dot"(%103, %96, %101) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %105 = "tt.glue"(%104) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %106 = "tt.extract"(%86) <{idx = 2 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %107 = "tt.cast"(%106) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %108 = "tt.dot"(%107, %91, %79) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %109 = "tt.extract"(%86) <{idx = 6 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %110 = "tt.cast"(%109) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %111 = "tt.dot"(%110, %96, %108) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %112 = "tt.glue"(%111) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %113 = "tt.extract"(%86) <{idx = 3 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %114 = "tt.cast"(%113) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %115 = "tt.dot"(%114, %91, %78) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %116 = "tt.extract"(%86) <{idx = 7 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
    %117 = "tt.cast"(%116) : (tensor<8x16xi16>) -> tensor<8x16xf16>
    %118 = "tt.dot"(%117, %96, %115) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %119 = "tt.glue"(%118) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %120 = "tt.extract"(%89) <{idx = 2 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %121 = "tt.cast"(%120) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %122 = "tt.dot"(%88, %121, %77) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %123 = "tt.extract"(%89) <{idx = 3 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %124 = "tt.cast"(%123) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %125 = "tt.dot"(%94, %124, %122) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %126 = "tt.glue"(%125) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %127 = "tt.dot"(%100, %121, %76) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %128 = "tt.dot"(%103, %124, %127) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %129 = "tt.glue"(%128) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %130 = "tt.dot"(%107, %121, %75) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %131 = "tt.dot"(%110, %124, %130) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %132 = "tt.glue"(%131) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %133 = "tt.dot"(%114, %121, %74) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %134 = "tt.dot"(%117, %124, %133) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %135 = "tt.glue"(%134) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %136 = "tt.cast"(%85) : (tensor<32x32xf16>) -> tensor<16x32xi32>
    %137 = "tt.extract"(%136) <{idx = 0 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %138 = "tt.cast"(%137) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %139 = "tt.dot"(%88, %138, %73) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %140 = "tt.extract"(%136) <{idx = 1 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %141 = "tt.cast"(%140) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %142 = "tt.dot"(%94, %141, %139) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %143 = "tt.glue"(%142) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %144 = "tt.dot"(%100, %138, %72) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %145 = "tt.dot"(%103, %141, %144) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %146 = "tt.glue"(%145) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %147 = "tt.dot"(%107, %138, %71) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %148 = "tt.dot"(%110, %141, %147) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %149 = "tt.glue"(%148) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %150 = "tt.dot"(%114, %138, %70) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %151 = "tt.dot"(%117, %141, %150) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %152 = "tt.glue"(%151) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %153 = "tt.extract"(%136) <{idx = 2 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %154 = "tt.cast"(%153) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %155 = "tt.dot"(%88, %154, %69) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %156 = "tt.extract"(%136) <{idx = 3 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
    %157 = "tt.cast"(%156) : (tensor<8x16xi32>) -> tensor<16x16xf16>
    %158 = "tt.dot"(%94, %157, %155) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %159 = "tt.glue"(%158) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %160 = "tt.dot"(%100, %154, %68) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %161 = "tt.dot"(%103, %157, %160) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %162 = "tt.glue"(%161) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %163 = "tt.dot"(%107, %154, %67) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %164 = "tt.dot"(%110, %157, %163) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %165 = "tt.glue"(%164) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %166 = "tt.dot"(%114, %154, %66) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %167 = "tt.dot"(%117, %157, %166) <{allowTF32 = true, maxNumImpreciseAcc = 0 : i32}> : (tensor<8x16xf16>, tensor<16x16xf16>, tensor<8x16xf32>) -> tensor<8x16xf32>
    %168 = "tt.glue"(%167) : (tensor<8x16xf32>) -> tensor<8x16xf32>
    %169 = "tt.advance"(%65, %13, %14) : (!tt.ptr<tensor<32x32xf16>, 1>, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
    %170 = "tt.advance"(%64, %14, %13) : (!tt.ptr<tensor<32x32xf16>, 1>, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
    %171 = "tt.advance"(%63, %14, %13) : (!tt.ptr<tensor<32x32xf16>, 1>, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
    %172 = "arith.addi"(%43, %14) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    "cf.br"(%172, %98, %105, %112, %119, %126, %129, %132, %135, %143, %146, %149, %152, %159, %162, %165, %168, %169, %170, %171)[^bb1] : (i32, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, tensor<8x16xf32>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>, !tt.ptr<tensor<32x32xf16>, 1>) -> ()
  ^bb3:  // pred: ^bb1
    %173 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %34, %39) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %174 = "arith.addi"(%34, %8) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %175 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %174, %39) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %176 = "arith.addi"(%34, %17) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %177 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %176, %39) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %178 = "arith.addi"(%34, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %179 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %178, %39) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %180 = "arith.addi"(%39, %17) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %181 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %34, %180) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %182 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %174, %180) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %183 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %176, %180) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %184 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %178, %180) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %185 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %34, %41) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %186 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %174, %41) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %187 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %176, %41) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %188 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %178, %41) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %189 = "arith.addi"(%39, %6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
    %190 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %34, %189) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %191 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %174, %189) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %192 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %176, %189) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    %193 = "tt.make_tensor_ptr"(%0, %11, %11, %11, %12, %178, %189) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
    "tt.store"(%173, %81) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%175, %80) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%177, %79) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%179, %78) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%181, %77) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%182, %76) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%183, %75) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%184, %74) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%185, %73) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%186, %72) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%187, %71) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%188, %70) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%190, %69) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%191, %68) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%192, %67) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.store"(%193, %66) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
    "tt.return"() : () -> ()
  }) {genx.intel_reqd_sub_group_size = [16 : i32], genx.kernel = 1 : i32, genx.max_work_group_size = [512 : i32, 1 : i32, 1 : i32], noinline = false} : () -> ()
}) {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 16 : i32} : () -> ()
%90 = "tt.make_tensor_ptr"(%2, %21, %21, %21, %23, %89, %25) <{order = array<i32: 1, 0>}> : (!tt.ptr<f16, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
%105 = "tt.make_tensor_ptr"(%1, %22, %22, %22, %24, %26, %104) <{order = array<i32: 1, 0>}> : (!tt.ptr<f16, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
%113 = "tt.make_tensor_ptr"(%1, %22, %22, %22, %24, %26, %112) <{order = array<i32: 1, 0>}> : (!tt.ptr<f16, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
%161 = "tt.load"(%141) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DotIdx = 0 : i32} : (!tt.ptr<tensor<32x32xf16>, 1>) -> tensor<32x32xf16>
%190 = "tt.load"(%161) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DotIdx = 1 : i32} : (!tt.ptr<tensor<32x32xf16>, 1>) -> tensor<32x32xf16>
%219 = "tt.load"(%181) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DotIdx = 1 : i32} : (!tt.ptr<tensor<32x32xf16>, 1>) -> tensor<32x32xf16>
%247 = "tt.cast"(%232) : (tensor<32x32xf16>) -> tensor<32x32xi16>
%249 = "tt.extract"(%248) <{idx = 0 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%251 = "tt.cast"(%250) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%253 = "tt.cast"(%239) : (tensor<32x32xf16>) -> tensor<16x32xi32>
%255 = "tt.extract"(%254) <{idx = 0 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%257 = "tt.cast"(%256) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%264 = "tt.extract"(%249) <{idx = 4 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%266 = "tt.cast"(%265) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%268 = "tt.extract"(%255) <{idx = 1 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%270 = "tt.cast"(%269) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%276 = "tt.glue"(%275) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%277 = "tt.extract"(%249) <{idx = 1 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%279 = "tt.cast"(%278) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%286 = "tt.extract"(%250) <{idx = 5 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%288 = "tt.cast"(%287) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%294 = "tt.glue"(%293) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%295 = "tt.extract"(%250) <{idx = 2 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%297 = "tt.cast"(%296) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%304 = "tt.extract"(%251) <{idx = 6 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%306 = "tt.cast"(%305) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%312 = "tt.glue"(%311) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%313 = "tt.extract"(%251) <{idx = 3 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%315 = "tt.cast"(%314) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%322 = "tt.extract"(%252) <{idx = 7 : i32}> : (tensor<32x32xi16>) -> tensor<8x16xi16>
%324 = "tt.cast"(%323) : (tensor<8x16xi16>) -> tensor<8x16xf16>
%330 = "tt.glue"(%329) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%331 = "tt.extract"(%258) <{idx = 2 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%333 = "tt.cast"(%332) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%340 = "tt.extract"(%259) <{idx = 3 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%342 = "tt.cast"(%341) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%348 = "tt.glue"(%347) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%358 = "tt.glue"(%357) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%368 = "tt.glue"(%367) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%378 = "tt.glue"(%377) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%379 = "tt.cast"(%254) : (tensor<32x32xf16>) -> tensor<16x32xi32>
%381 = "tt.extract"(%380) <{idx = 0 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%383 = "tt.cast"(%382) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%390 = "tt.extract"(%381) <{idx = 1 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%392 = "tt.cast"(%391) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%398 = "tt.glue"(%397) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%408 = "tt.glue"(%407) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%418 = "tt.glue"(%417) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%428 = "tt.glue"(%427) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%429 = "tt.extract"(%384) <{idx = 2 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%431 = "tt.cast"(%430) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%438 = "tt.extract"(%385) <{idx = 3 : i32}> : (tensor<16x32xi32>) -> tensor<8x16xi32>
%440 = "tt.cast"(%439) : (tensor<8x16xi32>) -> tensor<16x16xf16>
%446 = "tt.glue"(%445) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%456 = "tt.glue"(%455) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%466 = "tt.glue"(%465) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%476 = "tt.glue"(%475) : (tensor<8x16xf32>) -> tensor<8x16xf32>
%477 = "tt.advance"(%206, %26, %28) : (!tt.ptr<tensor<32x32xf16>, 1>, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
%481 = "llvm.insertelement"(%207, %480, %477) : (vector<2xi32>, i32, i32) -> vector<2xi32>
%483 = "tt.advance"(%204, %28, %26) : (!tt.ptr<tensor<32x32xf16>, 1>, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
%487 = "llvm.insertelement"(%205, %486, %484) : (vector<2xi32>, i32, i32) -> vector<2xi32>
%489 = "tt.advance"(%202, %28, %26) : (!tt.ptr<tensor<32x32xf16>, 1>, i32, i32) -> !tt.ptr<tensor<32x32xf16>, 1>
%493 = "llvm.insertelement"(%203, %492, %490) : (vector<2xi32>, i32, i32) -> vector<2xi32>
%498 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %91, %126) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%506 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %505, %126) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%514 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %513, %126) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%522 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %521, %126) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%530 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %91, %529) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%536 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %505, %529) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%542 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %513, %529) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%548 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %521, %529) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%554 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %91, %155) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%560 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %505, %155) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%566 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %513, %155) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%572 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %521, %155) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%580 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %91, %579) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%586 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %505, %579) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%592 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %513, %579) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
%598 = "tt.make_tensor_ptr"(%0, %23, %23, %23, %25, %521, %579) <{order = array<i32: 1, 0>}> : (!tt.ptr<f32, 1>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<8x16xf32>, 1>
"tt.store"(%503, %239) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%532, %237) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%561, %235) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%590, %233) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%619, %231) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%646, %229) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%673, %227) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%700, %225) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%727, %223) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%754, %221) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%781, %219) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%808, %217) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%837, %215) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%864, %213) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%891, %211) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
"tt.store"(%918, %209) <{boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32}> : (!tt.ptr<tensor<8x16xf32>, 1>, tensor<8x16xf32>) -> ()
// -----// IR Dump Before ConvertNVGPUToLLVM (convert-nv-gpu-to-llvm) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 16 : i32} {
  llvm.func @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {genx.intel_reqd_sub_group_size = [16 : i32], genx.kernel = 1 : i32, genx.max_work_group_size = [512 : i32, 1 : i32, 1 : i32], noinline = false} {
    %0 = builtin.unrealized_conversion_cast %arg2 : !llvm.ptr<1> to !tt.ptr<f32, 1> loc(#loc)
    %1 = builtin.unrealized_conversion_cast %arg1 : !llvm.ptr<1> to !tt.ptr<f16, 1> loc(#loc)
    %2 = builtin.unrealized_conversion_cast %arg0 : !llvm.ptr<1> to !tt.ptr<f16, 1> loc(#loc)
    %3 = llvm.mlir.constant(3 : i32) : i32 loc(#loc)
    %4 = llvm.mlir.constant(7 : i32) : i32 loc(#loc)
    %5 = llvm.mlir.constant(63 : i32) : i32 loc(#loc)
    %6 = llvm.mlir.constant(48 : i32) : i32 loc(#loc)
    %7 = llvm.mlir.constant(24 : i32) : i32 loc(#loc)
    %8 = llvm.mlir.constant(8 : i32) : i32 loc(#loc)
    %9 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %10 = llvm.mlir.constant(256 : i32) : i32 loc(#loc)
    %11 = llvm.mlir.constant(4096 : i64) : i64 loc(#loc)
    %12 = llvm.mlir.constant(1 : i64) : i64 loc(#loc)
    %13 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %14 = llvm.mlir.constant(32 : i32) : i32 loc(#loc)
    %15 = llvm.mlir.constant(dense<0.000000e+00> : vector<8xf32>) : vector<8xf32> loc(#loc)
    %16 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %17 = llvm.mlir.constant(16 : i32) : i32 loc(#loc)
    %18 = llvm.mlir.constant(64 : i32) : i32 loc(#loc)
    %19 = genx.workitem.id.x : i32 loc(#loc)
    %20 = genx.workitem.id.y : i32 loc(#loc)
    %21 = genx.workitem.id.z : i32 loc(#loc)
    %22 = genx.workgroup.dim.x : i32 loc(#loc)
    %23 = genx.workgroup.dim.y : i32 loc(#loc)
    %24 = llvm.mul %21, %23  : i32 loc(#loc)
    %25 = llvm.add %24, %20  : i32 loc(#loc)
    %26 = llvm.mul %25, %22  : i32 loc(#loc)
    %27 = llvm.add %26, %19  : i32 loc(#loc)
    %28 = llvm.mlir.constant(16 : i32) : i32 loc(#loc)
    %29 = llvm.udiv %27, %28  : i32 loc(#loc)
    %30 = genx.workgroup.id.x : i32 loc(#loc)
    %31 = llvm.sdiv %30, %18  : i32 loc(#loc)
    %32 = llvm.mul %31, %9  : i32 loc(#loc)
    %33 = llvm.sub %17, %32  : i32 loc(#loc)
    %34 = llvm.intr.smin(%33, %9)  : (i32, i32) -> i32 loc(#loc)
    %35 = llvm.srem %30, %34  : i32 loc(#loc)
    %36 = llvm.add %32, %35  : i32 loc(#loc)
    %37 = llvm.and %30, %5  : i32 loc(#loc)
    %38 = llvm.sdiv %37, %34  : i32 loc(#loc)
    %39 = llvm.mul %36, %10  : i32 loc(#loc)
    %40 = llvm.sdiv %29, %9  : i32 loc(#loc)
    %41 = llvm.and %40, %4  : i32 loc(#loc)
    %42 = llvm.mul %41, %14  : i32 loc(#loc)
    %43 = llvm.add %42, %39  : i32 loc(#loc)
    %44 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %45 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %46 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %47 = llvm.insertelement %13, %44[%45 : i32] : vector<2xi32> loc(#loc)
    %48 = llvm.insertelement %43, %47[%46 : i32] : vector<2xi32> loc(#loc)
    %49 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %50 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %51 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %52 = llvm.mul %51, %49  : i32 loc(#loc)
    %53 = llvm.sub %52, %50  : i32 loc(#loc)
    %54 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %55 = llvm.sub %54, %50  : i32 loc(#loc)
    %56 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %57 = llvm.mul %56, %49  : i32 loc(#loc)
    %58 = llvm.sub %57, %50  : i32 loc(#loc)
    %59 = llvm.mul %38, %10  : i32 loc(#loc)
    %60 = llvm.and %29, %3  : i32 loc(#loc)
    %61 = llvm.mul %60, %18  : i32 loc(#loc)
    %62 = llvm.add %61, %59  : i32 loc(#loc)
    %63 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %64 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %65 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %66 = llvm.insertelement %62, %63[%64 : i32] : vector<2xi32> loc(#loc)
    %67 = llvm.insertelement %13, %66[%65 : i32] : vector<2xi32> loc(#loc)
    %68 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %69 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %70 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %71 = llvm.mul %70, %68  : i32 loc(#loc)
    %72 = llvm.sub %71, %69  : i32 loc(#loc)
    %73 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %74 = llvm.sub %73, %69  : i32 loc(#loc)
    %75 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %76 = llvm.mul %75, %68  : i32 loc(#loc)
    %77 = llvm.sub %76, %69  : i32 loc(#loc)
    %78 = llvm.add %62, %14  : i32 loc(#loc)
    %79 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %80 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %81 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %82 = llvm.insertelement %78, %79[%80 : i32] : vector<2xi32> loc(#loc)
    %83 = llvm.insertelement %13, %82[%81 : i32] : vector<2xi32> loc(#loc)
    %84 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %85 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %86 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %87 = llvm.mul %86, %84  : i32 loc(#loc)
    %88 = llvm.sub %87, %85  : i32 loc(#loc)
    %89 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %90 = llvm.sub %89, %85  : i32 loc(#loc)
    %91 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %92 = llvm.mul %91, %84  : i32 loc(#loc)
    %93 = llvm.sub %92, %85  : i32 loc(#loc)
    llvm.br ^bb1(%13, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %15, %48, %67, %83 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb1(%94: i32 loc(unknown), %95: vector<8xf32> loc(unknown), %96: vector<8xf32> loc(unknown), %97: vector<8xf32> loc(unknown), %98: vector<8xf32> loc(unknown), %99: vector<8xf32> loc(unknown), %100: vector<8xf32> loc(unknown), %101: vector<8xf32> loc(unknown), %102: vector<8xf32> loc(unknown), %103: vector<8xf32> loc(unknown), %104: vector<8xf32> loc(unknown), %105: vector<8xf32> loc(unknown), %106: vector<8xf32> loc(unknown), %107: vector<8xf32> loc(unknown), %108: vector<8xf32> loc(unknown), %109: vector<8xf32> loc(unknown), %110: vector<8xf32> loc(unknown), %111: vector<2xi32> loc(unknown), %112: vector<2xi32> loc(unknown), %113: vector<2xi32> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %114 = builtin.unrealized_conversion_cast %113 : vector<2xi32> to !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    %115 = builtin.unrealized_conversion_cast %112 : vector<2xi32> to !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    %116 = builtin.unrealized_conversion_cast %111 : vector<2xi32> to !tt.ptr<tensor<32x32xf16>, 1> loc(#loc)
    %117 = builtin.unrealized_conversion_cast %110 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %118 = builtin.unrealized_conversion_cast %109 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %119 = builtin.unrealized_conversion_cast %108 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %120 = builtin.unrealized_conversion_cast %107 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %121 = builtin.unrealized_conversion_cast %106 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %122 = builtin.unrealized_conversion_cast %105 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %123 = builtin.unrealized_conversion_cast %104 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %124 = builtin.unrealized_conversion_cast %103 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %125 = builtin.unrealized_conversion_cast %102 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %126 = builtin.unrealized_conversion_cast %101 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %127 = builtin.unrealized_conversion_cast %100 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %128 = builtin.unrealized_conversion_cast %99 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %129 = builtin.unrealized_conversion_cast %98 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %130 = builtin.unrealized_conversion_cast %97 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %131 = builtin.unrealized_conversion_cast %96 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %132 = builtin.unrealized_conversion_cast %95 : vector<8xf32> to tensor<8x16xf32> loc(#loc)
    %133 = llvm.icmp "slt" %94, %16 : i32 loc(#loc)
    llvm.cond_br %133, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %134 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %135 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %136 = llvm.extractelement %111[%134 : i32] : vector<2xi32> loc(#loc)
    %137 = llvm.extractelement %111[%135 : i32] : vector<2xi32> loc(#loc)
    %138 = genx.matrix.2Dblockload %arg0, %53, %55, %58, %136, %137 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<64xi16> loc(#loc)
    %139 = llvm.bitcast %138 : vector<64xi16> to vector<64xf16> loc(#loc)
    %140 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %141 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %142 = llvm.extractelement %112[%140 : i32] : vector<2xi32> loc(#loc)
    %143 = llvm.extractelement %112[%141 : i32] : vector<2xi32> loc(#loc)
    %144 = genx.matrix.2Dblockload %arg1, %72, %74, %77, %142, %143 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %145 = llvm.bitcast %144 : vector<32xi32> to vector<64xf16> loc(#loc)
    %146 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %147 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %148 = llvm.extractelement %113[%146 : i32] : vector<2xi32> loc(#loc)
    %149 = llvm.extractelement %113[%147 : i32] : vector<2xi32> loc(#loc)
    %150 = genx.matrix.2Dblockload %arg1, %88, %90, %93, %148, %149 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %151 = llvm.bitcast %150 : vector<32xi32> to vector<64xf16> loc(#loc)
    %152 = llvm.bitcast %139 : vector<64xf16> to vector<64xi16> loc(#loc)
    %153 = llvm.shufflevector %152, %152 [0, 1, 2, 3, 4, 5, 6, 7] : vector<64xi16>  loc(#loc)
    %154 = llvm.bitcast %153 : vector<8xi16> to vector<8xf16> loc(#loc)
    %155 = llvm.bitcast %145 : vector<64xf16> to vector<32xi32> loc(#loc)
    %156 = llvm.shufflevector %155, %155 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %157 = llvm.bitcast %156 : vector<8xi32> to vector<16xf16> loc(#loc)
    %158 = llvm.bitcast %154 : vector<8xf16> to vector<8xi16> loc(#loc)
    %159 = llvm.bitcast %157 : vector<16xf16> to vector<8xi32> loc(#loc)
    %160 = genx.matrix.dpas %95, %158, %159 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %161 = llvm.shufflevector %152, %152 [32, 33, 34, 35, 36, 37, 38, 39] : vector<64xi16>  loc(#loc)
    %162 = llvm.bitcast %161 : vector<8xi16> to vector<8xf16> loc(#loc)
    %163 = llvm.shufflevector %155, %155 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %164 = llvm.bitcast %163 : vector<8xi32> to vector<16xf16> loc(#loc)
    %165 = llvm.bitcast %162 : vector<8xf16> to vector<8xi16> loc(#loc)
    %166 = llvm.bitcast %164 : vector<16xf16> to vector<8xi32> loc(#loc)
    %167 = genx.matrix.dpas %160, %165, %166 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %168 = llvm.shufflevector %152, %152 [8, 9, 10, 11, 12, 13, 14, 15] : vector<64xi16>  loc(#loc)
    %169 = llvm.bitcast %168 : vector<8xi16> to vector<8xf16> loc(#loc)
    %170 = llvm.bitcast %169 : vector<8xf16> to vector<8xi16> loc(#loc)
    %171 = llvm.bitcast %157 : vector<16xf16> to vector<8xi32> loc(#loc)
    %172 = genx.matrix.dpas %96, %170, %171 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %173 = llvm.shufflevector %152, %152 [40, 41, 42, 43, 44, 45, 46, 47] : vector<64xi16>  loc(#loc)
    %174 = llvm.bitcast %173 : vector<8xi16> to vector<8xf16> loc(#loc)
    %175 = llvm.bitcast %174 : vector<8xf16> to vector<8xi16> loc(#loc)
    %176 = llvm.bitcast %164 : vector<16xf16> to vector<8xi32> loc(#loc)
    %177 = genx.matrix.dpas %172, %175, %176 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %178 = llvm.shufflevector %152, %152 [16, 17, 18, 19, 20, 21, 22, 23] : vector<64xi16>  loc(#loc)
    %179 = llvm.bitcast %178 : vector<8xi16> to vector<8xf16> loc(#loc)
    %180 = llvm.bitcast %179 : vector<8xf16> to vector<8xi16> loc(#loc)
    %181 = llvm.bitcast %157 : vector<16xf16> to vector<8xi32> loc(#loc)
    %182 = genx.matrix.dpas %97, %180, %181 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %183 = llvm.shufflevector %152, %152 [48, 49, 50, 51, 52, 53, 54, 55] : vector<64xi16>  loc(#loc)
    %184 = llvm.bitcast %183 : vector<8xi16> to vector<8xf16> loc(#loc)
    %185 = llvm.bitcast %184 : vector<8xf16> to vector<8xi16> loc(#loc)
    %186 = llvm.bitcast %164 : vector<16xf16> to vector<8xi32> loc(#loc)
    %187 = genx.matrix.dpas %182, %185, %186 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %188 = llvm.shufflevector %152, %152 [24, 25, 26, 27, 28, 29, 30, 31] : vector<64xi16>  loc(#loc)
    %189 = llvm.bitcast %188 : vector<8xi16> to vector<8xf16> loc(#loc)
    %190 = llvm.bitcast %189 : vector<8xf16> to vector<8xi16> loc(#loc)
    %191 = llvm.bitcast %157 : vector<16xf16> to vector<8xi32> loc(#loc)
    %192 = genx.matrix.dpas %98, %190, %191 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %193 = llvm.shufflevector %152, %152 [56, 57, 58, 59, 60, 61, 62, 63] : vector<64xi16>  loc(#loc)
    %194 = llvm.bitcast %193 : vector<8xi16> to vector<8xf16> loc(#loc)
    %195 = llvm.bitcast %194 : vector<8xf16> to vector<8xi16> loc(#loc)
    %196 = llvm.bitcast %164 : vector<16xf16> to vector<8xi32> loc(#loc)
    %197 = genx.matrix.dpas %192, %195, %196 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %198 = llvm.shufflevector %155, %155 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %199 = llvm.bitcast %198 : vector<8xi32> to vector<16xf16> loc(#loc)
    %200 = llvm.bitcast %154 : vector<8xf16> to vector<8xi16> loc(#loc)
    %201 = llvm.bitcast %199 : vector<16xf16> to vector<8xi32> loc(#loc)
    %202 = genx.matrix.dpas %99, %200, %201 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %203 = llvm.shufflevector %155, %155 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %204 = llvm.bitcast %203 : vector<8xi32> to vector<16xf16> loc(#loc)
    %205 = llvm.bitcast %162 : vector<8xf16> to vector<8xi16> loc(#loc)
    %206 = llvm.bitcast %204 : vector<16xf16> to vector<8xi32> loc(#loc)
    %207 = genx.matrix.dpas %202, %205, %206 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %208 = llvm.bitcast %169 : vector<8xf16> to vector<8xi16> loc(#loc)
    %209 = llvm.bitcast %199 : vector<16xf16> to vector<8xi32> loc(#loc)
    %210 = genx.matrix.dpas %100, %208, %209 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %211 = llvm.bitcast %174 : vector<8xf16> to vector<8xi16> loc(#loc)
    %212 = llvm.bitcast %204 : vector<16xf16> to vector<8xi32> loc(#loc)
    %213 = genx.matrix.dpas %210, %211, %212 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %214 = llvm.bitcast %179 : vector<8xf16> to vector<8xi16> loc(#loc)
    %215 = llvm.bitcast %199 : vector<16xf16> to vector<8xi32> loc(#loc)
    %216 = genx.matrix.dpas %101, %214, %215 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %217 = llvm.bitcast %184 : vector<8xf16> to vector<8xi16> loc(#loc)
    %218 = llvm.bitcast %204 : vector<16xf16> to vector<8xi32> loc(#loc)
    %219 = genx.matrix.dpas %216, %217, %218 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %220 = llvm.bitcast %189 : vector<8xf16> to vector<8xi16> loc(#loc)
    %221 = llvm.bitcast %199 : vector<16xf16> to vector<8xi32> loc(#loc)
    %222 = genx.matrix.dpas %102, %220, %221 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %223 = llvm.bitcast %194 : vector<8xf16> to vector<8xi16> loc(#loc)
    %224 = llvm.bitcast %204 : vector<16xf16> to vector<8xi32> loc(#loc)
    %225 = genx.matrix.dpas %222, %223, %224 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %226 = llvm.bitcast %151 : vector<64xf16> to vector<32xi32> loc(#loc)
    %227 = llvm.shufflevector %226, %226 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %228 = llvm.bitcast %227 : vector<8xi32> to vector<16xf16> loc(#loc)
    %229 = llvm.bitcast %154 : vector<8xf16> to vector<8xi16> loc(#loc)
    %230 = llvm.bitcast %228 : vector<16xf16> to vector<8xi32> loc(#loc)
    %231 = genx.matrix.dpas %103, %229, %230 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %232 = llvm.shufflevector %226, %226 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %233 = llvm.bitcast %232 : vector<8xi32> to vector<16xf16> loc(#loc)
    %234 = llvm.bitcast %162 : vector<8xf16> to vector<8xi16> loc(#loc)
    %235 = llvm.bitcast %233 : vector<16xf16> to vector<8xi32> loc(#loc)
    %236 = genx.matrix.dpas %231, %234, %235 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %237 = llvm.bitcast %169 : vector<8xf16> to vector<8xi16> loc(#loc)
    %238 = llvm.bitcast %228 : vector<16xf16> to vector<8xi32> loc(#loc)
    %239 = genx.matrix.dpas %104, %237, %238 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %240 = llvm.bitcast %174 : vector<8xf16> to vector<8xi16> loc(#loc)
    %241 = llvm.bitcast %233 : vector<16xf16> to vector<8xi32> loc(#loc)
    %242 = genx.matrix.dpas %239, %240, %241 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %243 = llvm.bitcast %179 : vector<8xf16> to vector<8xi16> loc(#loc)
    %244 = llvm.bitcast %228 : vector<16xf16> to vector<8xi32> loc(#loc)
    %245 = genx.matrix.dpas %105, %243, %244 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %246 = llvm.bitcast %184 : vector<8xf16> to vector<8xi16> loc(#loc)
    %247 = llvm.bitcast %233 : vector<16xf16> to vector<8xi32> loc(#loc)
    %248 = genx.matrix.dpas %245, %246, %247 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %249 = llvm.bitcast %189 : vector<8xf16> to vector<8xi16> loc(#loc)
    %250 = llvm.bitcast %228 : vector<16xf16> to vector<8xi32> loc(#loc)
    %251 = genx.matrix.dpas %106, %249, %250 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %252 = llvm.bitcast %194 : vector<8xf16> to vector<8xi16> loc(#loc)
    %253 = llvm.bitcast %233 : vector<16xf16> to vector<8xi32> loc(#loc)
    %254 = genx.matrix.dpas %251, %252, %253 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %255 = llvm.shufflevector %226, %226 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %256 = llvm.bitcast %255 : vector<8xi32> to vector<16xf16> loc(#loc)
    %257 = llvm.bitcast %154 : vector<8xf16> to vector<8xi16> loc(#loc)
    %258 = llvm.bitcast %256 : vector<16xf16> to vector<8xi32> loc(#loc)
    %259 = genx.matrix.dpas %107, %257, %258 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %260 = llvm.shufflevector %226, %226 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %261 = llvm.bitcast %260 : vector<8xi32> to vector<16xf16> loc(#loc)
    %262 = llvm.bitcast %162 : vector<8xf16> to vector<8xi16> loc(#loc)
    %263 = llvm.bitcast %261 : vector<16xf16> to vector<8xi32> loc(#loc)
    %264 = genx.matrix.dpas %259, %262, %263 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %265 = llvm.bitcast %169 : vector<8xf16> to vector<8xi16> loc(#loc)
    %266 = llvm.bitcast %256 : vector<16xf16> to vector<8xi32> loc(#loc)
    %267 = genx.matrix.dpas %108, %265, %266 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %268 = llvm.bitcast %174 : vector<8xf16> to vector<8xi16> loc(#loc)
    %269 = llvm.bitcast %261 : vector<16xf16> to vector<8xi32> loc(#loc)
    %270 = genx.matrix.dpas %267, %268, %269 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %271 = llvm.bitcast %179 : vector<8xf16> to vector<8xi16> loc(#loc)
    %272 = llvm.bitcast %256 : vector<16xf16> to vector<8xi32> loc(#loc)
    %273 = genx.matrix.dpas %109, %271, %272 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %274 = llvm.bitcast %184 : vector<8xf16> to vector<8xi16> loc(#loc)
    %275 = llvm.bitcast %261 : vector<16xf16> to vector<8xi32> loc(#loc)
    %276 = genx.matrix.dpas %273, %274, %275 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %277 = llvm.bitcast %189 : vector<8xf16> to vector<8xi16> loc(#loc)
    %278 = llvm.bitcast %256 : vector<16xf16> to vector<8xi32> loc(#loc)
    %279 = genx.matrix.dpas %110, %277, %278 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %280 = llvm.bitcast %194 : vector<8xf16> to vector<8xi16> loc(#loc)
    %281 = llvm.bitcast %261 : vector<16xf16> to vector<8xi32> loc(#loc)
    %282 = genx.matrix.dpas %279, %280, %281 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %283 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %284 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %285 = llvm.extractelement %111[%283 : i32] : vector<2xi32> loc(#loc)
    %286 = llvm.add %285, %14  : i32 loc(#loc)
    %287 = llvm.insertelement %286, %111[%283 : i32] : vector<2xi32> loc(#loc)
    %288 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %289 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %290 = llvm.extractelement %112[%289 : i32] : vector<2xi32> loc(#loc)
    %291 = llvm.add %290, %14  : i32 loc(#loc)
    %292 = llvm.insertelement %291, %112[%289 : i32] : vector<2xi32> loc(#loc)
    %293 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %294 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %295 = llvm.extractelement %113[%294 : i32] : vector<2xi32> loc(#loc)
    %296 = llvm.add %295, %14  : i32 loc(#loc)
    %297 = llvm.insertelement %296, %113[%294 : i32] : vector<2xi32> loc(#loc)
    %298 = llvm.add %94, %14  : i32 loc(#loc)
    llvm.br ^bb1(%298, %167, %177, %187, %197, %207, %213, %219, %225, %236, %242, %248, %254, %264, %270, %276, %282, %287, %292, %297 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %299 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %300 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %301 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %302 = llvm.insertelement %62, %299[%300 : i32] : vector<2xi32> loc(#loc)
    %303 = llvm.insertelement %43, %302[%301 : i32] : vector<2xi32> loc(#loc)
    %304 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %305 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %306 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %307 = llvm.mul %306, %304  : i32 loc(#loc)
    %308 = llvm.sub %307, %305  : i32 loc(#loc)
    %309 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %310 = llvm.sub %309, %305  : i32 loc(#loc)
    %311 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %312 = llvm.mul %311, %304  : i32 loc(#loc)
    %313 = llvm.sub %312, %305  : i32 loc(#loc)
    %314 = llvm.add %43, %8  : i32 loc(#loc)
    %315 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %316 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %317 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %318 = llvm.insertelement %62, %315[%316 : i32] : vector<2xi32> loc(#loc)
    %319 = llvm.insertelement %314, %318[%317 : i32] : vector<2xi32> loc(#loc)
    %320 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %321 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %322 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %323 = llvm.mul %322, %320  : i32 loc(#loc)
    %324 = llvm.sub %323, %321  : i32 loc(#loc)
    %325 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %326 = llvm.sub %325, %321  : i32 loc(#loc)
    %327 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %328 = llvm.mul %327, %320  : i32 loc(#loc)
    %329 = llvm.sub %328, %321  : i32 loc(#loc)
    %330 = llvm.add %43, %17  : i32 loc(#loc)
    %331 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %332 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %333 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %334 = llvm.insertelement %62, %331[%332 : i32] : vector<2xi32> loc(#loc)
    %335 = llvm.insertelement %330, %334[%333 : i32] : vector<2xi32> loc(#loc)
    %336 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %337 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %338 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %339 = llvm.mul %338, %336  : i32 loc(#loc)
    %340 = llvm.sub %339, %337  : i32 loc(#loc)
    %341 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %342 = llvm.sub %341, %337  : i32 loc(#loc)
    %343 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %344 = llvm.mul %343, %336  : i32 loc(#loc)
    %345 = llvm.sub %344, %337  : i32 loc(#loc)
    %346 = llvm.add %43, %7  : i32 loc(#loc)
    %347 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %348 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %349 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %350 = llvm.insertelement %62, %347[%348 : i32] : vector<2xi32> loc(#loc)
    %351 = llvm.insertelement %346, %350[%349 : i32] : vector<2xi32> loc(#loc)
    %352 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %353 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %354 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %355 = llvm.mul %354, %352  : i32 loc(#loc)
    %356 = llvm.sub %355, %353  : i32 loc(#loc)
    %357 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %358 = llvm.sub %357, %353  : i32 loc(#loc)
    %359 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %360 = llvm.mul %359, %352  : i32 loc(#loc)
    %361 = llvm.sub %360, %353  : i32 loc(#loc)
    %362 = llvm.add %62, %17  : i32 loc(#loc)
    %363 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %364 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %365 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %366 = llvm.insertelement %362, %363[%364 : i32] : vector<2xi32> loc(#loc)
    %367 = llvm.insertelement %43, %366[%365 : i32] : vector<2xi32> loc(#loc)
    %368 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %369 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %370 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %371 = llvm.mul %370, %368  : i32 loc(#loc)
    %372 = llvm.sub %371, %369  : i32 loc(#loc)
    %373 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %374 = llvm.sub %373, %369  : i32 loc(#loc)
    %375 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %376 = llvm.mul %375, %368  : i32 loc(#loc)
    %377 = llvm.sub %376, %369  : i32 loc(#loc)
    %378 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %379 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %380 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %381 = llvm.insertelement %362, %378[%379 : i32] : vector<2xi32> loc(#loc)
    %382 = llvm.insertelement %314, %381[%380 : i32] : vector<2xi32> loc(#loc)
    %383 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %384 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %385 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %386 = llvm.mul %385, %383  : i32 loc(#loc)
    %387 = llvm.sub %386, %384  : i32 loc(#loc)
    %388 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %389 = llvm.sub %388, %384  : i32 loc(#loc)
    %390 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %391 = llvm.mul %390, %383  : i32 loc(#loc)
    %392 = llvm.sub %391, %384  : i32 loc(#loc)
    %393 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %394 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %395 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %396 = llvm.insertelement %362, %393[%394 : i32] : vector<2xi32> loc(#loc)
    %397 = llvm.insertelement %330, %396[%395 : i32] : vector<2xi32> loc(#loc)
    %398 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %399 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %400 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %401 = llvm.mul %400, %398  : i32 loc(#loc)
    %402 = llvm.sub %401, %399  : i32 loc(#loc)
    %403 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %404 = llvm.sub %403, %399  : i32 loc(#loc)
    %405 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %406 = llvm.mul %405, %398  : i32 loc(#loc)
    %407 = llvm.sub %406, %399  : i32 loc(#loc)
    %408 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %409 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %410 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %411 = llvm.insertelement %362, %408[%409 : i32] : vector<2xi32> loc(#loc)
    %412 = llvm.insertelement %346, %411[%410 : i32] : vector<2xi32> loc(#loc)
    %413 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %414 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %415 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %416 = llvm.mul %415, %413  : i32 loc(#loc)
    %417 = llvm.sub %416, %414  : i32 loc(#loc)
    %418 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %419 = llvm.sub %418, %414  : i32 loc(#loc)
    %420 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %421 = llvm.mul %420, %413  : i32 loc(#loc)
    %422 = llvm.sub %421, %414  : i32 loc(#loc)
    %423 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %424 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %425 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %426 = llvm.insertelement %78, %423[%424 : i32] : vector<2xi32> loc(#loc)
    %427 = llvm.insertelement %43, %426[%425 : i32] : vector<2xi32> loc(#loc)
    %428 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %429 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %430 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %431 = llvm.mul %430, %428  : i32 loc(#loc)
    %432 = llvm.sub %431, %429  : i32 loc(#loc)
    %433 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %434 = llvm.sub %433, %429  : i32 loc(#loc)
    %435 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %436 = llvm.mul %435, %428  : i32 loc(#loc)
    %437 = llvm.sub %436, %429  : i32 loc(#loc)
    %438 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %439 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %440 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %441 = llvm.insertelement %78, %438[%439 : i32] : vector<2xi32> loc(#loc)
    %442 = llvm.insertelement %314, %441[%440 : i32] : vector<2xi32> loc(#loc)
    %443 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %444 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %445 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %446 = llvm.mul %445, %443  : i32 loc(#loc)
    %447 = llvm.sub %446, %444  : i32 loc(#loc)
    %448 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %449 = llvm.sub %448, %444  : i32 loc(#loc)
    %450 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %451 = llvm.mul %450, %443  : i32 loc(#loc)
    %452 = llvm.sub %451, %444  : i32 loc(#loc)
    %453 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %454 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %455 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %456 = llvm.insertelement %78, %453[%454 : i32] : vector<2xi32> loc(#loc)
    %457 = llvm.insertelement %330, %456[%455 : i32] : vector<2xi32> loc(#loc)
    %458 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %459 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %460 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %461 = llvm.mul %460, %458  : i32 loc(#loc)
    %462 = llvm.sub %461, %459  : i32 loc(#loc)
    %463 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %464 = llvm.sub %463, %459  : i32 loc(#loc)
    %465 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %466 = llvm.mul %465, %458  : i32 loc(#loc)
    %467 = llvm.sub %466, %459  : i32 loc(#loc)
    %468 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %469 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %470 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %471 = llvm.insertelement %78, %468[%469 : i32] : vector<2xi32> loc(#loc)
    %472 = llvm.insertelement %346, %471[%470 : i32] : vector<2xi32> loc(#loc)
    %473 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %474 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %475 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %476 = llvm.mul %475, %473  : i32 loc(#loc)
    %477 = llvm.sub %476, %474  : i32 loc(#loc)
    %478 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %479 = llvm.sub %478, %474  : i32 loc(#loc)
    %480 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %481 = llvm.mul %480, %473  : i32 loc(#loc)
    %482 = llvm.sub %481, %474  : i32 loc(#loc)
    %483 = llvm.add %62, %6  : i32 loc(#loc)
    %484 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %485 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %486 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %487 = llvm.insertelement %483, %484[%485 : i32] : vector<2xi32> loc(#loc)
    %488 = llvm.insertelement %43, %487[%486 : i32] : vector<2xi32> loc(#loc)
    %489 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %490 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %491 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %492 = llvm.mul %491, %489  : i32 loc(#loc)
    %493 = llvm.sub %492, %490  : i32 loc(#loc)
    %494 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %495 = llvm.sub %494, %490  : i32 loc(#loc)
    %496 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %497 = llvm.mul %496, %489  : i32 loc(#loc)
    %498 = llvm.sub %497, %490  : i32 loc(#loc)
    %499 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %500 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %501 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %502 = llvm.insertelement %483, %499[%500 : i32] : vector<2xi32> loc(#loc)
    %503 = llvm.insertelement %314, %502[%501 : i32] : vector<2xi32> loc(#loc)
    %504 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %505 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %506 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %507 = llvm.mul %506, %504  : i32 loc(#loc)
    %508 = llvm.sub %507, %505  : i32 loc(#loc)
    %509 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %510 = llvm.sub %509, %505  : i32 loc(#loc)
    %511 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %512 = llvm.mul %511, %504  : i32 loc(#loc)
    %513 = llvm.sub %512, %505  : i32 loc(#loc)
    %514 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %515 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %516 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %517 = llvm.insertelement %483, %514[%515 : i32] : vector<2xi32> loc(#loc)
    %518 = llvm.insertelement %330, %517[%516 : i32] : vector<2xi32> loc(#loc)
    %519 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %520 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %521 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %522 = llvm.mul %521, %519  : i32 loc(#loc)
    %523 = llvm.sub %522, %520  : i32 loc(#loc)
    %524 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %525 = llvm.sub %524, %520  : i32 loc(#loc)
    %526 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %527 = llvm.mul %526, %519  : i32 loc(#loc)
    %528 = llvm.sub %527, %520  : i32 loc(#loc)
    %529 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %530 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %531 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %532 = llvm.insertelement %483, %529[%530 : i32] : vector<2xi32> loc(#loc)
    %533 = llvm.insertelement %346, %532[%531 : i32] : vector<2xi32> loc(#loc)
    %534 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %535 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %536 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %537 = llvm.mul %536, %534  : i32 loc(#loc)
    %538 = llvm.sub %537, %535  : i32 loc(#loc)
    %539 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %540 = llvm.sub %539, %535  : i32 loc(#loc)
    %541 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %542 = llvm.mul %541, %534  : i32 loc(#loc)
    %543 = llvm.sub %542, %535  : i32 loc(#loc)
    %544 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %545 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %546 = llvm.extractelement %303[%544 : i32] : vector<2xi32> loc(#loc)
    %547 = llvm.extractelement %303[%545 : i32] : vector<2xi32> loc(#loc)
    %548 = llvm.bitcast %95 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %308, %310, %313, %546, %547, %548 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %549 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %550 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %551 = llvm.extractelement %319[%549 : i32] : vector<2xi32> loc(#loc)
    %552 = llvm.extractelement %319[%550 : i32] : vector<2xi32> loc(#loc)
    %553 = llvm.bitcast %96 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %324, %326, %329, %551, %552, %553 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %554 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %555 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %556 = llvm.extractelement %335[%554 : i32] : vector<2xi32> loc(#loc)
    %557 = llvm.extractelement %335[%555 : i32] : vector<2xi32> loc(#loc)
    %558 = llvm.bitcast %97 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %340, %342, %345, %556, %557, %558 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %559 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %560 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %561 = llvm.extractelement %351[%559 : i32] : vector<2xi32> loc(#loc)
    %562 = llvm.extractelement %351[%560 : i32] : vector<2xi32> loc(#loc)
    %563 = llvm.bitcast %98 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %356, %358, %361, %561, %562, %563 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %564 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %565 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %566 = llvm.extractelement %367[%564 : i32] : vector<2xi32> loc(#loc)
    %567 = llvm.extractelement %367[%565 : i32] : vector<2xi32> loc(#loc)
    %568 = llvm.bitcast %99 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %372, %374, %377, %566, %567, %568 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %569 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %570 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %571 = llvm.extractelement %382[%569 : i32] : vector<2xi32> loc(#loc)
    %572 = llvm.extractelement %382[%570 : i32] : vector<2xi32> loc(#loc)
    %573 = llvm.bitcast %100 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %387, %389, %392, %571, %572, %573 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %574 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %575 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %576 = llvm.extractelement %397[%574 : i32] : vector<2xi32> loc(#loc)
    %577 = llvm.extractelement %397[%575 : i32] : vector<2xi32> loc(#loc)
    %578 = llvm.bitcast %101 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %402, %404, %407, %576, %577, %578 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %579 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %580 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %581 = llvm.extractelement %412[%579 : i32] : vector<2xi32> loc(#loc)
    %582 = llvm.extractelement %412[%580 : i32] : vector<2xi32> loc(#loc)
    %583 = llvm.bitcast %102 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %417, %419, %422, %581, %582, %583 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %584 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %585 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %586 = llvm.extractelement %427[%584 : i32] : vector<2xi32> loc(#loc)
    %587 = llvm.extractelement %427[%585 : i32] : vector<2xi32> loc(#loc)
    %588 = llvm.bitcast %103 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %432, %434, %437, %586, %587, %588 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %589 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %590 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %591 = llvm.extractelement %442[%589 : i32] : vector<2xi32> loc(#loc)
    %592 = llvm.extractelement %442[%590 : i32] : vector<2xi32> loc(#loc)
    %593 = llvm.bitcast %104 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %447, %449, %452, %591, %592, %593 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %594 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %595 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %596 = llvm.extractelement %457[%594 : i32] : vector<2xi32> loc(#loc)
    %597 = llvm.extractelement %457[%595 : i32] : vector<2xi32> loc(#loc)
    %598 = llvm.bitcast %105 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %462, %464, %467, %596, %597, %598 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %599 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %600 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %601 = llvm.extractelement %472[%599 : i32] : vector<2xi32> loc(#loc)
    %602 = llvm.extractelement %472[%600 : i32] : vector<2xi32> loc(#loc)
    %603 = llvm.bitcast %106 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %477, %479, %482, %601, %602, %603 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %604 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %605 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %606 = llvm.extractelement %488[%604 : i32] : vector<2xi32> loc(#loc)
    %607 = llvm.extractelement %488[%605 : i32] : vector<2xi32> loc(#loc)
    %608 = llvm.bitcast %107 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %493, %495, %498, %606, %607, %608 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %609 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %610 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %611 = llvm.extractelement %503[%609 : i32] : vector<2xi32> loc(#loc)
    %612 = llvm.extractelement %503[%610 : i32] : vector<2xi32> loc(#loc)
    %613 = llvm.bitcast %108 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %508, %510, %513, %611, %612, %613 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %614 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %615 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %616 = llvm.extractelement %518[%614 : i32] : vector<2xi32> loc(#loc)
    %617 = llvm.extractelement %518[%615 : i32] : vector<2xi32> loc(#loc)
    %618 = llvm.bitcast %109 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %523, %525, %528, %616, %617, %618 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %619 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %620 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %621 = llvm.extractelement %533[%619 : i32] : vector<2xi32> loc(#loc)
    %622 = llvm.extractelement %533[%620 : i32] : vector<2xi32> loc(#loc)
    %623 = llvm.bitcast %110 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %538, %540, %543, %621, %622, %623 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    llvm.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before ArithToLLVMConversionPass (convert-arith-to-llvm) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 16 : i32} {
  llvm.func @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {genx.intel_reqd_sub_group_size = [16 : i32], genx.kernel = 1 : i32, genx.max_work_group_size = [512 : i32, 1 : i32, 1 : i32], noinline = false} {
    %0 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %1 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %2 = llvm.mlir.constant(64 : i32) : i32 loc(#loc)
    %3 = llvm.mlir.constant(16 : i32) : i32 loc(#loc)
    %4 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %5 = llvm.mlir.constant(dense<0.000000e+00> : vector<8xf32>) : vector<8xf32> loc(#loc)
    %6 = llvm.mlir.constant(32 : i32) : i32 loc(#loc)
    %7 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %8 = llvm.mlir.constant(256 : i32) : i32 loc(#loc)
    %9 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %10 = llvm.mlir.constant(8 : i32) : i32 loc(#loc)
    %11 = llvm.mlir.constant(24 : i32) : i32 loc(#loc)
    %12 = llvm.mlir.constant(48 : i32) : i32 loc(#loc)
    %13 = llvm.mlir.constant(63 : i32) : i32 loc(#loc)
    %14 = llvm.mlir.constant(7 : i32) : i32 loc(#loc)
    %15 = llvm.mlir.constant(3 : i32) : i32 loc(#loc)
    %16 = genx.workitem.id.x : i32 loc(#loc)
    %17 = genx.workitem.id.y : i32 loc(#loc)
    %18 = genx.workitem.id.z : i32 loc(#loc)
    %19 = genx.workgroup.dim.x : i32 loc(#loc)
    %20 = genx.workgroup.dim.y : i32 loc(#loc)
    %21 = llvm.mul %18, %20  : i32 loc(#loc)
    %22 = llvm.add %21, %17  : i32 loc(#loc)
    %23 = llvm.mul %22, %19  : i32 loc(#loc)
    %24 = llvm.add %23, %16  : i32 loc(#loc)
    %25 = llvm.udiv %24, %3  : i32 loc(#loc)
    %26 = genx.workgroup.id.x : i32 loc(#loc)
    %27 = llvm.sdiv %26, %2  : i32 loc(#loc)
    %28 = llvm.mul %27, %9  : i32 loc(#loc)
    %29 = llvm.sub %3, %28  : i32 loc(#loc)
    %30 = llvm.intr.smin(%29, %9)  : (i32, i32) -> i32 loc(#loc)
    %31 = llvm.srem %26, %30  : i32 loc(#loc)
    %32 = llvm.add %28, %31  : i32 loc(#loc)
    %33 = llvm.and %26, %13  : i32 loc(#loc)
    %34 = llvm.sdiv %33, %30  : i32 loc(#loc)
    %35 = llvm.mul %32, %8  : i32 loc(#loc)
    %36 = llvm.sdiv %25, %9  : i32 loc(#loc)
    %37 = llvm.and %36, %14  : i32 loc(#loc)
    %38 = llvm.mul %37, %6  : i32 loc(#loc)
    %39 = llvm.add %38, %35  : i32 loc(#loc)
    %40 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %41 = llvm.insertelement %7, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %42 = llvm.insertelement %39, %41[%1 : i32] : vector<2xi32> loc(#loc)
    %43 = llvm.mul %4, %0  : i32 loc(#loc)
    %44 = llvm.sub %43, %1  : i32 loc(#loc)
    %45 = llvm.sub %4, %1  : i32 loc(#loc)
    %46 = llvm.mul %4, %0  : i32 loc(#loc)
    %47 = llvm.sub %46, %1  : i32 loc(#loc)
    %48 = llvm.mul %34, %8  : i32 loc(#loc)
    %49 = llvm.and %25, %15  : i32 loc(#loc)
    %50 = llvm.mul %49, %2  : i32 loc(#loc)
    %51 = llvm.add %50, %48  : i32 loc(#loc)
    %52 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %53 = llvm.insertelement %51, %52[%7 : i32] : vector<2xi32> loc(#loc)
    %54 = llvm.insertelement %7, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %55 = llvm.mul %4, %0  : i32 loc(#loc)
    %56 = llvm.sub %55, %1  : i32 loc(#loc)
    %57 = llvm.sub %4, %1  : i32 loc(#loc)
    %58 = llvm.mul %4, %0  : i32 loc(#loc)
    %59 = llvm.sub %58, %1  : i32 loc(#loc)
    %60 = llvm.add %51, %6  : i32 loc(#loc)
    %61 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %62 = llvm.insertelement %60, %61[%7 : i32] : vector<2xi32> loc(#loc)
    %63 = llvm.insertelement %7, %62[%1 : i32] : vector<2xi32> loc(#loc)
    %64 = llvm.mul %4, %0  : i32 loc(#loc)
    %65 = llvm.sub %64, %1  : i32 loc(#loc)
    %66 = llvm.sub %4, %1  : i32 loc(#loc)
    %67 = llvm.mul %4, %0  : i32 loc(#loc)
    %68 = llvm.sub %67, %1  : i32 loc(#loc)
    llvm.br ^bb1(%7, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %42, %54, %63 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb1(%69: i32 loc(unknown), %70: vector<8xf32> loc(unknown), %71: vector<8xf32> loc(unknown), %72: vector<8xf32> loc(unknown), %73: vector<8xf32> loc(unknown), %74: vector<8xf32> loc(unknown), %75: vector<8xf32> loc(unknown), %76: vector<8xf32> loc(unknown), %77: vector<8xf32> loc(unknown), %78: vector<8xf32> loc(unknown), %79: vector<8xf32> loc(unknown), %80: vector<8xf32> loc(unknown), %81: vector<8xf32> loc(unknown), %82: vector<8xf32> loc(unknown), %83: vector<8xf32> loc(unknown), %84: vector<8xf32> loc(unknown), %85: vector<8xf32> loc(unknown), %86: vector<2xi32> loc(unknown), %87: vector<2xi32> loc(unknown), %88: vector<2xi32> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %89 = llvm.icmp "slt" %69, %4 : i32 loc(#loc)
    llvm.cond_br %89, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %90 = llvm.extractelement %86[%7 : i32] : vector<2xi32> loc(#loc)
    %91 = llvm.extractelement %86[%1 : i32] : vector<2xi32> loc(#loc)
    %92 = genx.matrix.2Dblockload %arg0, %44, %45, %47, %90, %91 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<64xi16> loc(#loc)
    %93 = llvm.extractelement %87[%7 : i32] : vector<2xi32> loc(#loc)
    %94 = llvm.extractelement %87[%1 : i32] : vector<2xi32> loc(#loc)
    %95 = genx.matrix.2Dblockload %arg1, %56, %57, %59, %93, %94 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %96 = llvm.extractelement %88[%7 : i32] : vector<2xi32> loc(#loc)
    %97 = llvm.extractelement %88[%1 : i32] : vector<2xi32> loc(#loc)
    %98 = genx.matrix.2Dblockload %arg1, %65, %66, %68, %96, %97 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %99 = llvm.shufflevector %92, %92 [0, 1, 2, 3, 4, 5, 6, 7] : vector<64xi16>  loc(#loc)
    %100 = llvm.shufflevector %95, %95 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %101 = genx.matrix.dpas %70, %99, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %102 = llvm.shufflevector %92, %92 [32, 33, 34, 35, 36, 37, 38, 39] : vector<64xi16>  loc(#loc)
    %103 = llvm.shufflevector %95, %95 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %104 = genx.matrix.dpas %101, %102, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %105 = llvm.shufflevector %92, %92 [8, 9, 10, 11, 12, 13, 14, 15] : vector<64xi16>  loc(#loc)
    %106 = genx.matrix.dpas %71, %105, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %107 = llvm.shufflevector %92, %92 [40, 41, 42, 43, 44, 45, 46, 47] : vector<64xi16>  loc(#loc)
    %108 = genx.matrix.dpas %106, %107, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %109 = llvm.shufflevector %92, %92 [16, 17, 18, 19, 20, 21, 22, 23] : vector<64xi16>  loc(#loc)
    %110 = genx.matrix.dpas %72, %109, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %111 = llvm.shufflevector %92, %92 [48, 49, 50, 51, 52, 53, 54, 55] : vector<64xi16>  loc(#loc)
    %112 = genx.matrix.dpas %110, %111, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %113 = llvm.shufflevector %92, %92 [24, 25, 26, 27, 28, 29, 30, 31] : vector<64xi16>  loc(#loc)
    %114 = genx.matrix.dpas %73, %113, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %115 = llvm.shufflevector %92, %92 [56, 57, 58, 59, 60, 61, 62, 63] : vector<64xi16>  loc(#loc)
    %116 = genx.matrix.dpas %114, %115, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %117 = llvm.shufflevector %95, %95 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %118 = genx.matrix.dpas %74, %99, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %119 = llvm.shufflevector %95, %95 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %120 = genx.matrix.dpas %118, %102, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %121 = genx.matrix.dpas %75, %105, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %122 = genx.matrix.dpas %121, %107, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %123 = genx.matrix.dpas %76, %109, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %124 = genx.matrix.dpas %123, %111, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %125 = genx.matrix.dpas %77, %113, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %126 = genx.matrix.dpas %125, %115, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %127 = llvm.shufflevector %98, %98 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %128 = genx.matrix.dpas %78, %99, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %129 = llvm.shufflevector %98, %98 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %130 = genx.matrix.dpas %128, %102, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %131 = genx.matrix.dpas %79, %105, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %132 = genx.matrix.dpas %131, %107, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %133 = genx.matrix.dpas %80, %109, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %134 = genx.matrix.dpas %133, %111, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %135 = genx.matrix.dpas %81, %113, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %136 = genx.matrix.dpas %135, %115, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %137 = llvm.shufflevector %98, %98 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %138 = genx.matrix.dpas %82, %99, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %139 = llvm.shufflevector %98, %98 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %140 = genx.matrix.dpas %138, %102, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %141 = genx.matrix.dpas %83, %105, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %142 = genx.matrix.dpas %141, %107, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %143 = genx.matrix.dpas %84, %109, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %144 = genx.matrix.dpas %143, %111, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %145 = genx.matrix.dpas %85, %113, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %146 = genx.matrix.dpas %145, %115, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %147 = llvm.extractelement %86[%7 : i32] : vector<2xi32> loc(#loc)
    %148 = llvm.add %147, %6  : i32 loc(#loc)
    %149 = llvm.insertelement %148, %86[%7 : i32] : vector<2xi32> loc(#loc)
    %150 = llvm.extractelement %87[%1 : i32] : vector<2xi32> loc(#loc)
    %151 = llvm.add %150, %6  : i32 loc(#loc)
    %152 = llvm.insertelement %151, %87[%1 : i32] : vector<2xi32> loc(#loc)
    %153 = llvm.extractelement %88[%1 : i32] : vector<2xi32> loc(#loc)
    %154 = llvm.add %153, %6  : i32 loc(#loc)
    %155 = llvm.insertelement %154, %88[%1 : i32] : vector<2xi32> loc(#loc)
    %156 = llvm.add %69, %6  : i32 loc(#loc)
    llvm.br ^bb1(%156, %104, %108, %112, %116, %120, %122, %124, %126, %130, %132, %134, %136, %140, %142, %144, %146, %149, %152, %155 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %157 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %158 = llvm.insertelement %51, %157[%7 : i32] : vector<2xi32> loc(#loc)
    %159 = llvm.insertelement %39, %158[%1 : i32] : vector<2xi32> loc(#loc)
    %160 = llvm.mul %4, %9  : i32 loc(#loc)
    %161 = llvm.sub %160, %1  : i32 loc(#loc)
    %162 = llvm.sub %4, %1  : i32 loc(#loc)
    %163 = llvm.mul %4, %9  : i32 loc(#loc)
    %164 = llvm.sub %163, %1  : i32 loc(#loc)
    %165 = llvm.add %39, %10  : i32 loc(#loc)
    %166 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %167 = llvm.insertelement %51, %166[%7 : i32] : vector<2xi32> loc(#loc)
    %168 = llvm.insertelement %165, %167[%1 : i32] : vector<2xi32> loc(#loc)
    %169 = llvm.mul %4, %9  : i32 loc(#loc)
    %170 = llvm.sub %169, %1  : i32 loc(#loc)
    %171 = llvm.sub %4, %1  : i32 loc(#loc)
    %172 = llvm.mul %4, %9  : i32 loc(#loc)
    %173 = llvm.sub %172, %1  : i32 loc(#loc)
    %174 = llvm.add %39, %3  : i32 loc(#loc)
    %175 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %176 = llvm.insertelement %51, %175[%7 : i32] : vector<2xi32> loc(#loc)
    %177 = llvm.insertelement %174, %176[%1 : i32] : vector<2xi32> loc(#loc)
    %178 = llvm.mul %4, %9  : i32 loc(#loc)
    %179 = llvm.sub %178, %1  : i32 loc(#loc)
    %180 = llvm.sub %4, %1  : i32 loc(#loc)
    %181 = llvm.mul %4, %9  : i32 loc(#loc)
    %182 = llvm.sub %181, %1  : i32 loc(#loc)
    %183 = llvm.add %39, %11  : i32 loc(#loc)
    %184 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %185 = llvm.insertelement %51, %184[%7 : i32] : vector<2xi32> loc(#loc)
    %186 = llvm.insertelement %183, %185[%1 : i32] : vector<2xi32> loc(#loc)
    %187 = llvm.mul %4, %9  : i32 loc(#loc)
    %188 = llvm.sub %187, %1  : i32 loc(#loc)
    %189 = llvm.sub %4, %1  : i32 loc(#loc)
    %190 = llvm.mul %4, %9  : i32 loc(#loc)
    %191 = llvm.sub %190, %1  : i32 loc(#loc)
    %192 = llvm.add %51, %3  : i32 loc(#loc)
    %193 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %194 = llvm.insertelement %192, %193[%7 : i32] : vector<2xi32> loc(#loc)
    %195 = llvm.insertelement %39, %194[%1 : i32] : vector<2xi32> loc(#loc)
    %196 = llvm.mul %4, %9  : i32 loc(#loc)
    %197 = llvm.sub %196, %1  : i32 loc(#loc)
    %198 = llvm.sub %4, %1  : i32 loc(#loc)
    %199 = llvm.mul %4, %9  : i32 loc(#loc)
    %200 = llvm.sub %199, %1  : i32 loc(#loc)
    %201 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %202 = llvm.insertelement %192, %201[%7 : i32] : vector<2xi32> loc(#loc)
    %203 = llvm.insertelement %165, %202[%1 : i32] : vector<2xi32> loc(#loc)
    %204 = llvm.mul %4, %9  : i32 loc(#loc)
    %205 = llvm.sub %204, %1  : i32 loc(#loc)
    %206 = llvm.sub %4, %1  : i32 loc(#loc)
    %207 = llvm.mul %4, %9  : i32 loc(#loc)
    %208 = llvm.sub %207, %1  : i32 loc(#loc)
    %209 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %210 = llvm.insertelement %192, %209[%7 : i32] : vector<2xi32> loc(#loc)
    %211 = llvm.insertelement %174, %210[%1 : i32] : vector<2xi32> loc(#loc)
    %212 = llvm.mul %4, %9  : i32 loc(#loc)
    %213 = llvm.sub %212, %1  : i32 loc(#loc)
    %214 = llvm.sub %4, %1  : i32 loc(#loc)
    %215 = llvm.mul %4, %9  : i32 loc(#loc)
    %216 = llvm.sub %215, %1  : i32 loc(#loc)
    %217 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %218 = llvm.insertelement %192, %217[%7 : i32] : vector<2xi32> loc(#loc)
    %219 = llvm.insertelement %183, %218[%1 : i32] : vector<2xi32> loc(#loc)
    %220 = llvm.mul %4, %9  : i32 loc(#loc)
    %221 = llvm.sub %220, %1  : i32 loc(#loc)
    %222 = llvm.sub %4, %1  : i32 loc(#loc)
    %223 = llvm.mul %4, %9  : i32 loc(#loc)
    %224 = llvm.sub %223, %1  : i32 loc(#loc)
    %225 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %226 = llvm.insertelement %60, %225[%7 : i32] : vector<2xi32> loc(#loc)
    %227 = llvm.insertelement %39, %226[%1 : i32] : vector<2xi32> loc(#loc)
    %228 = llvm.mul %4, %9  : i32 loc(#loc)
    %229 = llvm.sub %228, %1  : i32 loc(#loc)
    %230 = llvm.sub %4, %1  : i32 loc(#loc)
    %231 = llvm.mul %4, %9  : i32 loc(#loc)
    %232 = llvm.sub %231, %1  : i32 loc(#loc)
    %233 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %234 = llvm.insertelement %60, %233[%7 : i32] : vector<2xi32> loc(#loc)
    %235 = llvm.insertelement %165, %234[%1 : i32] : vector<2xi32> loc(#loc)
    %236 = llvm.mul %4, %9  : i32 loc(#loc)
    %237 = llvm.sub %236, %1  : i32 loc(#loc)
    %238 = llvm.sub %4, %1  : i32 loc(#loc)
    %239 = llvm.mul %4, %9  : i32 loc(#loc)
    %240 = llvm.sub %239, %1  : i32 loc(#loc)
    %241 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %242 = llvm.insertelement %60, %241[%7 : i32] : vector<2xi32> loc(#loc)
    %243 = llvm.insertelement %174, %242[%1 : i32] : vector<2xi32> loc(#loc)
    %244 = llvm.mul %4, %9  : i32 loc(#loc)
    %245 = llvm.sub %244, %1  : i32 loc(#loc)
    %246 = llvm.sub %4, %1  : i32 loc(#loc)
    %247 = llvm.mul %4, %9  : i32 loc(#loc)
    %248 = llvm.sub %247, %1  : i32 loc(#loc)
    %249 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %250 = llvm.insertelement %60, %249[%7 : i32] : vector<2xi32> loc(#loc)
    %251 = llvm.insertelement %183, %250[%1 : i32] : vector<2xi32> loc(#loc)
    %252 = llvm.mul %4, %9  : i32 loc(#loc)
    %253 = llvm.sub %252, %1  : i32 loc(#loc)
    %254 = llvm.sub %4, %1  : i32 loc(#loc)
    %255 = llvm.mul %4, %9  : i32 loc(#loc)
    %256 = llvm.sub %255, %1  : i32 loc(#loc)
    %257 = llvm.add %51, %12  : i32 loc(#loc)
    %258 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %259 = llvm.insertelement %257, %258[%7 : i32] : vector<2xi32> loc(#loc)
    %260 = llvm.insertelement %39, %259[%1 : i32] : vector<2xi32> loc(#loc)
    %261 = llvm.mul %4, %9  : i32 loc(#loc)
    %262 = llvm.sub %261, %1  : i32 loc(#loc)
    %263 = llvm.sub %4, %1  : i32 loc(#loc)
    %264 = llvm.mul %4, %9  : i32 loc(#loc)
    %265 = llvm.sub %264, %1  : i32 loc(#loc)
    %266 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %267 = llvm.insertelement %257, %266[%7 : i32] : vector<2xi32> loc(#loc)
    %268 = llvm.insertelement %165, %267[%1 : i32] : vector<2xi32> loc(#loc)
    %269 = llvm.mul %4, %9  : i32 loc(#loc)
    %270 = llvm.sub %269, %1  : i32 loc(#loc)
    %271 = llvm.sub %4, %1  : i32 loc(#loc)
    %272 = llvm.mul %4, %9  : i32 loc(#loc)
    %273 = llvm.sub %272, %1  : i32 loc(#loc)
    %274 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %275 = llvm.insertelement %257, %274[%7 : i32] : vector<2xi32> loc(#loc)
    %276 = llvm.insertelement %174, %275[%1 : i32] : vector<2xi32> loc(#loc)
    %277 = llvm.mul %4, %9  : i32 loc(#loc)
    %278 = llvm.sub %277, %1  : i32 loc(#loc)
    %279 = llvm.sub %4, %1  : i32 loc(#loc)
    %280 = llvm.mul %4, %9  : i32 loc(#loc)
    %281 = llvm.sub %280, %1  : i32 loc(#loc)
    %282 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %283 = llvm.insertelement %257, %282[%7 : i32] : vector<2xi32> loc(#loc)
    %284 = llvm.insertelement %183, %283[%1 : i32] : vector<2xi32> loc(#loc)
    %285 = llvm.mul %4, %9  : i32 loc(#loc)
    %286 = llvm.sub %285, %1  : i32 loc(#loc)
    %287 = llvm.sub %4, %1  : i32 loc(#loc)
    %288 = llvm.mul %4, %9  : i32 loc(#loc)
    %289 = llvm.sub %288, %1  : i32 loc(#loc)
    %290 = llvm.extractelement %159[%7 : i32] : vector<2xi32> loc(#loc)
    %291 = llvm.extractelement %159[%1 : i32] : vector<2xi32> loc(#loc)
    %292 = llvm.bitcast %70 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %161, %162, %164, %290, %291, %292 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %293 = llvm.extractelement %168[%7 : i32] : vector<2xi32> loc(#loc)
    %294 = llvm.extractelement %168[%1 : i32] : vector<2xi32> loc(#loc)
    %295 = llvm.bitcast %71 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %170, %171, %173, %293, %294, %295 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %296 = llvm.extractelement %177[%7 : i32] : vector<2xi32> loc(#loc)
    %297 = llvm.extractelement %177[%1 : i32] : vector<2xi32> loc(#loc)
    %298 = llvm.bitcast %72 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %179, %180, %182, %296, %297, %298 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %299 = llvm.extractelement %186[%7 : i32] : vector<2xi32> loc(#loc)
    %300 = llvm.extractelement %186[%1 : i32] : vector<2xi32> loc(#loc)
    %301 = llvm.bitcast %73 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %188, %189, %191, %299, %300, %301 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %302 = llvm.extractelement %195[%7 : i32] : vector<2xi32> loc(#loc)
    %303 = llvm.extractelement %195[%1 : i32] : vector<2xi32> loc(#loc)
    %304 = llvm.bitcast %74 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %197, %198, %200, %302, %303, %304 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %305 = llvm.extractelement %203[%7 : i32] : vector<2xi32> loc(#loc)
    %306 = llvm.extractelement %203[%1 : i32] : vector<2xi32> loc(#loc)
    %307 = llvm.bitcast %75 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %205, %206, %208, %305, %306, %307 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %308 = llvm.extractelement %211[%7 : i32] : vector<2xi32> loc(#loc)
    %309 = llvm.extractelement %211[%1 : i32] : vector<2xi32> loc(#loc)
    %310 = llvm.bitcast %76 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %213, %214, %216, %308, %309, %310 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %311 = llvm.extractelement %219[%7 : i32] : vector<2xi32> loc(#loc)
    %312 = llvm.extractelement %219[%1 : i32] : vector<2xi32> loc(#loc)
    %313 = llvm.bitcast %77 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %221, %222, %224, %311, %312, %313 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %314 = llvm.extractelement %227[%7 : i32] : vector<2xi32> loc(#loc)
    %315 = llvm.extractelement %227[%1 : i32] : vector<2xi32> loc(#loc)
    %316 = llvm.bitcast %78 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %229, %230, %232, %314, %315, %316 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %317 = llvm.extractelement %235[%7 : i32] : vector<2xi32> loc(#loc)
    %318 = llvm.extractelement %235[%1 : i32] : vector<2xi32> loc(#loc)
    %319 = llvm.bitcast %79 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %237, %238, %240, %317, %318, %319 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %320 = llvm.extractelement %243[%7 : i32] : vector<2xi32> loc(#loc)
    %321 = llvm.extractelement %243[%1 : i32] : vector<2xi32> loc(#loc)
    %322 = llvm.bitcast %80 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %245, %246, %248, %320, %321, %322 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %323 = llvm.extractelement %251[%7 : i32] : vector<2xi32> loc(#loc)
    %324 = llvm.extractelement %251[%1 : i32] : vector<2xi32> loc(#loc)
    %325 = llvm.bitcast %81 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %253, %254, %256, %323, %324, %325 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %326 = llvm.extractelement %260[%7 : i32] : vector<2xi32> loc(#loc)
    %327 = llvm.extractelement %260[%1 : i32] : vector<2xi32> loc(#loc)
    %328 = llvm.bitcast %82 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %262, %263, %265, %326, %327, %328 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %329 = llvm.extractelement %268[%7 : i32] : vector<2xi32> loc(#loc)
    %330 = llvm.extractelement %268[%1 : i32] : vector<2xi32> loc(#loc)
    %331 = llvm.bitcast %83 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %270, %271, %273, %329, %330, %331 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %332 = llvm.extractelement %276[%7 : i32] : vector<2xi32> loc(#loc)
    %333 = llvm.extractelement %276[%1 : i32] : vector<2xi32> loc(#loc)
    %334 = llvm.bitcast %84 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %278, %279, %281, %332, %333, %334 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %335 = llvm.extractelement %284[%7 : i32] : vector<2xi32> loc(#loc)
    %336 = llvm.extractelement %284[%1 : i32] : vector<2xi32> loc(#loc)
    %337 = llvm.bitcast %85 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %286, %287, %289, %335, %336, %337 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    llvm.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 16 : i32} {
  llvm.func @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {genx.intel_reqd_sub_group_size = [16 : i32], genx.kernel = 1 : i32, genx.max_work_group_size = [512 : i32, 1 : i32, 1 : i32], noinline = false} {
    %0 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %1 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %2 = llvm.mlir.constant(64 : i32) : i32 loc(#loc)
    %3 = llvm.mlir.constant(16 : i32) : i32 loc(#loc)
    %4 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %5 = llvm.mlir.constant(dense<0.000000e+00> : vector<8xf32>) : vector<8xf32> loc(#loc)
    %6 = llvm.mlir.constant(32 : i32) : i32 loc(#loc)
    %7 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %8 = llvm.mlir.constant(256 : i32) : i32 loc(#loc)
    %9 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %10 = llvm.mlir.constant(8 : i32) : i32 loc(#loc)
    %11 = llvm.mlir.constant(24 : i32) : i32 loc(#loc)
    %12 = llvm.mlir.constant(48 : i32) : i32 loc(#loc)
    %13 = llvm.mlir.constant(63 : i32) : i32 loc(#loc)
    %14 = llvm.mlir.constant(7 : i32) : i32 loc(#loc)
    %15 = llvm.mlir.constant(3 : i32) : i32 loc(#loc)
    %16 = genx.workitem.id.x : i32 loc(#loc)
    %17 = genx.workitem.id.y : i32 loc(#loc)
    %18 = genx.workitem.id.z : i32 loc(#loc)
    %19 = genx.workgroup.dim.x : i32 loc(#loc)
    %20 = genx.workgroup.dim.y : i32 loc(#loc)
    %21 = llvm.mul %18, %20  : i32 loc(#loc)
    %22 = llvm.add %21, %17  : i32 loc(#loc)
    %23 = llvm.mul %22, %19  : i32 loc(#loc)
    %24 = llvm.add %23, %16  : i32 loc(#loc)
    %25 = llvm.udiv %24, %3  : i32 loc(#loc)
    %26 = genx.workgroup.id.x : i32 loc(#loc)
    %27 = llvm.sdiv %26, %2  : i32 loc(#loc)
    %28 = llvm.mul %27, %9  : i32 loc(#loc)
    %29 = llvm.sub %3, %28  : i32 loc(#loc)
    %30 = llvm.intr.smin(%29, %9)  : (i32, i32) -> i32 loc(#loc)
    %31 = llvm.srem %26, %30  : i32 loc(#loc)
    %32 = llvm.add %28, %31  : i32 loc(#loc)
    %33 = llvm.and %26, %13  : i32 loc(#loc)
    %34 = llvm.sdiv %33, %30  : i32 loc(#loc)
    %35 = llvm.mul %32, %8  : i32 loc(#loc)
    %36 = llvm.sdiv %25, %9  : i32 loc(#loc)
    %37 = llvm.and %36, %14  : i32 loc(#loc)
    %38 = llvm.mul %37, %6  : i32 loc(#loc)
    %39 = llvm.add %38, %35  : i32 loc(#loc)
    %40 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %41 = llvm.insertelement %7, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %42 = llvm.insertelement %39, %41[%1 : i32] : vector<2xi32> loc(#loc)
    %43 = llvm.mul %4, %0  : i32 loc(#loc)
    %44 = llvm.sub %43, %1  : i32 loc(#loc)
    %45 = llvm.sub %4, %1  : i32 loc(#loc)
    %46 = llvm.mul %4, %0  : i32 loc(#loc)
    %47 = llvm.sub %46, %1  : i32 loc(#loc)
    %48 = llvm.mul %34, %8  : i32 loc(#loc)
    %49 = llvm.and %25, %15  : i32 loc(#loc)
    %50 = llvm.mul %49, %2  : i32 loc(#loc)
    %51 = llvm.add %50, %48  : i32 loc(#loc)
    %52 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %53 = llvm.insertelement %51, %52[%7 : i32] : vector<2xi32> loc(#loc)
    %54 = llvm.insertelement %7, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %55 = llvm.mul %4, %0  : i32 loc(#loc)
    %56 = llvm.sub %55, %1  : i32 loc(#loc)
    %57 = llvm.sub %4, %1  : i32 loc(#loc)
    %58 = llvm.mul %4, %0  : i32 loc(#loc)
    %59 = llvm.sub %58, %1  : i32 loc(#loc)
    %60 = llvm.add %51, %6  : i32 loc(#loc)
    %61 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %62 = llvm.insertelement %60, %61[%7 : i32] : vector<2xi32> loc(#loc)
    %63 = llvm.insertelement %7, %62[%1 : i32] : vector<2xi32> loc(#loc)
    %64 = llvm.mul %4, %0  : i32 loc(#loc)
    %65 = llvm.sub %64, %1  : i32 loc(#loc)
    %66 = llvm.sub %4, %1  : i32 loc(#loc)
    %67 = llvm.mul %4, %0  : i32 loc(#loc)
    %68 = llvm.sub %67, %1  : i32 loc(#loc)
    llvm.br ^bb1(%7, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %42, %54, %63 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb1(%69: i32 loc(unknown), %70: vector<8xf32> loc(unknown), %71: vector<8xf32> loc(unknown), %72: vector<8xf32> loc(unknown), %73: vector<8xf32> loc(unknown), %74: vector<8xf32> loc(unknown), %75: vector<8xf32> loc(unknown), %76: vector<8xf32> loc(unknown), %77: vector<8xf32> loc(unknown), %78: vector<8xf32> loc(unknown), %79: vector<8xf32> loc(unknown), %80: vector<8xf32> loc(unknown), %81: vector<8xf32> loc(unknown), %82: vector<8xf32> loc(unknown), %83: vector<8xf32> loc(unknown), %84: vector<8xf32> loc(unknown), %85: vector<8xf32> loc(unknown), %86: vector<2xi32> loc(unknown), %87: vector<2xi32> loc(unknown), %88: vector<2xi32> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %89 = llvm.icmp "slt" %69, %4 : i32 loc(#loc)
    llvm.cond_br %89, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %90 = llvm.extractelement %86[%7 : i32] : vector<2xi32> loc(#loc)
    %91 = llvm.extractelement %86[%1 : i32] : vector<2xi32> loc(#loc)
    %92 = genx.matrix.2Dblockload %arg0, %44, %45, %47, %90, %91 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<64xi16> loc(#loc)
    %93 = llvm.extractelement %87[%7 : i32] : vector<2xi32> loc(#loc)
    %94 = llvm.extractelement %87[%1 : i32] : vector<2xi32> loc(#loc)
    %95 = genx.matrix.2Dblockload %arg1, %56, %57, %59, %93, %94 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %96 = llvm.extractelement %88[%7 : i32] : vector<2xi32> loc(#loc)
    %97 = llvm.extractelement %88[%1 : i32] : vector<2xi32> loc(#loc)
    %98 = genx.matrix.2Dblockload %arg1, %65, %66, %68, %96, %97 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %99 = llvm.shufflevector %92, %92 [0, 1, 2, 3, 4, 5, 6, 7] : vector<64xi16>  loc(#loc)
    %100 = llvm.shufflevector %95, %95 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %101 = genx.matrix.dpas %70, %99, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %102 = llvm.shufflevector %92, %92 [32, 33, 34, 35, 36, 37, 38, 39] : vector<64xi16>  loc(#loc)
    %103 = llvm.shufflevector %95, %95 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %104 = genx.matrix.dpas %101, %102, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %105 = llvm.shufflevector %92, %92 [8, 9, 10, 11, 12, 13, 14, 15] : vector<64xi16>  loc(#loc)
    %106 = genx.matrix.dpas %71, %105, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %107 = llvm.shufflevector %92, %92 [40, 41, 42, 43, 44, 45, 46, 47] : vector<64xi16>  loc(#loc)
    %108 = genx.matrix.dpas %106, %107, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %109 = llvm.shufflevector %92, %92 [16, 17, 18, 19, 20, 21, 22, 23] : vector<64xi16>  loc(#loc)
    %110 = genx.matrix.dpas %72, %109, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %111 = llvm.shufflevector %92, %92 [48, 49, 50, 51, 52, 53, 54, 55] : vector<64xi16>  loc(#loc)
    %112 = genx.matrix.dpas %110, %111, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %113 = llvm.shufflevector %92, %92 [24, 25, 26, 27, 28, 29, 30, 31] : vector<64xi16>  loc(#loc)
    %114 = genx.matrix.dpas %73, %113, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %115 = llvm.shufflevector %92, %92 [56, 57, 58, 59, 60, 61, 62, 63] : vector<64xi16>  loc(#loc)
    %116 = genx.matrix.dpas %114, %115, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %117 = llvm.shufflevector %95, %95 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %118 = genx.matrix.dpas %74, %99, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %119 = llvm.shufflevector %95, %95 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %120 = genx.matrix.dpas %118, %102, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %121 = genx.matrix.dpas %75, %105, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %122 = genx.matrix.dpas %121, %107, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %123 = genx.matrix.dpas %76, %109, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %124 = genx.matrix.dpas %123, %111, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %125 = genx.matrix.dpas %77, %113, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %126 = genx.matrix.dpas %125, %115, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %127 = llvm.shufflevector %98, %98 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %128 = genx.matrix.dpas %78, %99, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %129 = llvm.shufflevector %98, %98 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %130 = genx.matrix.dpas %128, %102, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %131 = genx.matrix.dpas %79, %105, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %132 = genx.matrix.dpas %131, %107, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %133 = genx.matrix.dpas %80, %109, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %134 = genx.matrix.dpas %133, %111, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %135 = genx.matrix.dpas %81, %113, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %136 = genx.matrix.dpas %135, %115, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %137 = llvm.shufflevector %98, %98 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %138 = genx.matrix.dpas %82, %99, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %139 = llvm.shufflevector %98, %98 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %140 = genx.matrix.dpas %138, %102, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %141 = genx.matrix.dpas %83, %105, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %142 = genx.matrix.dpas %141, %107, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %143 = genx.matrix.dpas %84, %109, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %144 = genx.matrix.dpas %143, %111, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %145 = genx.matrix.dpas %85, %113, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %146 = genx.matrix.dpas %145, %115, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %147 = llvm.extractelement %86[%7 : i32] : vector<2xi32> loc(#loc)
    %148 = llvm.add %147, %6  : i32 loc(#loc)
    %149 = llvm.insertelement %148, %86[%7 : i32] : vector<2xi32> loc(#loc)
    %150 = llvm.extractelement %87[%1 : i32] : vector<2xi32> loc(#loc)
    %151 = llvm.add %150, %6  : i32 loc(#loc)
    %152 = llvm.insertelement %151, %87[%1 : i32] : vector<2xi32> loc(#loc)
    %153 = llvm.extractelement %88[%1 : i32] : vector<2xi32> loc(#loc)
    %154 = llvm.add %153, %6  : i32 loc(#loc)
    %155 = llvm.insertelement %154, %88[%1 : i32] : vector<2xi32> loc(#loc)
    %156 = llvm.add %69, %6  : i32 loc(#loc)
    llvm.br ^bb1(%156, %104, %108, %112, %116, %120, %122, %124, %126, %130, %132, %134, %136, %140, %142, %144, %146, %149, %152, %155 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %157 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %158 = llvm.insertelement %51, %157[%7 : i32] : vector<2xi32> loc(#loc)
    %159 = llvm.insertelement %39, %158[%1 : i32] : vector<2xi32> loc(#loc)
    %160 = llvm.mul %4, %9  : i32 loc(#loc)
    %161 = llvm.sub %160, %1  : i32 loc(#loc)
    %162 = llvm.sub %4, %1  : i32 loc(#loc)
    %163 = llvm.mul %4, %9  : i32 loc(#loc)
    %164 = llvm.sub %163, %1  : i32 loc(#loc)
    %165 = llvm.add %39, %10  : i32 loc(#loc)
    %166 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %167 = llvm.insertelement %51, %166[%7 : i32] : vector<2xi32> loc(#loc)
    %168 = llvm.insertelement %165, %167[%1 : i32] : vector<2xi32> loc(#loc)
    %169 = llvm.mul %4, %9  : i32 loc(#loc)
    %170 = llvm.sub %169, %1  : i32 loc(#loc)
    %171 = llvm.sub %4, %1  : i32 loc(#loc)
    %172 = llvm.mul %4, %9  : i32 loc(#loc)
    %173 = llvm.sub %172, %1  : i32 loc(#loc)
    %174 = llvm.add %39, %3  : i32 loc(#loc)
    %175 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %176 = llvm.insertelement %51, %175[%7 : i32] : vector<2xi32> loc(#loc)
    %177 = llvm.insertelement %174, %176[%1 : i32] : vector<2xi32> loc(#loc)
    %178 = llvm.mul %4, %9  : i32 loc(#loc)
    %179 = llvm.sub %178, %1  : i32 loc(#loc)
    %180 = llvm.sub %4, %1  : i32 loc(#loc)
    %181 = llvm.mul %4, %9  : i32 loc(#loc)
    %182 = llvm.sub %181, %1  : i32 loc(#loc)
    %183 = llvm.add %39, %11  : i32 loc(#loc)
    %184 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %185 = llvm.insertelement %51, %184[%7 : i32] : vector<2xi32> loc(#loc)
    %186 = llvm.insertelement %183, %185[%1 : i32] : vector<2xi32> loc(#loc)
    %187 = llvm.mul %4, %9  : i32 loc(#loc)
    %188 = llvm.sub %187, %1  : i32 loc(#loc)
    %189 = llvm.sub %4, %1  : i32 loc(#loc)
    %190 = llvm.mul %4, %9  : i32 loc(#loc)
    %191 = llvm.sub %190, %1  : i32 loc(#loc)
    %192 = llvm.add %51, %3  : i32 loc(#loc)
    %193 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %194 = llvm.insertelement %192, %193[%7 : i32] : vector<2xi32> loc(#loc)
    %195 = llvm.insertelement %39, %194[%1 : i32] : vector<2xi32> loc(#loc)
    %196 = llvm.mul %4, %9  : i32 loc(#loc)
    %197 = llvm.sub %196, %1  : i32 loc(#loc)
    %198 = llvm.sub %4, %1  : i32 loc(#loc)
    %199 = llvm.mul %4, %9  : i32 loc(#loc)
    %200 = llvm.sub %199, %1  : i32 loc(#loc)
    %201 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %202 = llvm.insertelement %192, %201[%7 : i32] : vector<2xi32> loc(#loc)
    %203 = llvm.insertelement %165, %202[%1 : i32] : vector<2xi32> loc(#loc)
    %204 = llvm.mul %4, %9  : i32 loc(#loc)
    %205 = llvm.sub %204, %1  : i32 loc(#loc)
    %206 = llvm.sub %4, %1  : i32 loc(#loc)
    %207 = llvm.mul %4, %9  : i32 loc(#loc)
    %208 = llvm.sub %207, %1  : i32 loc(#loc)
    %209 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %210 = llvm.insertelement %192, %209[%7 : i32] : vector<2xi32> loc(#loc)
    %211 = llvm.insertelement %174, %210[%1 : i32] : vector<2xi32> loc(#loc)
    %212 = llvm.mul %4, %9  : i32 loc(#loc)
    %213 = llvm.sub %212, %1  : i32 loc(#loc)
    %214 = llvm.sub %4, %1  : i32 loc(#loc)
    %215 = llvm.mul %4, %9  : i32 loc(#loc)
    %216 = llvm.sub %215, %1  : i32 loc(#loc)
    %217 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %218 = llvm.insertelement %192, %217[%7 : i32] : vector<2xi32> loc(#loc)
    %219 = llvm.insertelement %183, %218[%1 : i32] : vector<2xi32> loc(#loc)
    %220 = llvm.mul %4, %9  : i32 loc(#loc)
    %221 = llvm.sub %220, %1  : i32 loc(#loc)
    %222 = llvm.sub %4, %1  : i32 loc(#loc)
    %223 = llvm.mul %4, %9  : i32 loc(#loc)
    %224 = llvm.sub %223, %1  : i32 loc(#loc)
    %225 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %226 = llvm.insertelement %60, %225[%7 : i32] : vector<2xi32> loc(#loc)
    %227 = llvm.insertelement %39, %226[%1 : i32] : vector<2xi32> loc(#loc)
    %228 = llvm.mul %4, %9  : i32 loc(#loc)
    %229 = llvm.sub %228, %1  : i32 loc(#loc)
    %230 = llvm.sub %4, %1  : i32 loc(#loc)
    %231 = llvm.mul %4, %9  : i32 loc(#loc)
    %232 = llvm.sub %231, %1  : i32 loc(#loc)
    %233 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %234 = llvm.insertelement %60, %233[%7 : i32] : vector<2xi32> loc(#loc)
    %235 = llvm.insertelement %165, %234[%1 : i32] : vector<2xi32> loc(#loc)
    %236 = llvm.mul %4, %9  : i32 loc(#loc)
    %237 = llvm.sub %236, %1  : i32 loc(#loc)
    %238 = llvm.sub %4, %1  : i32 loc(#loc)
    %239 = llvm.mul %4, %9  : i32 loc(#loc)
    %240 = llvm.sub %239, %1  : i32 loc(#loc)
    %241 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %242 = llvm.insertelement %60, %241[%7 : i32] : vector<2xi32> loc(#loc)
    %243 = llvm.insertelement %174, %242[%1 : i32] : vector<2xi32> loc(#loc)
    %244 = llvm.mul %4, %9  : i32 loc(#loc)
    %245 = llvm.sub %244, %1  : i32 loc(#loc)
    %246 = llvm.sub %4, %1  : i32 loc(#loc)
    %247 = llvm.mul %4, %9  : i32 loc(#loc)
    %248 = llvm.sub %247, %1  : i32 loc(#loc)
    %249 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %250 = llvm.insertelement %60, %249[%7 : i32] : vector<2xi32> loc(#loc)
    %251 = llvm.insertelement %183, %250[%1 : i32] : vector<2xi32> loc(#loc)
    %252 = llvm.mul %4, %9  : i32 loc(#loc)
    %253 = llvm.sub %252, %1  : i32 loc(#loc)
    %254 = llvm.sub %4, %1  : i32 loc(#loc)
    %255 = llvm.mul %4, %9  : i32 loc(#loc)
    %256 = llvm.sub %255, %1  : i32 loc(#loc)
    %257 = llvm.add %51, %12  : i32 loc(#loc)
    %258 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %259 = llvm.insertelement %257, %258[%7 : i32] : vector<2xi32> loc(#loc)
    %260 = llvm.insertelement %39, %259[%1 : i32] : vector<2xi32> loc(#loc)
    %261 = llvm.mul %4, %9  : i32 loc(#loc)
    %262 = llvm.sub %261, %1  : i32 loc(#loc)
    %263 = llvm.sub %4, %1  : i32 loc(#loc)
    %264 = llvm.mul %4, %9  : i32 loc(#loc)
    %265 = llvm.sub %264, %1  : i32 loc(#loc)
    %266 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %267 = llvm.insertelement %257, %266[%7 : i32] : vector<2xi32> loc(#loc)
    %268 = llvm.insertelement %165, %267[%1 : i32] : vector<2xi32> loc(#loc)
    %269 = llvm.mul %4, %9  : i32 loc(#loc)
    %270 = llvm.sub %269, %1  : i32 loc(#loc)
    %271 = llvm.sub %4, %1  : i32 loc(#loc)
    %272 = llvm.mul %4, %9  : i32 loc(#loc)
    %273 = llvm.sub %272, %1  : i32 loc(#loc)
    %274 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %275 = llvm.insertelement %257, %274[%7 : i32] : vector<2xi32> loc(#loc)
    %276 = llvm.insertelement %174, %275[%1 : i32] : vector<2xi32> loc(#loc)
    %277 = llvm.mul %4, %9  : i32 loc(#loc)
    %278 = llvm.sub %277, %1  : i32 loc(#loc)
    %279 = llvm.sub %4, %1  : i32 loc(#loc)
    %280 = llvm.mul %4, %9  : i32 loc(#loc)
    %281 = llvm.sub %280, %1  : i32 loc(#loc)
    %282 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %283 = llvm.insertelement %257, %282[%7 : i32] : vector<2xi32> loc(#loc)
    %284 = llvm.insertelement %183, %283[%1 : i32] : vector<2xi32> loc(#loc)
    %285 = llvm.mul %4, %9  : i32 loc(#loc)
    %286 = llvm.sub %285, %1  : i32 loc(#loc)
    %287 = llvm.sub %4, %1  : i32 loc(#loc)
    %288 = llvm.mul %4, %9  : i32 loc(#loc)
    %289 = llvm.sub %288, %1  : i32 loc(#loc)
    %290 = llvm.extractelement %159[%7 : i32] : vector<2xi32> loc(#loc)
    %291 = llvm.extractelement %159[%1 : i32] : vector<2xi32> loc(#loc)
    %292 = llvm.bitcast %70 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %161, %162, %164, %290, %291, %292 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %293 = llvm.extractelement %168[%7 : i32] : vector<2xi32> loc(#loc)
    %294 = llvm.extractelement %168[%1 : i32] : vector<2xi32> loc(#loc)
    %295 = llvm.bitcast %71 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %170, %171, %173, %293, %294, %295 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %296 = llvm.extractelement %177[%7 : i32] : vector<2xi32> loc(#loc)
    %297 = llvm.extractelement %177[%1 : i32] : vector<2xi32> loc(#loc)
    %298 = llvm.bitcast %72 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %179, %180, %182, %296, %297, %298 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %299 = llvm.extractelement %186[%7 : i32] : vector<2xi32> loc(#loc)
    %300 = llvm.extractelement %186[%1 : i32] : vector<2xi32> loc(#loc)
    %301 = llvm.bitcast %73 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %188, %189, %191, %299, %300, %301 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %302 = llvm.extractelement %195[%7 : i32] : vector<2xi32> loc(#loc)
    %303 = llvm.extractelement %195[%1 : i32] : vector<2xi32> loc(#loc)
    %304 = llvm.bitcast %74 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %197, %198, %200, %302, %303, %304 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %305 = llvm.extractelement %203[%7 : i32] : vector<2xi32> loc(#loc)
    %306 = llvm.extractelement %203[%1 : i32] : vector<2xi32> loc(#loc)
    %307 = llvm.bitcast %75 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %205, %206, %208, %305, %306, %307 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %308 = llvm.extractelement %211[%7 : i32] : vector<2xi32> loc(#loc)
    %309 = llvm.extractelement %211[%1 : i32] : vector<2xi32> loc(#loc)
    %310 = llvm.bitcast %76 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %213, %214, %216, %308, %309, %310 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %311 = llvm.extractelement %219[%7 : i32] : vector<2xi32> loc(#loc)
    %312 = llvm.extractelement %219[%1 : i32] : vector<2xi32> loc(#loc)
    %313 = llvm.bitcast %77 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %221, %222, %224, %311, %312, %313 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %314 = llvm.extractelement %227[%7 : i32] : vector<2xi32> loc(#loc)
    %315 = llvm.extractelement %227[%1 : i32] : vector<2xi32> loc(#loc)
    %316 = llvm.bitcast %78 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %229, %230, %232, %314, %315, %316 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %317 = llvm.extractelement %235[%7 : i32] : vector<2xi32> loc(#loc)
    %318 = llvm.extractelement %235[%1 : i32] : vector<2xi32> loc(#loc)
    %319 = llvm.bitcast %79 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %237, %238, %240, %317, %318, %319 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %320 = llvm.extractelement %243[%7 : i32] : vector<2xi32> loc(#loc)
    %321 = llvm.extractelement %243[%1 : i32] : vector<2xi32> loc(#loc)
    %322 = llvm.bitcast %80 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %245, %246, %248, %320, %321, %322 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %323 = llvm.extractelement %251[%7 : i32] : vector<2xi32> loc(#loc)
    %324 = llvm.extractelement %251[%1 : i32] : vector<2xi32> loc(#loc)
    %325 = llvm.bitcast %81 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %253, %254, %256, %323, %324, %325 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %326 = llvm.extractelement %260[%7 : i32] : vector<2xi32> loc(#loc)
    %327 = llvm.extractelement %260[%1 : i32] : vector<2xi32> loc(#loc)
    %328 = llvm.bitcast %82 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %262, %263, %265, %326, %327, %328 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %329 = llvm.extractelement %268[%7 : i32] : vector<2xi32> loc(#loc)
    %330 = llvm.extractelement %268[%1 : i32] : vector<2xi32> loc(#loc)
    %331 = llvm.bitcast %83 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %270, %271, %273, %329, %330, %331 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %332 = llvm.extractelement %276[%7 : i32] : vector<2xi32> loc(#loc)
    %333 = llvm.extractelement %276[%1 : i32] : vector<2xi32> loc(#loc)
    %334 = llvm.bitcast %84 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %278, %279, %281, %332, %333, %334 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %335 = llvm.extractelement %284[%7 : i32] : vector<2xi32> loc(#loc)
    %336 = llvm.extractelement %284[%1 : i32] : vector<2xi32> loc(#loc)
    %337 = llvm.bitcast %85 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %286, %287, %289, %335, %336, %337 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    llvm.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 16 : i32} {
  llvm.func @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {genx.intel_reqd_sub_group_size = [16 : i32], genx.kernel = 1 : i32, genx.max_work_group_size = [512 : i32, 1 : i32, 1 : i32], noinline = false} {
    %0 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %1 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %2 = llvm.mlir.constant(64 : i32) : i32 loc(#loc)
    %3 = llvm.mlir.constant(16 : i32) : i32 loc(#loc)
    %4 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %5 = llvm.mlir.constant(dense<0.000000e+00> : vector<8xf32>) : vector<8xf32> loc(#loc)
    %6 = llvm.mlir.constant(32 : i32) : i32 loc(#loc)
    %7 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %8 = llvm.mlir.constant(256 : i32) : i32 loc(#loc)
    %9 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %10 = llvm.mlir.constant(8 : i32) : i32 loc(#loc)
    %11 = llvm.mlir.constant(24 : i32) : i32 loc(#loc)
    %12 = llvm.mlir.constant(48 : i32) : i32 loc(#loc)
    %13 = llvm.mlir.constant(63 : i32) : i32 loc(#loc)
    %14 = llvm.mlir.constant(7 : i32) : i32 loc(#loc)
    %15 = llvm.mlir.constant(3 : i32) : i32 loc(#loc)
    %16 = genx.workitem.id.x : i32 loc(#loc)
    %17 = genx.workitem.id.y : i32 loc(#loc)
    %18 = genx.workitem.id.z : i32 loc(#loc)
    %19 = genx.workgroup.dim.x : i32 loc(#loc)
    %20 = genx.workgroup.dim.y : i32 loc(#loc)
    %21 = llvm.mul %18, %20  : i32 loc(#loc)
    %22 = llvm.add %21, %17  : i32 loc(#loc)
    %23 = llvm.mul %22, %19  : i32 loc(#loc)
    %24 = llvm.add %23, %16  : i32 loc(#loc)
    %25 = llvm.udiv %24, %3  : i32 loc(#loc)
    %26 = genx.workgroup.id.x : i32 loc(#loc)
    %27 = llvm.sdiv %26, %2  : i32 loc(#loc)
    %28 = llvm.mul %27, %9  : i32 loc(#loc)
    %29 = llvm.sub %3, %28  : i32 loc(#loc)
    %30 = llvm.intr.smin(%29, %9)  : (i32, i32) -> i32 loc(#loc)
    %31 = llvm.srem %26, %30  : i32 loc(#loc)
    %32 = llvm.add %28, %31  : i32 loc(#loc)
    %33 = llvm.and %26, %13  : i32 loc(#loc)
    %34 = llvm.sdiv %33, %30  : i32 loc(#loc)
    %35 = llvm.mul %32, %8  : i32 loc(#loc)
    %36 = llvm.sdiv %25, %9  : i32 loc(#loc)
    %37 = llvm.and %36, %14  : i32 loc(#loc)
    %38 = llvm.mul %37, %6  : i32 loc(#loc)
    %39 = llvm.add %38, %35  : i32 loc(#loc)
    %40 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %41 = llvm.insertelement %7, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %42 = llvm.insertelement %39, %41[%1 : i32] : vector<2xi32> loc(#loc)
    %43 = llvm.mul %4, %0  : i32 loc(#loc)
    %44 = llvm.sub %43, %1  : i32 loc(#loc)
    %45 = llvm.sub %4, %1  : i32 loc(#loc)
    %46 = llvm.mul %4, %0  : i32 loc(#loc)
    %47 = llvm.sub %46, %1  : i32 loc(#loc)
    %48 = llvm.mul %34, %8  : i32 loc(#loc)
    %49 = llvm.and %25, %15  : i32 loc(#loc)
    %50 = llvm.mul %49, %2  : i32 loc(#loc)
    %51 = llvm.add %50, %48  : i32 loc(#loc)
    %52 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %53 = llvm.insertelement %51, %52[%7 : i32] : vector<2xi32> loc(#loc)
    %54 = llvm.insertelement %7, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %55 = llvm.mul %4, %0  : i32 loc(#loc)
    %56 = llvm.sub %55, %1  : i32 loc(#loc)
    %57 = llvm.sub %4, %1  : i32 loc(#loc)
    %58 = llvm.mul %4, %0  : i32 loc(#loc)
    %59 = llvm.sub %58, %1  : i32 loc(#loc)
    %60 = llvm.add %51, %6  : i32 loc(#loc)
    %61 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %62 = llvm.insertelement %60, %61[%7 : i32] : vector<2xi32> loc(#loc)
    %63 = llvm.insertelement %7, %62[%1 : i32] : vector<2xi32> loc(#loc)
    %64 = llvm.mul %4, %0  : i32 loc(#loc)
    %65 = llvm.sub %64, %1  : i32 loc(#loc)
    %66 = llvm.sub %4, %1  : i32 loc(#loc)
    %67 = llvm.mul %4, %0  : i32 loc(#loc)
    %68 = llvm.sub %67, %1  : i32 loc(#loc)
    llvm.br ^bb1(%7, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %42, %54, %63 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb1(%69: i32 loc(unknown), %70: vector<8xf32> loc(unknown), %71: vector<8xf32> loc(unknown), %72: vector<8xf32> loc(unknown), %73: vector<8xf32> loc(unknown), %74: vector<8xf32> loc(unknown), %75: vector<8xf32> loc(unknown), %76: vector<8xf32> loc(unknown), %77: vector<8xf32> loc(unknown), %78: vector<8xf32> loc(unknown), %79: vector<8xf32> loc(unknown), %80: vector<8xf32> loc(unknown), %81: vector<8xf32> loc(unknown), %82: vector<8xf32> loc(unknown), %83: vector<8xf32> loc(unknown), %84: vector<8xf32> loc(unknown), %85: vector<8xf32> loc(unknown), %86: vector<2xi32> loc(unknown), %87: vector<2xi32> loc(unknown), %88: vector<2xi32> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %89 = llvm.icmp "slt" %69, %4 : i32 loc(#loc)
    llvm.cond_br %89, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %90 = llvm.extractelement %86[%7 : i32] : vector<2xi32> loc(#loc)
    %91 = llvm.extractelement %86[%1 : i32] : vector<2xi32> loc(#loc)
    %92 = genx.matrix.2Dblockload %arg0, %44, %45, %47, %90, %91 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<64xi16> loc(#loc)
    %93 = llvm.extractelement %87[%7 : i32] : vector<2xi32> loc(#loc)
    %94 = llvm.extractelement %87[%1 : i32] : vector<2xi32> loc(#loc)
    %95 = genx.matrix.2Dblockload %arg1, %56, %57, %59, %93, %94 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %96 = llvm.extractelement %88[%7 : i32] : vector<2xi32> loc(#loc)
    %97 = llvm.extractelement %88[%1 : i32] : vector<2xi32> loc(#loc)
    %98 = genx.matrix.2Dblockload %arg1, %65, %66, %68, %96, %97 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %99 = llvm.shufflevector %92, %92 [0, 1, 2, 3, 4, 5, 6, 7] : vector<64xi16>  loc(#loc)
    %100 = llvm.shufflevector %95, %95 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %101 = genx.matrix.dpas %70, %99, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %102 = llvm.shufflevector %92, %92 [32, 33, 34, 35, 36, 37, 38, 39] : vector<64xi16>  loc(#loc)
    %103 = llvm.shufflevector %95, %95 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %104 = genx.matrix.dpas %101, %102, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %105 = llvm.shufflevector %92, %92 [8, 9, 10, 11, 12, 13, 14, 15] : vector<64xi16>  loc(#loc)
    %106 = genx.matrix.dpas %71, %105, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %107 = llvm.shufflevector %92, %92 [40, 41, 42, 43, 44, 45, 46, 47] : vector<64xi16>  loc(#loc)
    %108 = genx.matrix.dpas %106, %107, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %109 = llvm.shufflevector %92, %92 [16, 17, 18, 19, 20, 21, 22, 23] : vector<64xi16>  loc(#loc)
    %110 = genx.matrix.dpas %72, %109, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %111 = llvm.shufflevector %92, %92 [48, 49, 50, 51, 52, 53, 54, 55] : vector<64xi16>  loc(#loc)
    %112 = genx.matrix.dpas %110, %111, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %113 = llvm.shufflevector %92, %92 [24, 25, 26, 27, 28, 29, 30, 31] : vector<64xi16>  loc(#loc)
    %114 = genx.matrix.dpas %73, %113, %100 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %115 = llvm.shufflevector %92, %92 [56, 57, 58, 59, 60, 61, 62, 63] : vector<64xi16>  loc(#loc)
    %116 = genx.matrix.dpas %114, %115, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %117 = llvm.shufflevector %95, %95 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %118 = genx.matrix.dpas %74, %99, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %119 = llvm.shufflevector %95, %95 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %120 = genx.matrix.dpas %118, %102, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %121 = genx.matrix.dpas %75, %105, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %122 = genx.matrix.dpas %121, %107, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %123 = genx.matrix.dpas %76, %109, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %124 = genx.matrix.dpas %123, %111, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %125 = genx.matrix.dpas %77, %113, %117 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %126 = genx.matrix.dpas %125, %115, %119 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %127 = llvm.shufflevector %98, %98 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %128 = genx.matrix.dpas %78, %99, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %129 = llvm.shufflevector %98, %98 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %130 = genx.matrix.dpas %128, %102, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %131 = genx.matrix.dpas %79, %105, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %132 = genx.matrix.dpas %131, %107, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %133 = genx.matrix.dpas %80, %109, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %134 = genx.matrix.dpas %133, %111, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %135 = genx.matrix.dpas %81, %113, %127 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %136 = genx.matrix.dpas %135, %115, %129 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %137 = llvm.shufflevector %98, %98 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %138 = genx.matrix.dpas %82, %99, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %139 = llvm.shufflevector %98, %98 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %140 = genx.matrix.dpas %138, %102, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %141 = genx.matrix.dpas %83, %105, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %142 = genx.matrix.dpas %141, %107, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %143 = genx.matrix.dpas %84, %109, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %144 = genx.matrix.dpas %143, %111, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %145 = genx.matrix.dpas %85, %113, %137 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %146 = genx.matrix.dpas %145, %115, %139 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %147 = llvm.extractelement %86[%7 : i32] : vector<2xi32> loc(#loc)
    %148 = llvm.add %147, %6  : i32 loc(#loc)
    %149 = llvm.insertelement %148, %86[%7 : i32] : vector<2xi32> loc(#loc)
    %150 = llvm.extractelement %87[%1 : i32] : vector<2xi32> loc(#loc)
    %151 = llvm.add %150, %6  : i32 loc(#loc)
    %152 = llvm.insertelement %151, %87[%1 : i32] : vector<2xi32> loc(#loc)
    %153 = llvm.extractelement %88[%1 : i32] : vector<2xi32> loc(#loc)
    %154 = llvm.add %153, %6  : i32 loc(#loc)
    %155 = llvm.insertelement %154, %88[%1 : i32] : vector<2xi32> loc(#loc)
    %156 = llvm.add %69, %6  : i32 loc(#loc)
    llvm.br ^bb1(%156, %104, %108, %112, %116, %120, %122, %124, %126, %130, %132, %134, %136, %140, %142, %144, %146, %149, %152, %155 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %157 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %158 = llvm.insertelement %51, %157[%7 : i32] : vector<2xi32> loc(#loc)
    %159 = llvm.insertelement %39, %158[%1 : i32] : vector<2xi32> loc(#loc)
    %160 = llvm.mul %4, %9  : i32 loc(#loc)
    %161 = llvm.sub %160, %1  : i32 loc(#loc)
    %162 = llvm.sub %4, %1  : i32 loc(#loc)
    %163 = llvm.mul %4, %9  : i32 loc(#loc)
    %164 = llvm.sub %163, %1  : i32 loc(#loc)
    %165 = llvm.add %39, %10  : i32 loc(#loc)
    %166 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %167 = llvm.insertelement %51, %166[%7 : i32] : vector<2xi32> loc(#loc)
    %168 = llvm.insertelement %165, %167[%1 : i32] : vector<2xi32> loc(#loc)
    %169 = llvm.mul %4, %9  : i32 loc(#loc)
    %170 = llvm.sub %169, %1  : i32 loc(#loc)
    %171 = llvm.sub %4, %1  : i32 loc(#loc)
    %172 = llvm.mul %4, %9  : i32 loc(#loc)
    %173 = llvm.sub %172, %1  : i32 loc(#loc)
    %174 = llvm.add %39, %3  : i32 loc(#loc)
    %175 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %176 = llvm.insertelement %51, %175[%7 : i32] : vector<2xi32> loc(#loc)
    %177 = llvm.insertelement %174, %176[%1 : i32] : vector<2xi32> loc(#loc)
    %178 = llvm.mul %4, %9  : i32 loc(#loc)
    %179 = llvm.sub %178, %1  : i32 loc(#loc)
    %180 = llvm.sub %4, %1  : i32 loc(#loc)
    %181 = llvm.mul %4, %9  : i32 loc(#loc)
    %182 = llvm.sub %181, %1  : i32 loc(#loc)
    %183 = llvm.add %39, %11  : i32 loc(#loc)
    %184 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %185 = llvm.insertelement %51, %184[%7 : i32] : vector<2xi32> loc(#loc)
    %186 = llvm.insertelement %183, %185[%1 : i32] : vector<2xi32> loc(#loc)
    %187 = llvm.mul %4, %9  : i32 loc(#loc)
    %188 = llvm.sub %187, %1  : i32 loc(#loc)
    %189 = llvm.sub %4, %1  : i32 loc(#loc)
    %190 = llvm.mul %4, %9  : i32 loc(#loc)
    %191 = llvm.sub %190, %1  : i32 loc(#loc)
    %192 = llvm.add %51, %3  : i32 loc(#loc)
    %193 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %194 = llvm.insertelement %192, %193[%7 : i32] : vector<2xi32> loc(#loc)
    %195 = llvm.insertelement %39, %194[%1 : i32] : vector<2xi32> loc(#loc)
    %196 = llvm.mul %4, %9  : i32 loc(#loc)
    %197 = llvm.sub %196, %1  : i32 loc(#loc)
    %198 = llvm.sub %4, %1  : i32 loc(#loc)
    %199 = llvm.mul %4, %9  : i32 loc(#loc)
    %200 = llvm.sub %199, %1  : i32 loc(#loc)
    %201 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %202 = llvm.insertelement %192, %201[%7 : i32] : vector<2xi32> loc(#loc)
    %203 = llvm.insertelement %165, %202[%1 : i32] : vector<2xi32> loc(#loc)
    %204 = llvm.mul %4, %9  : i32 loc(#loc)
    %205 = llvm.sub %204, %1  : i32 loc(#loc)
    %206 = llvm.sub %4, %1  : i32 loc(#loc)
    %207 = llvm.mul %4, %9  : i32 loc(#loc)
    %208 = llvm.sub %207, %1  : i32 loc(#loc)
    %209 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %210 = llvm.insertelement %192, %209[%7 : i32] : vector<2xi32> loc(#loc)
    %211 = llvm.insertelement %174, %210[%1 : i32] : vector<2xi32> loc(#loc)
    %212 = llvm.mul %4, %9  : i32 loc(#loc)
    %213 = llvm.sub %212, %1  : i32 loc(#loc)
    %214 = llvm.sub %4, %1  : i32 loc(#loc)
    %215 = llvm.mul %4, %9  : i32 loc(#loc)
    %216 = llvm.sub %215, %1  : i32 loc(#loc)
    %217 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %218 = llvm.insertelement %192, %217[%7 : i32] : vector<2xi32> loc(#loc)
    %219 = llvm.insertelement %183, %218[%1 : i32] : vector<2xi32> loc(#loc)
    %220 = llvm.mul %4, %9  : i32 loc(#loc)
    %221 = llvm.sub %220, %1  : i32 loc(#loc)
    %222 = llvm.sub %4, %1  : i32 loc(#loc)
    %223 = llvm.mul %4, %9  : i32 loc(#loc)
    %224 = llvm.sub %223, %1  : i32 loc(#loc)
    %225 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %226 = llvm.insertelement %60, %225[%7 : i32] : vector<2xi32> loc(#loc)
    %227 = llvm.insertelement %39, %226[%1 : i32] : vector<2xi32> loc(#loc)
    %228 = llvm.mul %4, %9  : i32 loc(#loc)
    %229 = llvm.sub %228, %1  : i32 loc(#loc)
    %230 = llvm.sub %4, %1  : i32 loc(#loc)
    %231 = llvm.mul %4, %9  : i32 loc(#loc)
    %232 = llvm.sub %231, %1  : i32 loc(#loc)
    %233 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %234 = llvm.insertelement %60, %233[%7 : i32] : vector<2xi32> loc(#loc)
    %235 = llvm.insertelement %165, %234[%1 : i32] : vector<2xi32> loc(#loc)
    %236 = llvm.mul %4, %9  : i32 loc(#loc)
    %237 = llvm.sub %236, %1  : i32 loc(#loc)
    %238 = llvm.sub %4, %1  : i32 loc(#loc)
    %239 = llvm.mul %4, %9  : i32 loc(#loc)
    %240 = llvm.sub %239, %1  : i32 loc(#loc)
    %241 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %242 = llvm.insertelement %60, %241[%7 : i32] : vector<2xi32> loc(#loc)
    %243 = llvm.insertelement %174, %242[%1 : i32] : vector<2xi32> loc(#loc)
    %244 = llvm.mul %4, %9  : i32 loc(#loc)
    %245 = llvm.sub %244, %1  : i32 loc(#loc)
    %246 = llvm.sub %4, %1  : i32 loc(#loc)
    %247 = llvm.mul %4, %9  : i32 loc(#loc)
    %248 = llvm.sub %247, %1  : i32 loc(#loc)
    %249 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %250 = llvm.insertelement %60, %249[%7 : i32] : vector<2xi32> loc(#loc)
    %251 = llvm.insertelement %183, %250[%1 : i32] : vector<2xi32> loc(#loc)
    %252 = llvm.mul %4, %9  : i32 loc(#loc)
    %253 = llvm.sub %252, %1  : i32 loc(#loc)
    %254 = llvm.sub %4, %1  : i32 loc(#loc)
    %255 = llvm.mul %4, %9  : i32 loc(#loc)
    %256 = llvm.sub %255, %1  : i32 loc(#loc)
    %257 = llvm.add %51, %12  : i32 loc(#loc)
    %258 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %259 = llvm.insertelement %257, %258[%7 : i32] : vector<2xi32> loc(#loc)
    %260 = llvm.insertelement %39, %259[%1 : i32] : vector<2xi32> loc(#loc)
    %261 = llvm.mul %4, %9  : i32 loc(#loc)
    %262 = llvm.sub %261, %1  : i32 loc(#loc)
    %263 = llvm.sub %4, %1  : i32 loc(#loc)
    %264 = llvm.mul %4, %9  : i32 loc(#loc)
    %265 = llvm.sub %264, %1  : i32 loc(#loc)
    %266 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %267 = llvm.insertelement %257, %266[%7 : i32] : vector<2xi32> loc(#loc)
    %268 = llvm.insertelement %165, %267[%1 : i32] : vector<2xi32> loc(#loc)
    %269 = llvm.mul %4, %9  : i32 loc(#loc)
    %270 = llvm.sub %269, %1  : i32 loc(#loc)
    %271 = llvm.sub %4, %1  : i32 loc(#loc)
    %272 = llvm.mul %4, %9  : i32 loc(#loc)
    %273 = llvm.sub %272, %1  : i32 loc(#loc)
    %274 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %275 = llvm.insertelement %257, %274[%7 : i32] : vector<2xi32> loc(#loc)
    %276 = llvm.insertelement %174, %275[%1 : i32] : vector<2xi32> loc(#loc)
    %277 = llvm.mul %4, %9  : i32 loc(#loc)
    %278 = llvm.sub %277, %1  : i32 loc(#loc)
    %279 = llvm.sub %4, %1  : i32 loc(#loc)
    %280 = llvm.mul %4, %9  : i32 loc(#loc)
    %281 = llvm.sub %280, %1  : i32 loc(#loc)
    %282 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %283 = llvm.insertelement %257, %282[%7 : i32] : vector<2xi32> loc(#loc)
    %284 = llvm.insertelement %183, %283[%1 : i32] : vector<2xi32> loc(#loc)
    %285 = llvm.mul %4, %9  : i32 loc(#loc)
    %286 = llvm.sub %285, %1  : i32 loc(#loc)
    %287 = llvm.sub %4, %1  : i32 loc(#loc)
    %288 = llvm.mul %4, %9  : i32 loc(#loc)
    %289 = llvm.sub %288, %1  : i32 loc(#loc)
    %290 = llvm.extractelement %159[%7 : i32] : vector<2xi32> loc(#loc)
    %291 = llvm.extractelement %159[%1 : i32] : vector<2xi32> loc(#loc)
    %292 = llvm.bitcast %70 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %161, %162, %164, %290, %291, %292 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %293 = llvm.extractelement %168[%7 : i32] : vector<2xi32> loc(#loc)
    %294 = llvm.extractelement %168[%1 : i32] : vector<2xi32> loc(#loc)
    %295 = llvm.bitcast %71 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %170, %171, %173, %293, %294, %295 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %296 = llvm.extractelement %177[%7 : i32] : vector<2xi32> loc(#loc)
    %297 = llvm.extractelement %177[%1 : i32] : vector<2xi32> loc(#loc)
    %298 = llvm.bitcast %72 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %179, %180, %182, %296, %297, %298 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %299 = llvm.extractelement %186[%7 : i32] : vector<2xi32> loc(#loc)
    %300 = llvm.extractelement %186[%1 : i32] : vector<2xi32> loc(#loc)
    %301 = llvm.bitcast %73 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %188, %189, %191, %299, %300, %301 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %302 = llvm.extractelement %195[%7 : i32] : vector<2xi32> loc(#loc)
    %303 = llvm.extractelement %195[%1 : i32] : vector<2xi32> loc(#loc)
    %304 = llvm.bitcast %74 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %197, %198, %200, %302, %303, %304 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %305 = llvm.extractelement %203[%7 : i32] : vector<2xi32> loc(#loc)
    %306 = llvm.extractelement %203[%1 : i32] : vector<2xi32> loc(#loc)
    %307 = llvm.bitcast %75 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %205, %206, %208, %305, %306, %307 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %308 = llvm.extractelement %211[%7 : i32] : vector<2xi32> loc(#loc)
    %309 = llvm.extractelement %211[%1 : i32] : vector<2xi32> loc(#loc)
    %310 = llvm.bitcast %76 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %213, %214, %216, %308, %309, %310 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %311 = llvm.extractelement %219[%7 : i32] : vector<2xi32> loc(#loc)
    %312 = llvm.extractelement %219[%1 : i32] : vector<2xi32> loc(#loc)
    %313 = llvm.bitcast %77 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %221, %222, %224, %311, %312, %313 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %314 = llvm.extractelement %227[%7 : i32] : vector<2xi32> loc(#loc)
    %315 = llvm.extractelement %227[%1 : i32] : vector<2xi32> loc(#loc)
    %316 = llvm.bitcast %78 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %229, %230, %232, %314, %315, %316 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %317 = llvm.extractelement %235[%7 : i32] : vector<2xi32> loc(#loc)
    %318 = llvm.extractelement %235[%1 : i32] : vector<2xi32> loc(#loc)
    %319 = llvm.bitcast %79 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %237, %238, %240, %317, %318, %319 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %320 = llvm.extractelement %243[%7 : i32] : vector<2xi32> loc(#loc)
    %321 = llvm.extractelement %243[%1 : i32] : vector<2xi32> loc(#loc)
    %322 = llvm.bitcast %80 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %245, %246, %248, %320, %321, %322 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %323 = llvm.extractelement %251[%7 : i32] : vector<2xi32> loc(#loc)
    %324 = llvm.extractelement %251[%1 : i32] : vector<2xi32> loc(#loc)
    %325 = llvm.bitcast %81 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %253, %254, %256, %323, %324, %325 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %326 = llvm.extractelement %260[%7 : i32] : vector<2xi32> loc(#loc)
    %327 = llvm.extractelement %260[%1 : i32] : vector<2xi32> loc(#loc)
    %328 = llvm.bitcast %82 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %262, %263, %265, %326, %327, %328 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %329 = llvm.extractelement %268[%7 : i32] : vector<2xi32> loc(#loc)
    %330 = llvm.extractelement %268[%1 : i32] : vector<2xi32> loc(#loc)
    %331 = llvm.bitcast %83 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %270, %271, %273, %329, %330, %331 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %332 = llvm.extractelement %276[%7 : i32] : vector<2xi32> loc(#loc)
    %333 = llvm.extractelement %276[%1 : i32] : vector<2xi32> loc(#loc)
    %334 = llvm.bitcast %84 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %278, %279, %281, %332, %333, %334 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %335 = llvm.extractelement %284[%7 : i32] : vector<2xi32> loc(#loc)
    %336 = llvm.extractelement %284[%1 : i32] : vector<2xi32> loc(#loc)
    %337 = llvm.bitcast %85 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %286, %287, %289, %335, %336, %337 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    llvm.return loc(#loc)
  } loc(#loc)
} loc(#loc)


// -----// IR Dump Before SymbolDCE (symbol-dce) ('builtin.module' operation) //----- //
#loc = loc(unknown)
module attributes {"triton_gpu.compute-capability" = 90 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 32 : i32, triton_gpu.shared = 0 : i32, "triton_gpu.threads-per-warp" = 16 : i32} {
  llvm.func @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(%arg0: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg1: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg2: !llvm.ptr<1> {tt.divisibility = 16 : i32} loc(unknown), %arg3: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg4: i32 {tt.divisibility = 16 : i32} loc(unknown), %arg5: i32 {tt.divisibility = 16 : i32} loc(unknown)) attributes {genx.intel_reqd_sub_group_size = [16 : i32], genx.kernel = 1 : i32, genx.max_work_group_size = [512 : i32, 1 : i32, 1 : i32], noinline = false} {
    %0 = llvm.mlir.constant(2 : i32) : i32 loc(#loc)
    %1 = llvm.mlir.constant(1 : i32) : i32 loc(#loc)
    %2 = llvm.mlir.constant(64 : i32) : i32 loc(#loc)
    %3 = llvm.mlir.constant(16 : i32) : i32 loc(#loc)
    %4 = llvm.mlir.constant(4096 : i32) : i32 loc(#loc)
    %5 = llvm.mlir.constant(dense<0.000000e+00> : vector<8xf32>) : vector<8xf32> loc(#loc)
    %6 = llvm.mlir.constant(32 : i32) : i32 loc(#loc)
    %7 = llvm.mlir.constant(0 : i32) : i32 loc(#loc)
    %8 = llvm.mlir.constant(256 : i32) : i32 loc(#loc)
    %9 = llvm.mlir.constant(4 : i32) : i32 loc(#loc)
    %10 = llvm.mlir.constant(8 : i32) : i32 loc(#loc)
    %11 = llvm.mlir.constant(24 : i32) : i32 loc(#loc)
    %12 = llvm.mlir.constant(48 : i32) : i32 loc(#loc)
    %13 = llvm.mlir.constant(63 : i32) : i32 loc(#loc)
    %14 = llvm.mlir.constant(7 : i32) : i32 loc(#loc)
    %15 = llvm.mlir.constant(3 : i32) : i32 loc(#loc)
    %16 = genx.workitem.id.x : i32 loc(#loc)
    %17 = genx.workitem.id.y : i32 loc(#loc)
    %18 = genx.workitem.id.z : i32 loc(#loc)
    %19 = genx.workgroup.dim.x : i32 loc(#loc)
    %20 = genx.workgroup.dim.y : i32 loc(#loc)
    %21 = llvm.mul %18, %20  : i32 loc(#loc)
    %22 = llvm.add %21, %17  : i32 loc(#loc)
    %23 = llvm.mul %22, %19  : i32 loc(#loc)
    %24 = llvm.add %23, %16  : i32 loc(#loc)
    %25 = llvm.udiv %24, %3  : i32 loc(#loc)
    %26 = genx.workgroup.id.x : i32 loc(#loc)
    %27 = llvm.sdiv %26, %2  : i32 loc(#loc)
    %28 = llvm.mul %27, %9  : i32 loc(#loc)
    %29 = llvm.sub %3, %28  : i32 loc(#loc)
    %30 = llvm.intr.smin(%29, %9)  : (i32, i32) -> i32 loc(#loc)
    %31 = llvm.srem %26, %30  : i32 loc(#loc)
    %32 = llvm.add %28, %31  : i32 loc(#loc)
    %33 = llvm.and %26, %13  : i32 loc(#loc)
    %34 = llvm.sdiv %33, %30  : i32 loc(#loc)
    %35 = llvm.mul %32, %8  : i32 loc(#loc)
    %36 = llvm.sdiv %25, %9  : i32 loc(#loc)
    %37 = llvm.and %36, %14  : i32 loc(#loc)
    %38 = llvm.mul %37, %6  : i32 loc(#loc)
    %39 = llvm.add %38, %35  : i32 loc(#loc)
    %40 = llvm.mlir.undef : vector<2xi32> loc(#loc)
    %41 = llvm.insertelement %7, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %42 = llvm.insertelement %39, %41[%1 : i32] : vector<2xi32> loc(#loc)
    %43 = llvm.mul %4, %0  : i32 loc(#loc)
    %44 = llvm.sub %43, %1  : i32 loc(#loc)
    %45 = llvm.sub %4, %1  : i32 loc(#loc)
    %46 = llvm.mul %34, %8  : i32 loc(#loc)
    %47 = llvm.and %25, %15  : i32 loc(#loc)
    %48 = llvm.mul %47, %2  : i32 loc(#loc)
    %49 = llvm.add %48, %46  : i32 loc(#loc)
    %50 = llvm.insertelement %49, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %51 = llvm.insertelement %7, %50[%1 : i32] : vector<2xi32> loc(#loc)
    %52 = llvm.add %49, %6  : i32 loc(#loc)
    %53 = llvm.insertelement %52, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %54 = llvm.insertelement %7, %53[%1 : i32] : vector<2xi32> loc(#loc)
    llvm.br ^bb1(%7, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %5, %42, %51, %54 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb1(%55: i32 loc(unknown), %56: vector<8xf32> loc(unknown), %57: vector<8xf32> loc(unknown), %58: vector<8xf32> loc(unknown), %59: vector<8xf32> loc(unknown), %60: vector<8xf32> loc(unknown), %61: vector<8xf32> loc(unknown), %62: vector<8xf32> loc(unknown), %63: vector<8xf32> loc(unknown), %64: vector<8xf32> loc(unknown), %65: vector<8xf32> loc(unknown), %66: vector<8xf32> loc(unknown), %67: vector<8xf32> loc(unknown), %68: vector<8xf32> loc(unknown), %69: vector<8xf32> loc(unknown), %70: vector<8xf32> loc(unknown), %71: vector<8xf32> loc(unknown), %72: vector<2xi32> loc(unknown), %73: vector<2xi32> loc(unknown), %74: vector<2xi32> loc(unknown)):  // 2 preds: ^bb0, ^bb2
    %75 = llvm.icmp "slt" %55, %4 : i32 loc(#loc)
    llvm.cond_br %75, ^bb2, ^bb3 loc(#loc)
  ^bb2:  // pred: ^bb1
    %76 = llvm.extractelement %72[%7 : i32] : vector<2xi32> loc(#loc)
    %77 = llvm.extractelement %72[%1 : i32] : vector<2xi32> loc(#loc)
    %78 = genx.matrix.2Dblockload %arg0, %44, %45, %44, %76, %77 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<64xi16> loc(#loc)
    %79 = llvm.extractelement %73[%7 : i32] : vector<2xi32> loc(#loc)
    %80 = llvm.extractelement %73[%1 : i32] : vector<2xi32> loc(#loc)
    %81 = genx.matrix.2Dblockload %arg1, %44, %45, %44, %79, %80 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %82 = llvm.extractelement %74[%7 : i32] : vector<2xi32> loc(#loc)
    %83 = llvm.extractelement %74[%1 : i32] : vector<2xi32> loc(#loc)
    %84 = genx.matrix.2Dblockload %arg1, %44, %45, %44, %82, %83 {elem_size_in_bits = 16 : i32, tile_height = 32 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 2 : i32, vnni_transform = true} : (!llvm.ptr<1>, i32, i32, i32, i32, i32) -> vector<32xi32> loc(#loc)
    %85 = llvm.shufflevector %78, %78 [0, 1, 2, 3, 4, 5, 6, 7] : vector<64xi16>  loc(#loc)
    %86 = llvm.shufflevector %81, %81 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %87 = genx.matrix.dpas %56, %85, %86 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %88 = llvm.shufflevector %78, %78 [32, 33, 34, 35, 36, 37, 38, 39] : vector<64xi16>  loc(#loc)
    %89 = llvm.shufflevector %81, %81 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %90 = genx.matrix.dpas %87, %88, %89 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %91 = llvm.shufflevector %78, %78 [8, 9, 10, 11, 12, 13, 14, 15] : vector<64xi16>  loc(#loc)
    %92 = genx.matrix.dpas %57, %91, %86 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %93 = llvm.shufflevector %78, %78 [40, 41, 42, 43, 44, 45, 46, 47] : vector<64xi16>  loc(#loc)
    %94 = genx.matrix.dpas %92, %93, %89 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %95 = llvm.shufflevector %78, %78 [16, 17, 18, 19, 20, 21, 22, 23] : vector<64xi16>  loc(#loc)
    %96 = genx.matrix.dpas %58, %95, %86 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %97 = llvm.shufflevector %78, %78 [48, 49, 50, 51, 52, 53, 54, 55] : vector<64xi16>  loc(#loc)
    %98 = genx.matrix.dpas %96, %97, %89 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %99 = llvm.shufflevector %78, %78 [24, 25, 26, 27, 28, 29, 30, 31] : vector<64xi16>  loc(#loc)
    %100 = genx.matrix.dpas %59, %99, %86 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %101 = llvm.shufflevector %78, %78 [56, 57, 58, 59, 60, 61, 62, 63] : vector<64xi16>  loc(#loc)
    %102 = genx.matrix.dpas %100, %101, %89 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %103 = llvm.shufflevector %81, %81 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %104 = genx.matrix.dpas %60, %85, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %105 = llvm.shufflevector %81, %81 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %106 = genx.matrix.dpas %104, %88, %105 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %107 = genx.matrix.dpas %61, %91, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %108 = genx.matrix.dpas %107, %93, %105 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %109 = genx.matrix.dpas %62, %95, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %110 = genx.matrix.dpas %109, %97, %105 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %111 = genx.matrix.dpas %63, %99, %103 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %112 = genx.matrix.dpas %111, %101, %105 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %113 = llvm.shufflevector %84, %84 [0, 1, 2, 3, 4, 5, 6, 7] : vector<32xi32>  loc(#loc)
    %114 = genx.matrix.dpas %64, %85, %113 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %115 = llvm.shufflevector %84, %84 [8, 9, 10, 11, 12, 13, 14, 15] : vector<32xi32>  loc(#loc)
    %116 = genx.matrix.dpas %114, %88, %115 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %117 = genx.matrix.dpas %65, %91, %113 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %118 = genx.matrix.dpas %117, %93, %115 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %119 = genx.matrix.dpas %66, %95, %113 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %120 = genx.matrix.dpas %119, %97, %115 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %121 = genx.matrix.dpas %67, %99, %113 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %122 = genx.matrix.dpas %121, %101, %115 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %123 = llvm.shufflevector %84, %84 [16, 17, 18, 19, 20, 21, 22, 23] : vector<32xi32>  loc(#loc)
    %124 = genx.matrix.dpas %68, %85, %123 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %125 = llvm.shufflevector %84, %84 [24, 25, 26, 27, 28, 29, 30, 31] : vector<32xi32>  loc(#loc)
    %126 = genx.matrix.dpas %124, %88, %125 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %127 = genx.matrix.dpas %69, %91, %123 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %128 = genx.matrix.dpas %127, %93, %125 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %129 = genx.matrix.dpas %70, %95, %123 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %130 = genx.matrix.dpas %129, %97, %125 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %131 = genx.matrix.dpas %71, %99, %123 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %132 = genx.matrix.dpas %131, %101, %125 {pa = #genx.precision_type<FP16>, pb = #genx.precision_type<FP16>, rc = 8 : i32} : (vector<8xf32>, vector<8xi16>, vector<8xi32>) -> vector<8xf32> loc(#loc)
    %133 = llvm.add %76, %6  : i32 loc(#loc)
    %134 = llvm.insertelement %133, %72[%7 : i32] : vector<2xi32> loc(#loc)
    %135 = llvm.add %80, %6  : i32 loc(#loc)
    %136 = llvm.insertelement %135, %73[%1 : i32] : vector<2xi32> loc(#loc)
    %137 = llvm.add %83, %6  : i32 loc(#loc)
    %138 = llvm.insertelement %137, %74[%1 : i32] : vector<2xi32> loc(#loc)
    %139 = llvm.add %55, %6  : i32 loc(#loc)
    llvm.br ^bb1(%139, %90, %94, %98, %102, %106, %108, %110, %112, %116, %118, %120, %122, %126, %128, %130, %132, %134, %136, %138 : i32, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<8xf32>, vector<2xi32>, vector<2xi32>, vector<2xi32>) loc(#loc)
  ^bb3:  // pred: ^bb1
    %140 = llvm.insertelement %39, %50[%1 : i32] : vector<2xi32> loc(#loc)
    %141 = llvm.mul %4, %9  : i32 loc(#loc)
    %142 = llvm.sub %141, %1  : i32 loc(#loc)
    %143 = llvm.add %39, %10  : i32 loc(#loc)
    %144 = llvm.insertelement %143, %50[%1 : i32] : vector<2xi32> loc(#loc)
    %145 = llvm.add %39, %3  : i32 loc(#loc)
    %146 = llvm.insertelement %145, %50[%1 : i32] : vector<2xi32> loc(#loc)
    %147 = llvm.add %39, %11  : i32 loc(#loc)
    %148 = llvm.insertelement %147, %50[%1 : i32] : vector<2xi32> loc(#loc)
    %149 = llvm.add %49, %3  : i32 loc(#loc)
    %150 = llvm.insertelement %149, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %151 = llvm.insertelement %39, %150[%1 : i32] : vector<2xi32> loc(#loc)
    %152 = llvm.insertelement %143, %150[%1 : i32] : vector<2xi32> loc(#loc)
    %153 = llvm.insertelement %145, %150[%1 : i32] : vector<2xi32> loc(#loc)
    %154 = llvm.insertelement %147, %150[%1 : i32] : vector<2xi32> loc(#loc)
    %155 = llvm.insertelement %39, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %156 = llvm.insertelement %143, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %157 = llvm.insertelement %145, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %158 = llvm.insertelement %147, %53[%1 : i32] : vector<2xi32> loc(#loc)
    %159 = llvm.add %49, %12  : i32 loc(#loc)
    %160 = llvm.insertelement %159, %40[%7 : i32] : vector<2xi32> loc(#loc)
    %161 = llvm.insertelement %39, %160[%1 : i32] : vector<2xi32> loc(#loc)
    %162 = llvm.insertelement %143, %160[%1 : i32] : vector<2xi32> loc(#loc)
    %163 = llvm.insertelement %145, %160[%1 : i32] : vector<2xi32> loc(#loc)
    %164 = llvm.insertelement %147, %160[%1 : i32] : vector<2xi32> loc(#loc)
    %165 = llvm.extractelement %140[%7 : i32] : vector<2xi32> loc(#loc)
    %166 = llvm.extractelement %140[%1 : i32] : vector<2xi32> loc(#loc)
    %167 = llvm.bitcast %56 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %165, %166, %167 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %168 = llvm.extractelement %144[%7 : i32] : vector<2xi32> loc(#loc)
    %169 = llvm.extractelement %144[%1 : i32] : vector<2xi32> loc(#loc)
    %170 = llvm.bitcast %57 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %168, %169, %170 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %171 = llvm.extractelement %146[%7 : i32] : vector<2xi32> loc(#loc)
    %172 = llvm.extractelement %146[%1 : i32] : vector<2xi32> loc(#loc)
    %173 = llvm.bitcast %58 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %171, %172, %173 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %174 = llvm.extractelement %148[%7 : i32] : vector<2xi32> loc(#loc)
    %175 = llvm.extractelement %148[%1 : i32] : vector<2xi32> loc(#loc)
    %176 = llvm.bitcast %59 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %174, %175, %176 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %177 = llvm.extractelement %151[%7 : i32] : vector<2xi32> loc(#loc)
    %178 = llvm.extractelement %151[%1 : i32] : vector<2xi32> loc(#loc)
    %179 = llvm.bitcast %60 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %177, %178, %179 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %180 = llvm.extractelement %152[%7 : i32] : vector<2xi32> loc(#loc)
    %181 = llvm.extractelement %152[%1 : i32] : vector<2xi32> loc(#loc)
    %182 = llvm.bitcast %61 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %180, %181, %182 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %183 = llvm.extractelement %153[%7 : i32] : vector<2xi32> loc(#loc)
    %184 = llvm.extractelement %153[%1 : i32] : vector<2xi32> loc(#loc)
    %185 = llvm.bitcast %62 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %183, %184, %185 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %186 = llvm.extractelement %154[%7 : i32] : vector<2xi32> loc(#loc)
    %187 = llvm.extractelement %154[%1 : i32] : vector<2xi32> loc(#loc)
    %188 = llvm.bitcast %63 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %186, %187, %188 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %189 = llvm.extractelement %155[%7 : i32] : vector<2xi32> loc(#loc)
    %190 = llvm.extractelement %155[%1 : i32] : vector<2xi32> loc(#loc)
    %191 = llvm.bitcast %64 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %189, %190, %191 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %192 = llvm.extractelement %156[%7 : i32] : vector<2xi32> loc(#loc)
    %193 = llvm.extractelement %156[%1 : i32] : vector<2xi32> loc(#loc)
    %194 = llvm.bitcast %65 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %192, %193, %194 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %195 = llvm.extractelement %157[%7 : i32] : vector<2xi32> loc(#loc)
    %196 = llvm.extractelement %157[%1 : i32] : vector<2xi32> loc(#loc)
    %197 = llvm.bitcast %66 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %195, %196, %197 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %198 = llvm.extractelement %158[%7 : i32] : vector<2xi32> loc(#loc)
    %199 = llvm.extractelement %158[%1 : i32] : vector<2xi32> loc(#loc)
    %200 = llvm.bitcast %67 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %198, %199, %200 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %201 = llvm.extractelement %161[%7 : i32] : vector<2xi32> loc(#loc)
    %202 = llvm.extractelement %161[%1 : i32] : vector<2xi32> loc(#loc)
    %203 = llvm.bitcast %68 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %201, %202, %203 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %204 = llvm.extractelement %162[%7 : i32] : vector<2xi32> loc(#loc)
    %205 = llvm.extractelement %162[%1 : i32] : vector<2xi32> loc(#loc)
    %206 = llvm.bitcast %69 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %204, %205, %206 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %207 = llvm.extractelement %163[%7 : i32] : vector<2xi32> loc(#loc)
    %208 = llvm.extractelement %163[%1 : i32] : vector<2xi32> loc(#loc)
    %209 = llvm.bitcast %70 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %207, %208, %209 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    %210 = llvm.extractelement %164[%7 : i32] : vector<2xi32> loc(#loc)
    %211 = llvm.extractelement %164[%1 : i32] : vector<2xi32> loc(#loc)
    %212 = llvm.bitcast %71 : vector<8xf32> to vector<8xi32> loc(#loc)
    genx.matrix.2Dblockstore %arg2, %142, %45, %142, %210, %211, %212 {elem_size_in_bits = 32 : i32, tile_height = 8 : i32, tile_width = 16 : i32, transpose = false, v_blocks = 1 : i32, vnni_transform = false} : (!llvm.ptr<1>, i32, i32, i32, i32, i32, vector<8xi32>) loc(#loc)
    llvm.return loc(#loc)
  } loc(#loc)
} loc(#loc)


num_warps 32
first llvm module
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

define spir_kernel void @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(ptr addrspace(1) %0, ptr addrspace(1) %1, ptr addrspace(1) %2, i32 %3, i32 %4, i32 %5) !intel_reqd_sub_group_size !0 !max_work_group_size !1 {
  %7 = call i64 @_Z12get_local_idj(i32 0)
  %8 = trunc i64 %7 to i32
  %9 = call i64 @_Z12get_local_idj(i32 1)
  %10 = trunc i64 %9 to i32
  %11 = call i64 @_Z12get_local_idj(i32 2)
  %12 = trunc i64 %11 to i32
  %13 = call i64 @_Z14get_local_sizej(i32 0)
  %14 = trunc i64 %13 to i32
  %15 = call i64 @_Z14get_local_sizej(i32 1)
  %16 = trunc i64 %15 to i32
  %17 = mul i32 %12, %16
  %18 = add i32 %17, %10
  %19 = mul i32 %18, %14
  %20 = add i32 %19, %8
  %21 = udiv i32 %20, 16
  %22 = call i64 @_Z12get_group_idj(i32 0)
  %23 = trunc i64 %22 to i32
  %24 = sdiv i32 %23, 64
  %25 = mul i32 %24, 4
  %26 = sub i32 16, %25
  %27 = call i32 @llvm.smin.i32(i32 %26, i32 4)
  %28 = srem i32 %23, %27
  %29 = add i32 %25, %28
  %30 = and i32 %23, 63
  %31 = sdiv i32 %30, %27
  %32 = mul i32 %29, 256
  %33 = sdiv i32 %21, 4
  %34 = and i32 %33, 7
  %35 = mul i32 %34, 32
  %36 = add i32 %35, %32
  %37 = insertelement <2 x i32> <i32 0, i32 undef>, i32 %36, i32 1
  %38 = mul i32 %31, 256
  %39 = and i32 %21, 3
  %40 = mul i32 %39, 64
  %41 = add i32 %40, %38
  %42 = insertelement <2 x i32> undef, i32 %41, i32 0
  %43 = insertelement <2 x i32> %42, i32 0, i32 1
  %44 = add i32 %41, 32
  %45 = insertelement <2 x i32> undef, i32 %44, i32 0
  %46 = insertelement <2 x i32> %45, i32 0, i32 1
  br label %47

47:                                               ; preds = %69, %6
  %48 = phi i32 [ %136, %69 ], [ 0, %6 ]
  %49 = phi <8 x float> [ %87, %69 ], [ zeroinitializer, %6 ]
  %50 = phi <8 x float> [ %91, %69 ], [ zeroinitializer, %6 ]
  %51 = phi <8 x float> [ %95, %69 ], [ zeroinitializer, %6 ]
  %52 = phi <8 x float> [ %99, %69 ], [ zeroinitializer, %6 ]
  %53 = phi <8 x float> [ %103, %69 ], [ zeroinitializer, %6 ]
  %54 = phi <8 x float> [ %105, %69 ], [ zeroinitializer, %6 ]
  %55 = phi <8 x float> [ %107, %69 ], [ zeroinitializer, %6 ]
  %56 = phi <8 x float> [ %109, %69 ], [ zeroinitializer, %6 ]
  %57 = phi <8 x float> [ %113, %69 ], [ zeroinitializer, %6 ]
  %58 = phi <8 x float> [ %115, %69 ], [ zeroinitializer, %6 ]
  %59 = phi <8 x float> [ %117, %69 ], [ zeroinitializer, %6 ]
  %60 = phi <8 x float> [ %119, %69 ], [ zeroinitializer, %6 ]
  %61 = phi <8 x float> [ %123, %69 ], [ zeroinitializer, %6 ]
  %62 = phi <8 x float> [ %125, %69 ], [ zeroinitializer, %6 ]
  %63 = phi <8 x float> [ %127, %69 ], [ zeroinitializer, %6 ]
  %64 = phi <8 x float> [ %129, %69 ], [ zeroinitializer, %6 ]
  %65 = phi <2 x i32> [ %131, %69 ], [ %37, %6 ]
  %66 = phi <2 x i32> [ %133, %69 ], [ %43, %6 ]
  %67 = phi <2 x i32> [ %135, %69 ], [ %46, %6 ]
  %68 = icmp slt i32 %48, 4096
  br i1 %68, label %69, label %137

69:                                               ; preds = %47
  %70 = extractelement <2 x i32> %65, i32 0
  %71 = extractelement <2 x i32> %65, i32 1
  %72 = ptrtoint ptr addrspace(1) %0 to i64
  %73 = call <64 x i16> @llvm.genx.GenISA.LSC2DBlockRead.v64i16(i64 %72, i32 8191, i32 4095, i32 8191, i32 %70, i32 %71, i32 16, i32 16, i32 32, i32 2, i1 false, i1 false, i32 0)
  %74 = extractelement <2 x i32> %66, i32 0
  %75 = extractelement <2 x i32> %66, i32 1
  %76 = ptrtoint ptr addrspace(1) %1 to i64
  %77 = call <32 x i32> @llvm.genx.GenISA.LSC2DBlockRead.v32i32(i64 %76, i32 8191, i32 4095, i32 8191, i32 %74, i32 %75, i32 16, i32 16, i32 32, i32 2, i1 false, i1 true, i32 0)
  %78 = extractelement <2 x i32> %67, i32 0
  %79 = extractelement <2 x i32> %67, i32 1
  %80 = ptrtoint ptr addrspace(1) %1 to i64
  %81 = call <32 x i32> @llvm.genx.GenISA.LSC2DBlockRead.v32i32(i64 %80, i32 8191, i32 4095, i32 8191, i32 %78, i32 %79, i32 16, i32 16, i32 32, i32 2, i1 false, i1 true, i32 0)
  %82 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %83 = shufflevector <32 x i32> %77, <32 x i32> %77, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %84 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %49, <8 x i16> %82, <8 x i32> %83, i32 12, i32 12, i32 8, i32 8, i1 false)
  %85 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39>
  %86 = shufflevector <32 x i32> %77, <32 x i32> %77, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %87 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %84, <8 x i16> %85, <8 x i32> %86, i32 12, i32 12, i32 8, i32 8, i1 false)
  %88 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %89 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %50, <8 x i16> %88, <8 x i32> %83, i32 12, i32 12, i32 8, i32 8, i1 false)
  %90 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47>
  %91 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %89, <8 x i16> %90, <8 x i32> %86, i32 12, i32 12, i32 8, i32 8, i1 false)
  %92 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %93 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %51, <8 x i16> %92, <8 x i32> %83, i32 12, i32 12, i32 8, i32 8, i1 false)
  %94 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 48, i32 49, i32 50, i32 51, i32 52, i32 53, i32 54, i32 55>
  %95 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %93, <8 x i16> %94, <8 x i32> %86, i32 12, i32 12, i32 8, i32 8, i1 false)
  %96 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %97 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %52, <8 x i16> %96, <8 x i32> %83, i32 12, i32 12, i32 8, i32 8, i1 false)
  %98 = shufflevector <64 x i16> %73, <64 x i16> %73, <8 x i32> <i32 56, i32 57, i32 58, i32 59, i32 60, i32 61, i32 62, i32 63>
  %99 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %97, <8 x i16> %98, <8 x i32> %86, i32 12, i32 12, i32 8, i32 8, i1 false)
  %100 = shufflevector <32 x i32> %77, <32 x i32> %77, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %101 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %53, <8 x i16> %82, <8 x i32> %100, i32 12, i32 12, i32 8, i32 8, i1 false)
  %102 = shufflevector <32 x i32> %77, <32 x i32> %77, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %103 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %101, <8 x i16> %85, <8 x i32> %102, i32 12, i32 12, i32 8, i32 8, i1 false)
  %104 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %54, <8 x i16> %88, <8 x i32> %100, i32 12, i32 12, i32 8, i32 8, i1 false)
  %105 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %104, <8 x i16> %90, <8 x i32> %102, i32 12, i32 12, i32 8, i32 8, i1 false)
  %106 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %55, <8 x i16> %92, <8 x i32> %100, i32 12, i32 12, i32 8, i32 8, i1 false)
  %107 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %106, <8 x i16> %94, <8 x i32> %102, i32 12, i32 12, i32 8, i32 8, i1 false)
  %108 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %56, <8 x i16> %96, <8 x i32> %100, i32 12, i32 12, i32 8, i32 8, i1 false)
  %109 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %108, <8 x i16> %98, <8 x i32> %102, i32 12, i32 12, i32 8, i32 8, i1 false)
  %110 = shufflevector <32 x i32> %81, <32 x i32> %81, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %111 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %57, <8 x i16> %82, <8 x i32> %110, i32 12, i32 12, i32 8, i32 8, i1 false)
  %112 = shufflevector <32 x i32> %81, <32 x i32> %81, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %113 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %111, <8 x i16> %85, <8 x i32> %112, i32 12, i32 12, i32 8, i32 8, i1 false)
  %114 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %58, <8 x i16> %88, <8 x i32> %110, i32 12, i32 12, i32 8, i32 8, i1 false)
  %115 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %114, <8 x i16> %90, <8 x i32> %112, i32 12, i32 12, i32 8, i32 8, i1 false)
  %116 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %59, <8 x i16> %92, <8 x i32> %110, i32 12, i32 12, i32 8, i32 8, i1 false)
  %117 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %116, <8 x i16> %94, <8 x i32> %112, i32 12, i32 12, i32 8, i32 8, i1 false)
  %118 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %60, <8 x i16> %96, <8 x i32> %110, i32 12, i32 12, i32 8, i32 8, i1 false)
  %119 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %118, <8 x i16> %98, <8 x i32> %112, i32 12, i32 12, i32 8, i32 8, i1 false)
  %120 = shufflevector <32 x i32> %81, <32 x i32> %81, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %121 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %61, <8 x i16> %82, <8 x i32> %120, i32 12, i32 12, i32 8, i32 8, i1 false)
  %122 = shufflevector <32 x i32> %81, <32 x i32> %81, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %123 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %121, <8 x i16> %85, <8 x i32> %122, i32 12, i32 12, i32 8, i32 8, i1 false)
  %124 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %62, <8 x i16> %88, <8 x i32> %120, i32 12, i32 12, i32 8, i32 8, i1 false)
  %125 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %124, <8 x i16> %90, <8 x i32> %122, i32 12, i32 12, i32 8, i32 8, i1 false)
  %126 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %63, <8 x i16> %92, <8 x i32> %120, i32 12, i32 12, i32 8, i32 8, i1 false)
  %127 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %126, <8 x i16> %94, <8 x i32> %122, i32 12, i32 12, i32 8, i32 8, i1 false)
  %128 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %64, <8 x i16> %96, <8 x i32> %120, i32 12, i32 12, i32 8, i32 8, i1 false)
  %129 = call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %128, <8 x i16> %98, <8 x i32> %122, i32 12, i32 12, i32 8, i32 8, i1 false)
  %130 = add i32 %70, 32
  %131 = insertelement <2 x i32> %65, i32 %130, i32 0
  %132 = add i32 %75, 32
  %133 = insertelement <2 x i32> %66, i32 %132, i32 1
  %134 = add i32 %79, 32
  %135 = insertelement <2 x i32> %67, i32 %134, i32 1
  %136 = add i32 %48, 32
  br label %47

137:                                              ; preds = %47
  %138 = insertelement <2 x i32> %42, i32 %36, i32 1
  %139 = add i32 %36, 8
  %140 = insertelement <2 x i32> %42, i32 %139, i32 1
  %141 = add i32 %36, 16
  %142 = insertelement <2 x i32> %42, i32 %141, i32 1
  %143 = add i32 %36, 24
  %144 = insertelement <2 x i32> %42, i32 %143, i32 1
  %145 = add i32 %41, 16
  %146 = insertelement <2 x i32> undef, i32 %145, i32 0
  %147 = insertelement <2 x i32> %146, i32 %36, i32 1
  %148 = insertelement <2 x i32> %146, i32 %139, i32 1
  %149 = insertelement <2 x i32> %146, i32 %141, i32 1
  %150 = insertelement <2 x i32> %146, i32 %143, i32 1
  %151 = insertelement <2 x i32> %45, i32 %36, i32 1
  %152 = insertelement <2 x i32> %45, i32 %139, i32 1
  %153 = insertelement <2 x i32> %45, i32 %141, i32 1
  %154 = insertelement <2 x i32> %45, i32 %143, i32 1
  %155 = add i32 %41, 48
  %156 = insertelement <2 x i32> undef, i32 %155, i32 0
  %157 = insertelement <2 x i32> %156, i32 %36, i32 1
  %158 = insertelement <2 x i32> %156, i32 %139, i32 1
  %159 = insertelement <2 x i32> %156, i32 %141, i32 1
  %160 = insertelement <2 x i32> %156, i32 %143, i32 1
  %161 = extractelement <2 x i32> %138, i32 0
  %162 = extractelement <2 x i32> %138, i32 1
  %163 = bitcast <8 x float> %49 to <8 x i32>
  %164 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %164, i32 16383, i32 4095, i32 16383, i32 %161, i32 %162, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %163)
  %165 = extractelement <2 x i32> %140, i32 0
  %166 = extractelement <2 x i32> %140, i32 1
  %167 = bitcast <8 x float> %50 to <8 x i32>
  %168 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %168, i32 16383, i32 4095, i32 16383, i32 %165, i32 %166, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %167)
  %169 = extractelement <2 x i32> %142, i32 0
  %170 = extractelement <2 x i32> %142, i32 1
  %171 = bitcast <8 x float> %51 to <8 x i32>
  %172 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %172, i32 16383, i32 4095, i32 16383, i32 %169, i32 %170, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %171)
  %173 = extractelement <2 x i32> %144, i32 0
  %174 = extractelement <2 x i32> %144, i32 1
  %175 = bitcast <8 x float> %52 to <8 x i32>
  %176 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %176, i32 16383, i32 4095, i32 16383, i32 %173, i32 %174, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %175)
  %177 = extractelement <2 x i32> %147, i32 0
  %178 = extractelement <2 x i32> %147, i32 1
  %179 = bitcast <8 x float> %53 to <8 x i32>
  %180 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %180, i32 16383, i32 4095, i32 16383, i32 %177, i32 %178, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %179)
  %181 = extractelement <2 x i32> %148, i32 0
  %182 = extractelement <2 x i32> %148, i32 1
  %183 = bitcast <8 x float> %54 to <8 x i32>
  %184 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %184, i32 16383, i32 4095, i32 16383, i32 %181, i32 %182, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %183)
  %185 = extractelement <2 x i32> %149, i32 0
  %186 = extractelement <2 x i32> %149, i32 1
  %187 = bitcast <8 x float> %55 to <8 x i32>
  %188 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %188, i32 16383, i32 4095, i32 16383, i32 %185, i32 %186, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %187)
  %189 = extractelement <2 x i32> %150, i32 0
  %190 = extractelement <2 x i32> %150, i32 1
  %191 = bitcast <8 x float> %56 to <8 x i32>
  %192 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %192, i32 16383, i32 4095, i32 16383, i32 %189, i32 %190, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %191)
  %193 = extractelement <2 x i32> %151, i32 0
  %194 = extractelement <2 x i32> %151, i32 1
  %195 = bitcast <8 x float> %57 to <8 x i32>
  %196 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %196, i32 16383, i32 4095, i32 16383, i32 %193, i32 %194, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %195)
  %197 = extractelement <2 x i32> %152, i32 0
  %198 = extractelement <2 x i32> %152, i32 1
  %199 = bitcast <8 x float> %58 to <8 x i32>
  %200 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %200, i32 16383, i32 4095, i32 16383, i32 %197, i32 %198, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %199)
  %201 = extractelement <2 x i32> %153, i32 0
  %202 = extractelement <2 x i32> %153, i32 1
  %203 = bitcast <8 x float> %59 to <8 x i32>
  %204 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %204, i32 16383, i32 4095, i32 16383, i32 %201, i32 %202, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %203)
  %205 = extractelement <2 x i32> %154, i32 0
  %206 = extractelement <2 x i32> %154, i32 1
  %207 = bitcast <8 x float> %60 to <8 x i32>
  %208 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %208, i32 16383, i32 4095, i32 16383, i32 %205, i32 %206, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %207)
  %209 = extractelement <2 x i32> %157, i32 0
  %210 = extractelement <2 x i32> %157, i32 1
  %211 = bitcast <8 x float> %61 to <8 x i32>
  %212 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %212, i32 16383, i32 4095, i32 16383, i32 %209, i32 %210, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %211)
  %213 = extractelement <2 x i32> %158, i32 0
  %214 = extractelement <2 x i32> %158, i32 1
  %215 = bitcast <8 x float> %62 to <8 x i32>
  %216 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %216, i32 16383, i32 4095, i32 16383, i32 %213, i32 %214, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %215)
  %217 = extractelement <2 x i32> %159, i32 0
  %218 = extractelement <2 x i32> %159, i32 1
  %219 = bitcast <8 x float> %63 to <8 x i32>
  %220 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %220, i32 16383, i32 4095, i32 16383, i32 %217, i32 %218, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %219)
  %221 = extractelement <2 x i32> %160, i32 0
  %222 = extractelement <2 x i32> %160, i32 1
  %223 = bitcast <8 x float> %64 to <8 x i32>
  %224 = ptrtoint ptr addrspace(1) %2 to i64
  call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %224, i32 16383, i32 4095, i32 16383, i32 %221, i32 %222, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %223)
  ret void
}

declare spir_func i64 @_Z12get_local_idj(i32)

declare spir_func i64 @_Z14get_local_sizej(i32)

declare spir_func i64 @_Z12get_group_idj(i32)

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.smin.i32(i32, i32) #0

; Function Attrs: nounwind
declare void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64, i32, i32, i32, i32, i32, i32, i32, i32, i32, i1, i1, i32, <8 x i32>) #1

; Function Attrs: nounwind
declare <64 x i16> @llvm.genx.GenISA.LSC2DBlockRead.v64i16(i64, i32, i32, i32, i32, i32, i32, i32, i32, i32, i1, i1, i32) #1

; Function Attrs: nounwind
declare <32 x i32> @llvm.genx.GenISA.LSC2DBlockRead.v32i32(i64, i32, i32, i32, i32, i32, i32, i32, i32, i32, i1, i1, i32) #1

; Function Attrs: convergent nounwind
declare <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float>, <8 x i16>, <8 x i32>, i32, i32, i32, i32, i1) #2

attributes #0 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #1 = { nounwind }
attributes #2 = { convergent nounwind }

!0 = !{i64 16}
!1 = !{i64 512, i64 1, i64 1}

second llvm module
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"
target datalayout = "e-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-n8:16:32:64"
target triple = "spir64-unknown-unknown"

define spir_kernel void @matmul_kernel_with_block_pointers_0d1d2d3d4d5d(ptr addrspace(1) %0, ptr addrspace(1) %1, ptr addrspace(1) %2, i32 %3, i32 %4, i32 %5) local_unnamed_addr !intel_reqd_sub_group_size !6 !max_work_group_size !7 {
  %7 = tail call i64 @_Z12get_local_idj(i32 0)
  %8 = trunc i64 %7 to i32
  %9 = tail call i64 @_Z12get_local_idj(i32 1)
  %10 = trunc i64 %9 to i32
  %11 = tail call i64 @_Z12get_local_idj(i32 2)
  %12 = trunc i64 %11 to i32
  %13 = tail call i64 @_Z14get_local_sizej(i32 0)
  %14 = trunc i64 %13 to i32
  %15 = tail call i64 @_Z14get_local_sizej(i32 1)
  %16 = trunc i64 %15 to i32
  %17 = mul i32 %16, %12
  %18 = add i32 %17, %10
  %19 = mul i32 %18, %14
  %20 = add i32 %19, %8
  %21 = tail call i64 @_Z12get_group_idj(i32 0)
  %22 = trunc i64 %21 to i32
  %23 = sdiv i32 %22, 64
  %24 = shl nsw i32 %23, 2
  %25 = sub nsw i32 16, %24
  %26 = tail call i32 @llvm.smin.i32(i32 %25, i32 4)
  %27 = srem i32 %22, %26
  %28 = add nsw i32 %24, %27
  %29 = and i32 %22, 63
  %30 = sdiv i32 %29, %26
  %31 = shl i32 %28, 8
  %32 = lshr i32 %20, 1
  %33 = and i32 %32, 224
  %34 = or disjoint i32 %33, %31
  %35 = insertelement <2 x i32> <i32 0, i32 poison>, i32 %34, i64 1
  %36 = shl nsw i32 %30, 8
  %37 = shl i32 %20, 2
  %38 = and i32 %37, 192
  %39 = or disjoint i32 %38, %36
  %40 = insertelement <2 x i32> <i32 poison, i32 0>, i32 %39, i64 0
  %41 = or disjoint i32 %39, 32
  %42 = insertelement <2 x i32> <i32 poison, i32 0>, i32 %41, i64 0
  %43 = ptrtoint ptr addrspace(1) %0 to i64
  %44 = ptrtoint ptr addrspace(1) %1 to i64
  br label %45

45:                                               ; preds = %6, %45
  %46 = phi <2 x i32> [ %42, %6 ], [ %128, %45 ]
  %47 = phi <2 x i32> [ %40, %6 ], [ %126, %45 ]
  %48 = phi <2 x i32> [ %35, %6 ], [ %124, %45 ]
  %49 = phi <8 x float> [ zeroinitializer, %6 ], [ %122, %45 ]
  %50 = phi <8 x float> [ zeroinitializer, %6 ], [ %120, %45 ]
  %51 = phi <8 x float> [ zeroinitializer, %6 ], [ %118, %45 ]
  %52 = phi <8 x float> [ zeroinitializer, %6 ], [ %116, %45 ]
  %53 = phi <8 x float> [ zeroinitializer, %6 ], [ %112, %45 ]
  %54 = phi <8 x float> [ zeroinitializer, %6 ], [ %110, %45 ]
  %55 = phi <8 x float> [ zeroinitializer, %6 ], [ %108, %45 ]
  %56 = phi <8 x float> [ zeroinitializer, %6 ], [ %106, %45 ]
  %57 = phi <8 x float> [ zeroinitializer, %6 ], [ %102, %45 ]
  %58 = phi <8 x float> [ zeroinitializer, %6 ], [ %100, %45 ]
  %59 = phi <8 x float> [ zeroinitializer, %6 ], [ %98, %45 ]
  %60 = phi <8 x float> [ zeroinitializer, %6 ], [ %96, %45 ]
  %61 = phi <8 x float> [ zeroinitializer, %6 ], [ %92, %45 ]
  %62 = phi <8 x float> [ zeroinitializer, %6 ], [ %88, %45 ]
  %63 = phi <8 x float> [ zeroinitializer, %6 ], [ %84, %45 ]
  %64 = phi <8 x float> [ zeroinitializer, %6 ], [ %80, %45 ]
  %65 = phi i32 [ 0, %6 ], [ %129, %45 ]
  %66 = extractelement <2 x i32> %48, i64 0
  %67 = extractelement <2 x i32> %48, i64 1
  %68 = tail call <64 x i16> @llvm.genx.GenISA.LSC2DBlockRead.v64i16(i64 %43, i32 8191, i32 4095, i32 8191, i32 %66, i32 %67, i32 16, i32 16, i32 32, i32 2, i1 false, i1 false, i32 0)
  %69 = extractelement <2 x i32> %47, i64 0
  %70 = extractelement <2 x i32> %47, i64 1
  %71 = tail call <32 x i32> @llvm.genx.GenISA.LSC2DBlockRead.v32i32(i64 %44, i32 8191, i32 4095, i32 8191, i32 %69, i32 %70, i32 16, i32 16, i32 32, i32 2, i1 false, i1 true, i32 0)
  %72 = extractelement <2 x i32> %46, i64 0
  %73 = extractelement <2 x i32> %46, i64 1
  %74 = tail call <32 x i32> @llvm.genx.GenISA.LSC2DBlockRead.v32i32(i64 %44, i32 8191, i32 4095, i32 8191, i32 %72, i32 %73, i32 16, i32 16, i32 32, i32 2, i1 false, i1 true, i32 0)
  %75 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %76 = shufflevector <32 x i32> %71, <32 x i32> poison, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %77 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %64, <8 x i16> %75, <8 x i32> %76, i32 12, i32 12, i32 8, i32 8, i1 false)
  %78 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39>
  %79 = shufflevector <32 x i32> %71, <32 x i32> poison, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %80 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %77, <8 x i16> %78, <8 x i32> %79, i32 12, i32 12, i32 8, i32 8, i1 false)
  %81 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %82 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %63, <8 x i16> %81, <8 x i32> %76, i32 12, i32 12, i32 8, i32 8, i1 false)
  %83 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47>
  %84 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %82, <8 x i16> %83, <8 x i32> %79, i32 12, i32 12, i32 8, i32 8, i1 false)
  %85 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %86 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %62, <8 x i16> %85, <8 x i32> %76, i32 12, i32 12, i32 8, i32 8, i1 false)
  %87 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 48, i32 49, i32 50, i32 51, i32 52, i32 53, i32 54, i32 55>
  %88 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %86, <8 x i16> %87, <8 x i32> %79, i32 12, i32 12, i32 8, i32 8, i1 false)
  %89 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %90 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %61, <8 x i16> %89, <8 x i32> %76, i32 12, i32 12, i32 8, i32 8, i1 false)
  %91 = shufflevector <64 x i16> %68, <64 x i16> poison, <8 x i32> <i32 56, i32 57, i32 58, i32 59, i32 60, i32 61, i32 62, i32 63>
  %92 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %90, <8 x i16> %91, <8 x i32> %79, i32 12, i32 12, i32 8, i32 8, i1 false)
  %93 = shufflevector <32 x i32> %71, <32 x i32> poison, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %94 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %60, <8 x i16> %75, <8 x i32> %93, i32 12, i32 12, i32 8, i32 8, i1 false)
  %95 = shufflevector <32 x i32> %71, <32 x i32> poison, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %96 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %94, <8 x i16> %78, <8 x i32> %95, i32 12, i32 12, i32 8, i32 8, i1 false)
  %97 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %59, <8 x i16> %81, <8 x i32> %93, i32 12, i32 12, i32 8, i32 8, i1 false)
  %98 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %97, <8 x i16> %83, <8 x i32> %95, i32 12, i32 12, i32 8, i32 8, i1 false)
  %99 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %58, <8 x i16> %85, <8 x i32> %93, i32 12, i32 12, i32 8, i32 8, i1 false)
  %100 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %99, <8 x i16> %87, <8 x i32> %95, i32 12, i32 12, i32 8, i32 8, i1 false)
  %101 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %57, <8 x i16> %89, <8 x i32> %93, i32 12, i32 12, i32 8, i32 8, i1 false)
  %102 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %101, <8 x i16> %91, <8 x i32> %95, i32 12, i32 12, i32 8, i32 8, i1 false)
  %103 = shufflevector <32 x i32> %74, <32 x i32> poison, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %104 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %56, <8 x i16> %75, <8 x i32> %103, i32 12, i32 12, i32 8, i32 8, i1 false)
  %105 = shufflevector <32 x i32> %74, <32 x i32> poison, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %106 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %104, <8 x i16> %78, <8 x i32> %105, i32 12, i32 12, i32 8, i32 8, i1 false)
  %107 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %55, <8 x i16> %81, <8 x i32> %103, i32 12, i32 12, i32 8, i32 8, i1 false)
  %108 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %107, <8 x i16> %83, <8 x i32> %105, i32 12, i32 12, i32 8, i32 8, i1 false)
  %109 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %54, <8 x i16> %85, <8 x i32> %103, i32 12, i32 12, i32 8, i32 8, i1 false)
  %110 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %109, <8 x i16> %87, <8 x i32> %105, i32 12, i32 12, i32 8, i32 8, i1 false)
  %111 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %53, <8 x i16> %89, <8 x i32> %103, i32 12, i32 12, i32 8, i32 8, i1 false)
  %112 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %111, <8 x i16> %91, <8 x i32> %105, i32 12, i32 12, i32 8, i32 8, i1 false)
  %113 = shufflevector <32 x i32> %74, <32 x i32> poison, <8 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %114 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %52, <8 x i16> %75, <8 x i32> %113, i32 12, i32 12, i32 8, i32 8, i1 false)
  %115 = shufflevector <32 x i32> %74, <32 x i32> poison, <8 x i32> <i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %116 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %114, <8 x i16> %78, <8 x i32> %115, i32 12, i32 12, i32 8, i32 8, i1 false)
  %117 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %51, <8 x i16> %81, <8 x i32> %113, i32 12, i32 12, i32 8, i32 8, i1 false)
  %118 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %117, <8 x i16> %83, <8 x i32> %115, i32 12, i32 12, i32 8, i32 8, i1 false)
  %119 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %50, <8 x i16> %85, <8 x i32> %113, i32 12, i32 12, i32 8, i32 8, i1 false)
  %120 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %119, <8 x i16> %87, <8 x i32> %115, i32 12, i32 12, i32 8, i32 8, i1 false)
  %121 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %49, <8 x i16> %89, <8 x i32> %113, i32 12, i32 12, i32 8, i32 8, i1 false)
  %122 = tail call <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float> %121, <8 x i16> %91, <8 x i32> %115, i32 12, i32 12, i32 8, i32 8, i1 false)
  %123 = add i32 %66, 32
  %124 = insertelement <2 x i32> %48, i32 %123, i64 0
  %125 = add i32 %70, 32
  %126 = insertelement <2 x i32> %47, i32 %125, i64 1
  %127 = add i32 %73, 32
  %128 = insertelement <2 x i32> %46, i32 %127, i64 1
  %129 = add nuw nsw i32 %65, 32
  %130 = icmp ult i32 %65, 4064
  br i1 %130, label %45, label %131

131:                                              ; preds = %45
  %132 = or disjoint i32 %34, 8
  %133 = or disjoint i32 %34, 16
  %134 = or disjoint i32 %34, 24
  %135 = or disjoint i32 %39, 16
  %136 = or disjoint i32 %39, 48
  %137 = bitcast <8 x float> %80 to <8 x i32>
  %138 = ptrtoint ptr addrspace(1) %2 to i64
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %39, i32 %34, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %137)
  %139 = bitcast <8 x float> %84 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %39, i32 %132, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %139)
  %140 = bitcast <8 x float> %88 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %39, i32 %133, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %140)
  %141 = bitcast <8 x float> %92 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %39, i32 %134, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %141)
  %142 = bitcast <8 x float> %96 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %135, i32 %34, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %142)
  %143 = bitcast <8 x float> %98 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %135, i32 %132, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %143)
  %144 = bitcast <8 x float> %100 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %135, i32 %133, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %144)
  %145 = bitcast <8 x float> %102 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %135, i32 %134, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %145)
  %146 = bitcast <8 x float> %106 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %41, i32 %34, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %146)
  %147 = bitcast <8 x float> %108 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %41, i32 %132, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %147)
  %148 = bitcast <8 x float> %110 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %41, i32 %133, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %148)
  %149 = bitcast <8 x float> %112 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %41, i32 %134, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %149)
  %150 = bitcast <8 x float> %116 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %136, i32 %34, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %150)
  %151 = bitcast <8 x float> %118 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %136, i32 %132, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %151)
  %152 = bitcast <8 x float> %120 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %136, i32 %133, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %152)
  %153 = bitcast <8 x float> %122 to <8 x i32>
  tail call void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64 %138, i32 16383, i32 4095, i32 16383, i32 %136, i32 %134, i32 32, i32 16, i32 8, i32 1, i1 false, i1 false, i32 0, <8 x i32> %153)
  ret void
}

declare spir_func i64 @_Z12get_local_idj(i32) local_unnamed_addr

declare spir_func i64 @_Z14get_local_sizej(i32) local_unnamed_addr

declare spir_func i64 @_Z12get_group_idj(i32) local_unnamed_addr

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.smin.i32(i32, i32) #0

; Function Attrs: nounwind
declare void @llvm.genx.GenISA.LSC2DBlockWrite.v8i32(i64, i32, i32, i32, i32, i32, i32, i32, i32, i32, i1, i1, i32, <8 x i32>) #1

; Function Attrs: nounwind
declare <64 x i16> @llvm.genx.GenISA.LSC2DBlockRead.v64i16(i64, i32, i32, i32, i32, i32, i32, i32, i32, i32, i1, i1, i32) #1

; Function Attrs: nounwind
declare <32 x i32> @llvm.genx.GenISA.LSC2DBlockRead.v32i32(i64, i32, i32, i32, i32, i32, i32, i32, i32, i32, i1, i1, i32) #1

; Function Attrs: convergent nounwind
declare <8 x float> @llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32(<8 x float>, <8 x i16>, <8 x i32>, i32, i32, i32, i32, i1) #2

attributes #0 = { mustprogress nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #1 = { nounwind }
attributes #2 = { convergent nounwind }

!opencl.spir.version = !{!0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0}
!spirv.Source = !{!1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1, !1}
!opencl.compiler.options = !{!2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2, !2}
!llvm.ident = !{!3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3, !3}
!llvm.module.flags = !{!4, !5}

!0 = !{i32 1, i32 2}
!1 = !{i32 4, i32 100000}
!2 = !{}
!3 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2023.2.0 (2023.2.0.20230622)"}
!4 = !{i32 1, !"wchar_size", i32 4}
!5 = !{i32 7, !"frame-pointer", i32 2}
!6 = !{i64 16}
!7 = !{i64 512, i64 1, i64 1}
#    3                                      '     :           
  SPV_INTEL_kernel_attributes 
  SPV_INTEL_vector_compute         OpenCL.std               *  matmul_kernel_with_block_pointers_0d1d2d3d4d5d    *       *  #        *                        _Z12get_local_idj        _Z14get_local_sizej   	   _Z12get_group_idj        llvm.genx.GenISA.LSC2DBlockWrite.v8i32    !   llvm.genx.GenISA.LSC2DBlockRead.v64i16    1   llvm.genx.GenISA.LSC2DBlockRead.v32i32    C   llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32   O   matmul_kernel_with_block_pointers_0d1d2d3d4d5d  G 	    )   _Z12get_local_idj      G 	    )   _Z14get_local_sizej    G 	 	   )   _Z12get_group_idj      G     )   llvm.genx.GenISA.LSC2DBlockWrite.v8i32     G  !   )   llvm.genx.GenISA.LSC2DBlockRead.v64i16     G  1   )   llvm.genx.GenISA.LSC2DBlockRead.v32i32     G  C   )   llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32    G  O   )   matmul_kernel_with_block_pointers_0d1d2d3d4d5d      G  n   u  G  p   u  G  u   u  G     u  G     u  G     v  G     u  G     v       @                                  L          +     Y       +     \      +     _      +     l   @   +     o      +     q      +     v   ?   +     y      +     |              +           +            +          +          +           +          +          +       0   +     
  ?  !                                !                                                          @   !                                                  /          !  0   /                                            ?         @   ?        A         !  B   @   @   A                        M      L   ! 	 N      M   M   M                       ,        Y      ,           Y   .  @      *        )                  /      6               7        8  6               7        8  6     	          7     
   8  6               7        7        7        7        7        7        7        7        7        7        7        7        7        7        8  6     !           7     "   7     #   7     $   7     %   7     &   7     '   7     (   7     )   7     *   7     +   7     ,   7     -   7     .   8  6  /   1       0   7     2   7     3   7     4   7     5   7     6   7     7   7     8   7     9   7     :   7     ;   7     <   7     =   7     >   8  6  @   C       B   7  @   D   7  A   E   7     F   7     G   7     H   7     I   7     J   7     K   8  6     O       N   7  M   P   7  M   Q   7  M   R   7     S   7     T   7     U     V   9     Z      Y   q     [   Z   9     ]      \   q     ^   ]   9     `      _   q     a   `   9     b      Y   q     c   b   9     d      \   q     e   d        f   e   a        g   f   ^        h   g   c        i   h   [   9     j   	   Y   q     k   j        m   k   l        n   m   _        p   o   n        r   p   q        s   r   p   q        t   k   s        u   n   t        w   k   v        x   w   s        z   u   y        {   i   \        }   {   |        ~   }   z   R        ~                 x   y           i   _                               R                                R                  u        P   u        Q     W     W              V      W              V      W              V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W     @         V      W           Y   V      W   Q               Q              9        !                     o   o      _         Y   Q               Q              9  /      1                     o   o      _         Y   Q               Q              9  /      1                     o   o      _         Y   O  A                                     O                                       9  @      C                  y   y      O  A                !   "   #   $   %   &   '   O                 	   
                  9  @      C                  y   y      O  A               	   
                  9  @      C                  y   y      O  A            (   )   *   +   ,   -   .   /   9  @      C                  y   y      O  A                                    9  @      C                  y   y      O  A            0   1   2   3   4   5   6   7   9  @      C                  y   y      O  A                                    9  @      C                  y   y      O  A            8   9   :   ;   <   =   >   ?   9  @      C                  y   y      O                                      9  @      C                  y   y      O                                      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      O                                       9  @      C                  y   y      O                 	   
                  9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      O                                      9  @      C                  y   y      O                                      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y      9  @      C                  y   y                    R                                R                               R                                                  W   X     X          ~   y          ~   o          ~               o               |          u     	  R   9          	  
     
     ~      o   y   \         Y     |          9          	  
     
          o   y   \         Y     |          9          	  
     
          o   y   \         Y     |          9          	  
     
          o   y   \         Y     |          9          	  
     
    ~      o   y   \         Y     |          9          	  
     
         o   y   \         Y     |          9          	  
     
         o   y   \         Y     |          9          	  
     
         o   y   \         Y     |          9          	  
     
     ~      o   y   \         Y     |          9          	  
     
          o   y   \         Y     |          9          	  
     
          o   y   \         Y     |           9     !     	  
     
          o   y   \         Y      |     "     9     #     	  
     
    ~      o   y   \         Y   "  |     $     9     %     	  
     
         o   y   \         Y   $  |     &     9     '     	  
     
         o   y   \         Y   &  |     (     9     )     	  
     
         o   y   \         Y   (    8  6     *      N   7  M   +  7  M   ,  7  M   -  7     .  7     /  7     0    1  9 
    2  O   +  ,  -  .  /  0    8  

first spirv module
b'\x03\x02#\x07\x00\x04\x01\x00\x0e\x00\x06\x003\x01\x00\x00\x00\x00\x00\x00\x11\x00\x02\x00\x04\x00\x00\x00\x11\x00\x02\x00\x05\x00\x00\x00\x11\x00\x02\x00\x06\x00\x00\x00\x11\x00\x02\x00\x07\x00\x00\x00\x11\x00\x02\x00\x0b\x00\x00\x00\x11\x00\x02\x00\x16\x00\x00\x00\x11\x00\x02\x00\'\x00\x00\x00\x11\x00\x02\x00:\x00\x00\x00\x11\x00\x02\x00\xf3\x15\x00\x00\x11\x00\x02\x00\x04\x17\x00\x00\n\x00\x08\x00SPV_INTEL_kernel_attributes\x00\n\x00\x08\x00SPV_INTEL_vector_compute\x00\x00\x00\x00\x0b\x00\x05\x00\x01\x00\x00\x00OpenCL.std\x00\x00\x0e\x00\x03\x00\x02\x00\x00\x00\x02\x00\x00\x00\x0f\x00\x0f\x00\x06\x00\x00\x00*\x01\x00\x00matmul_kernel_with_block_pointers_0d1d2d3d4d5d\x00\x00\x10\x00\x03\x00*\x01\x00\x00\x1f\x00\x00\x00\x10\x00\x04\x00*\x01\x00\x00#\x00\x00\x00\x10\x00\x00\x00\x10\x00\x06\x00*\x01\x00\x00\x05\x17\x00\x00\x00\x02\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x03\x00\x03\x00\x04\x00\x00\x00\xa0\x86\x01\x00\x05\x00\x07\x00\x05\x00\x00\x00_Z12get_local_idj\x00\x00\x00\x05\x00\x07\x00\x07\x00\x00\x00_Z14get_local_sizej\x00\x05\x00\x07\x00\t\x00\x00\x00_Z12get_group_idj\x00\x00\x00\x05\x00\x0c\x00\x0f\x00\x00\x00llvm.genx.GenISA.LSC2DBlockWrite.v8i32\x00\x00\x05\x00\x0c\x00!\x00\x00\x00llvm.genx.GenISA.LSC2DBlockRead.v64i16\x00\x00\x05\x00\x0c\x001\x00\x00\x00llvm.genx.GenISA.LSC2DBlockRead.v32i32\x00\x00\x05\x00\x10\x00C\x00\x00\x00llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32\x00\x05\x00\x0e\x00O\x00\x00\x00matmul_kernel_with_block_pointers_0d1d2d3d4d5d\x00\x00G\x00\t\x00\x05\x00\x00\x00)\x00\x00\x00_Z12get_local_idj\x00\x00\x00\x01\x00\x00\x00G\x00\t\x00\x07\x00\x00\x00)\x00\x00\x00_Z14get_local_sizej\x00\x01\x00\x00\x00G\x00\t\x00\t\x00\x00\x00)\x00\x00\x00_Z12get_group_idj\x00\x00\x00\x01\x00\x00\x00G\x00\x0e\x00\x0f\x00\x00\x00)\x00\x00\x00llvm.genx.GenISA.LSC2DBlockWrite.v8i32\x00\x00\x01\x00\x00\x00G\x00\x0e\x00!\x00\x00\x00)\x00\x00\x00llvm.genx.GenISA.LSC2DBlockRead.v64i16\x00\x00\x01\x00\x00\x00G\x00\x0e\x001\x00\x00\x00)\x00\x00\x00llvm.genx.GenISA.LSC2DBlockRead.v32i32\x00\x00\x01\x00\x00\x00G\x00\x12\x00C\x00\x00\x00)\x00\x00\x00llvm.genx.GenISA.sub.group.dpas.v8f32.v8f32.v8i16.v8i32\x00\x01\x00\x00\x00G\x00\x10\x00O\x00\x00\x00)\x00\x00\x00matmul_kernel_with_block_pointers_0d1d2d3d4d5d\x00\x00\x00\x00\x00\x00G\x00\x03\x00n\x00\x00\x00u\x11\x00\x00G\x00\x03\x00p\x00\x00\x00u\x11\x00\x00G\x00\x03\x00u\x00\x00\x00u\x11\x00\x00G\x00\x03\x00\x83\x00\x00\x00u\x11\x00\x00G\x00\x03\x00\xb6\x00\x00\x00u\x11\x00\x00G\x00\x03\x00\xb6\x00\x00\x00v\x11\x00\x00G\x00\x03\x00\xb6\x00\x00\x00u\x11\x00\x00G\x00\x03\x00\xb6\x00\x00\x00v\x11\x00\x00\x15\x00\x04\x00\x02\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00\x15\x00\x04\x00\x03\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x15\x00\x04\x00\x1e\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x15\x00\x04\x00L\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00Y\x00\x00\x00\x00\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\\\x00\x00\x00\x01\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00_\x00\x00\x00\x02\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00l\x00\x00\x00@\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00o\x00\x00\x00\x10\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00q\x00\x00\x00\x04\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00v\x00\x00\x00?\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00y\x00\x00\x00\x08\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00|\x00\x00\x00\xe0\x00\x00\x00\x01\x00\x03\x00\x03\x00\x00\x00\x80\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\x85\x00\x00\x00\xc0\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\x8a\x00\x00\x00 \x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\xba\x00\x00\x00\xff\x1f\x00\x00+\x00\x04\x00\x03\x00\x00\x00\xbb\x00\x00\x00\xff\x0f\x00\x00+\x00\x04\x00\x03\x00\x00\x00\xc9\x00\x00\x00\x0c\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\xff\x00\x00\x00\xe0\x0f\x00\x00+\x00\x04\x00\x03\x00\x00\x00\x03\x01\x00\x00\x18\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\x06\x01\x00\x000\x00\x00\x00+\x00\x04\x00\x03\x00\x00\x00\n\x01\x00\x00\xff?\x00\x00!\x00\x04\x00\x04\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x13\x00\x02\x00\x0b\x00\x00\x00\x14\x00\x02\x00\x0c\x00\x00\x00\x17\x00\x04\x00\r\x00\x00\x00\x03\x00\x00\x00\x08\x00\x00\x00!\x00\x11\x00\x0e\x00\x00\x00\x0b\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x0c\x00\x00\x00\x0c\x00\x00\x00\x03\x00\x00\x00\r\x00\x00\x00\x17\x00\x04\x00\x1f\x00\x00\x00\x1e\x00\x00\x00@\x00\x00\x00!\x00\x10\x00 \x00\x00\x00\x1f\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x0c\x00\x00\x00\x0c\x00\x00\x00\x03\x00\x00\x00\x17\x00\x04\x00/\x00\x00\x00\x03\x00\x00\x00 \x00\x00\x00!\x00\x10\x000\x00\x00\x00/\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x0c\x00\x00\x00\x0c\x00\x00\x00\x03\x00\x00\x00\x16\x00\x03\x00?\x00\x00\x00 \x00\x00\x00\x17\x00\x04\x00@\x00\x00\x00?\x00\x00\x00\x08\x00\x00\x00\x17\x00\x04\x00A\x00\x00\x00\x1e\x00\x00\x00\x08\x00\x00\x00!\x00\x0b\x00B\x00\x00\x00@\x00\x00\x00@\x00\x00\x00A\x00\x00\x00\r\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x0c\x00\x00\x00 \x00\x04\x00M\x00\x00\x00\x05\x00\x00\x00L\x00\x00\x00!\x00\t\x00N\x00\x00\x00\x0b\x00\x00\x00M\x00\x00\x00M\x00\x00\x00M\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x17\x00\x04\x00\x7f\x00\x00\x00\x03\x00\x00\x00\x02\x00\x00\x00,\x00\x05\x00\x7f\x00\x00\x00\x81\x00\x00\x00Y\x00\x00\x00\x80\x00\x00\x00,\x00\x05\x00\x7f\x00\x00\x00\x88\x00\x00\x00\x80\x00\x00\x00Y\x00\x00\x00.\x00\x03\x00@\x00\x00\x00\x95\x00\x00\x00*\x00\x03\x00\x0c\x00\x00\x00\xbc\x00\x00\x00)\x00\x03\x00\x0c\x00\x00\x00\xc0\x00\x00\x00\x01\x00\x03\x00\x1f\x00\x00\x00\xc5\x00\x00\x00\x01\x00\x03\x00/\x00\x00\x00\xc7\x00\x00\x006\x00\x05\x00\x02\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x06\x00\x00\x008\x00\x01\x006\x00\x05\x00\x02\x00\x00\x00\x07\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x08\x00\x00\x008\x00\x01\x006\x00\x05\x00\x02\x00\x00\x00\t\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\n\x00\x00\x008\x00\x01\x006\x00\x05\x00\x0b\x00\x00\x00\x0f\x00\x00\x00\x00\x00\x00\x00\x0e\x00\x00\x007\x00\x03\x00\x02\x00\x00\x00\x10\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x11\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x12\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x13\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x14\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x15\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x16\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x17\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x18\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x19\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00\x1a\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00\x1b\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\x1c\x00\x00\x007\x00\x03\x00\r\x00\x00\x00\x1d\x00\x00\x008\x00\x01\x006\x00\x05\x00\x1f\x00\x00\x00!\x00\x00\x00\x00\x00\x00\x00 \x00\x00\x007\x00\x03\x00\x02\x00\x00\x00"\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00#\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00$\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00%\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00&\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00\'\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00(\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00)\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00*\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00+\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00,\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00-\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00.\x00\x00\x008\x00\x01\x006\x00\x05\x00/\x00\x00\x001\x00\x00\x00\x00\x00\x00\x000\x00\x00\x007\x00\x03\x00\x02\x00\x00\x002\x00\x00\x007\x00\x03\x00\x03\x00\x00\x003\x00\x00\x007\x00\x03\x00\x03\x00\x00\x004\x00\x00\x007\x00\x03\x00\x03\x00\x00\x005\x00\x00\x007\x00\x03\x00\x03\x00\x00\x006\x00\x00\x007\x00\x03\x00\x03\x00\x00\x007\x00\x00\x007\x00\x03\x00\x03\x00\x00\x008\x00\x00\x007\x00\x03\x00\x03\x00\x00\x009\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00:\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00;\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00<\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00=\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00>\x00\x00\x008\x00\x01\x006\x00\x05\x00@\x00\x00\x00C\x00\x00\x00\x00\x00\x00\x00B\x00\x00\x007\x00\x03\x00@\x00\x00\x00D\x00\x00\x007\x00\x03\x00A\x00\x00\x00E\x00\x00\x007\x00\x03\x00\r\x00\x00\x00F\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00G\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00H\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00I\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00J\x00\x00\x007\x00\x03\x00\x0c\x00\x00\x00K\x00\x00\x008\x00\x01\x006\x00\x05\x00\x0b\x00\x00\x00O\x00\x00\x00\x00\x00\x00\x00N\x00\x00\x007\x00\x03\x00M\x00\x00\x00P\x00\x00\x007\x00\x03\x00M\x00\x00\x00Q\x00\x00\x007\x00\x03\x00M\x00\x00\x00R\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00S\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00T\x00\x00\x007\x00\x03\x00\x03\x00\x00\x00U\x00\x00\x00\xf8\x00\x02\x00V\x00\x00\x009\x00\x05\x00\x02\x00\x00\x00Z\x00\x00\x00\x05\x00\x00\x00Y\x00\x00\x00q\x00\x04\x00\x03\x00\x00\x00[\x00\x00\x00Z\x00\x00\x009\x00\x05\x00\x02\x00\x00\x00]\x00\x00\x00\x05\x00\x00\x00\\\x00\x00\x00q\x00\x04\x00\x03\x00\x00\x00^\x00\x00\x00]\x00\x00\x009\x00\x05\x00\x02\x00\x00\x00`\x00\x00\x00\x05\x00\x00\x00_\x00\x00\x00q\x00\x04\x00\x03\x00\x00\x00a\x00\x00\x00`\x00\x00\x009\x00\x05\x00\x02\x00\x00\x00b\x00\x00\x00\x07\x00\x00\x00Y\x00\x00\x00q\x00\x04\x00\x03\x00\x00\x00c\x00\x00\x00b\x00\x00\x009\x00\x05\x00\x02\x00\x00\x00d\x00\x00\x00\x07\x00\x00\x00\\\x00\x00\x00q\x00\x04\x00\x03\x00\x00\x00e\x00\x00\x00d\x00\x00\x00\x84\x00\x05\x00\x03\x00\x00\x00f\x00\x00\x00e\x00\x00\x00a\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00g\x00\x00\x00f\x00\x00\x00^\x00\x00\x00\x84\x00\x05\x00\x03\x00\x00\x00h\x00\x00\x00g\x00\x00\x00c\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00i\x00\x00\x00h\x00\x00\x00[\x00\x00\x009\x00\x05\x00\x02\x00\x00\x00j\x00\x00\x00\t\x00\x00\x00Y\x00\x00\x00q\x00\x04\x00\x03\x00\x00\x00k\x00\x00\x00j\x00\x00\x00\x87\x00\x05\x00\x03\x00\x00\x00m\x00\x00\x00k\x00\x00\x00l\x00\x00\x00\xc4\x00\x05\x00\x03\x00\x00\x00n\x00\x00\x00m\x00\x00\x00_\x00\x00\x00\x82\x00\x05\x00\x03\x00\x00\x00p\x00\x00\x00o\x00\x00\x00n\x00\x00\x00\xb1\x00\x05\x00\x0c\x00\x00\x00r\x00\x00\x00p\x00\x00\x00q\x00\x00\x00\xa9\x00\x06\x00\x03\x00\x00\x00s\x00\x00\x00r\x00\x00\x00p\x00\x00\x00q\x00\x00\x00\x8a\x00\x05\x00\x03\x00\x00\x00t\x00\x00\x00k\x00\x00\x00s\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00u\x00\x00\x00n\x00\x00\x00t\x00\x00\x00\xc7\x00\x05\x00\x03\x00\x00\x00w\x00\x00\x00k\x00\x00\x00v\x00\x00\x00\x87\x00\x05\x00\x03\x00\x00\x00x\x00\x00\x00w\x00\x00\x00s\x00\x00\x00\xc4\x00\x05\x00\x03\x00\x00\x00z\x00\x00\x00u\x00\x00\x00y\x00\x00\x00\xc2\x00\x05\x00\x03\x00\x00\x00{\x00\x00\x00i\x00\x00\x00\\\x00\x00\x00\xc7\x00\x05\x00\x03\x00\x00\x00}\x00\x00\x00{\x00\x00\x00|\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00~\x00\x00\x00}\x00\x00\x00z\x00\x00\x00R\x00\x06\x00\x7f\x00\x00\x00\x82\x00\x00\x00~\x00\x00\x00\x81\x00\x00\x00\x01\x00\x00\x00\xc4\x00\x05\x00\x03\x00\x00\x00\x83\x00\x00\x00x\x00\x00\x00y\x00\x00\x00\xc4\x00\x05\x00\x03\x00\x00\x00\x84\x00\x00\x00i\x00\x00\x00_\x00\x00\x00\xc7\x00\x05\x00\x03\x00\x00\x00\x86\x00\x00\x00\x84\x00\x00\x00\x85\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x87\x00\x00\x00\x86\x00\x00\x00\x83\x00\x00\x00R\x00\x06\x00\x7f\x00\x00\x00\x89\x00\x00\x00\x87\x00\x00\x00\x88\x00\x00\x00\x00\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x8b\x00\x00\x00\x87\x00\x00\x00\x8a\x00\x00\x00R\x00\x06\x00\x7f\x00\x00\x00\x8c\x00\x00\x00\x8b\x00\x00\x00\x88\x00\x00\x00\x00\x00\x00\x00u\x00\x04\x00\x02\x00\x00\x00\x8d\x00\x00\x00P\x00\x00\x00u\x00\x04\x00\x02\x00\x00\x00\x8e\x00\x00\x00Q\x00\x00\x00\xf9\x00\x02\x00W\x00\x00\x00\xf8\x00\x02\x00W\x00\x00\x00\xf5\x00\x07\x00\x7f\x00\x00\x00\x90\x00\x00\x00\x8c\x00\x00\x00V\x00\x00\x00\x8f\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00\x7f\x00\x00\x00\x92\x00\x00\x00\x89\x00\x00\x00V\x00\x00\x00\x91\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00\x7f\x00\x00\x00\x94\x00\x00\x00\x82\x00\x00\x00V\x00\x00\x00\x93\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\x97\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\x96\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\x99\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\x98\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\x9b\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\x9a\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\x9d\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\x9c\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\x9f\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\x9e\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xa1\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xa0\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xa3\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xa2\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xa5\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xa4\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xa7\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xa6\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xa9\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xa8\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xab\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xaa\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xad\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xac\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xaf\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xae\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xb1\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xb0\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xb3\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xb2\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00@\x00\x00\x00\xb5\x00\x00\x00\x95\x00\x00\x00V\x00\x00\x00\xb4\x00\x00\x00W\x00\x00\x00\xf5\x00\x07\x00\x03\x00\x00\x00\xb7\x00\x00\x00Y\x00\x00\x00V\x00\x00\x00\xb6\x00\x00\x00W\x00\x00\x00Q\x00\x05\x00\x03\x00\x00\x00\xb8\x00\x00\x00\x94\x00\x00\x00\x00\x00\x00\x00Q\x00\x05\x00\x03\x00\x00\x00\xb9\x00\x00\x00\x94\x00\x00\x00\x01\x00\x00\x009\x00\x11\x00\x1f\x00\x00\x00\xbd\x00\x00\x00!\x00\x00\x00\x8d\x00\x00\x00\xba\x00\x00\x00\xbb\x00\x00\x00\xba\x00\x00\x00\xb8\x00\x00\x00\xb9\x00\x00\x00o\x00\x00\x00o\x00\x00\x00\x8a\x00\x00\x00_\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00Q\x00\x05\x00\x03\x00\x00\x00\xbe\x00\x00\x00\x92\x00\x00\x00\x00\x00\x00\x00Q\x00\x05\x00\x03\x00\x00\x00\xbf\x00\x00\x00\x92\x00\x00\x00\x01\x00\x00\x009\x00\x11\x00/\x00\x00\x00\xc1\x00\x00\x001\x00\x00\x00\x8e\x00\x00\x00\xba\x00\x00\x00\xbb\x00\x00\x00\xba\x00\x00\x00\xbe\x00\x00\x00\xbf\x00\x00\x00o\x00\x00\x00o\x00\x00\x00\x8a\x00\x00\x00_\x00\x00\x00\xbc\x00\x00\x00\xc0\x00\x00\x00Y\x00\x00\x00Q\x00\x05\x00\x03\x00\x00\x00\xc2\x00\x00\x00\x90\x00\x00\x00\x00\x00\x00\x00Q\x00\x05\x00\x03\x00\x00\x00\xc3\x00\x00\x00\x90\x00\x00\x00\x01\x00\x00\x009\x00\x11\x00/\x00\x00\x00\xc4\x00\x00\x001\x00\x00\x00\x8e\x00\x00\x00\xba\x00\x00\x00\xbb\x00\x00\x00\xba\x00\x00\x00\xc2\x00\x00\x00\xc3\x00\x00\x00o\x00\x00\x00o\x00\x00\x00\x8a\x00\x00\x00_\x00\x00\x00\xbc\x00\x00\x00\xc0\x00\x00\x00Y\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xc6\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xc8\x00\x00\x00\xc1\x00\x00\x00\xc7\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xca\x00\x00\x00C\x00\x00\x00\xb5\x00\x00\x00\xc6\x00\x00\x00\xc8\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xcb\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x00 \x00\x00\x00!\x00\x00\x00"\x00\x00\x00#\x00\x00\x00$\x00\x00\x00%\x00\x00\x00&\x00\x00\x00\'\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xcc\x00\x00\x00\xc1\x00\x00\x00\xc7\x00\x00\x00\x08\x00\x00\x00\t\x00\x00\x00\n\x00\x00\x00\x0b\x00\x00\x00\x0c\x00\x00\x00\r\x00\x00\x00\x0e\x00\x00\x00\x0f\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xb4\x00\x00\x00C\x00\x00\x00\xca\x00\x00\x00\xcb\x00\x00\x00\xcc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xce\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x00\x08\x00\x00\x00\t\x00\x00\x00\n\x00\x00\x00\x0b\x00\x00\x00\x0c\x00\x00\x00\r\x00\x00\x00\x0e\x00\x00\x00\x0f\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xcf\x00\x00\x00C\x00\x00\x00\xb3\x00\x00\x00\xce\x00\x00\x00\xc8\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xd0\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x00(\x00\x00\x00)\x00\x00\x00*\x00\x00\x00+\x00\x00\x00,\x00\x00\x00-\x00\x00\x00.\x00\x00\x00/\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xb2\x00\x00\x00C\x00\x00\x00\xcf\x00\x00\x00\xd0\x00\x00\x00\xcc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xd2\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x00\x10\x00\x00\x00\x11\x00\x00\x00\x12\x00\x00\x00\x13\x00\x00\x00\x14\x00\x00\x00\x15\x00\x00\x00\x16\x00\x00\x00\x17\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xd3\x00\x00\x00C\x00\x00\x00\xb1\x00\x00\x00\xd2\x00\x00\x00\xc8\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xd4\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x000\x00\x00\x001\x00\x00\x002\x00\x00\x003\x00\x00\x004\x00\x00\x005\x00\x00\x006\x00\x00\x007\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xb0\x00\x00\x00C\x00\x00\x00\xd3\x00\x00\x00\xd4\x00\x00\x00\xcc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xd6\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x00\x18\x00\x00\x00\x19\x00\x00\x00\x1a\x00\x00\x00\x1b\x00\x00\x00\x1c\x00\x00\x00\x1d\x00\x00\x00\x1e\x00\x00\x00\x1f\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xd7\x00\x00\x00C\x00\x00\x00\xaf\x00\x00\x00\xd6\x00\x00\x00\xc8\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00A\x00\x00\x00\xd8\x00\x00\x00\xbd\x00\x00\x00\xc5\x00\x00\x008\x00\x00\x009\x00\x00\x00:\x00\x00\x00;\x00\x00\x00<\x00\x00\x00=\x00\x00\x00>\x00\x00\x00?\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xae\x00\x00\x00C\x00\x00\x00\xd7\x00\x00\x00\xd8\x00\x00\x00\xcc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xda\x00\x00\x00\xc1\x00\x00\x00\xc7\x00\x00\x00\x10\x00\x00\x00\x11\x00\x00\x00\x12\x00\x00\x00\x13\x00\x00\x00\x14\x00\x00\x00\x15\x00\x00\x00\x16\x00\x00\x00\x17\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xdb\x00\x00\x00C\x00\x00\x00\xad\x00\x00\x00\xc6\x00\x00\x00\xda\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xdc\x00\x00\x00\xc1\x00\x00\x00\xc7\x00\x00\x00\x18\x00\x00\x00\x19\x00\x00\x00\x1a\x00\x00\x00\x1b\x00\x00\x00\x1c\x00\x00\x00\x1d\x00\x00\x00\x1e\x00\x00\x00\x1f\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xac\x00\x00\x00C\x00\x00\x00\xdb\x00\x00\x00\xcb\x00\x00\x00\xdc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xde\x00\x00\x00C\x00\x00\x00\xab\x00\x00\x00\xce\x00\x00\x00\xda\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xaa\x00\x00\x00C\x00\x00\x00\xde\x00\x00\x00\xd0\x00\x00\x00\xdc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xe0\x00\x00\x00C\x00\x00\x00\xa9\x00\x00\x00\xd2\x00\x00\x00\xda\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xa8\x00\x00\x00C\x00\x00\x00\xe0\x00\x00\x00\xd4\x00\x00\x00\xdc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xe2\x00\x00\x00C\x00\x00\x00\xa7\x00\x00\x00\xd6\x00\x00\x00\xda\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xa6\x00\x00\x00C\x00\x00\x00\xe2\x00\x00\x00\xd8\x00\x00\x00\xdc\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xe4\x00\x00\x00\xc4\x00\x00\x00\xc7\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xe5\x00\x00\x00C\x00\x00\x00\xa5\x00\x00\x00\xc6\x00\x00\x00\xe4\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xe6\x00\x00\x00\xc4\x00\x00\x00\xc7\x00\x00\x00\x08\x00\x00\x00\t\x00\x00\x00\n\x00\x00\x00\x0b\x00\x00\x00\x0c\x00\x00\x00\r\x00\x00\x00\x0e\x00\x00\x00\x0f\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xa4\x00\x00\x00C\x00\x00\x00\xe5\x00\x00\x00\xcb\x00\x00\x00\xe6\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xe8\x00\x00\x00C\x00\x00\x00\xa3\x00\x00\x00\xce\x00\x00\x00\xe4\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xa2\x00\x00\x00C\x00\x00\x00\xe8\x00\x00\x00\xd0\x00\x00\x00\xe6\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xea\x00\x00\x00C\x00\x00\x00\xa1\x00\x00\x00\xd2\x00\x00\x00\xe4\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xa0\x00\x00\x00C\x00\x00\x00\xea\x00\x00\x00\xd4\x00\x00\x00\xe6\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xec\x00\x00\x00C\x00\x00\x00\x9f\x00\x00\x00\xd6\x00\x00\x00\xe4\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\x9e\x00\x00\x00C\x00\x00\x00\xec\x00\x00\x00\xd8\x00\x00\x00\xe6\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xee\x00\x00\x00\xc4\x00\x00\x00\xc7\x00\x00\x00\x10\x00\x00\x00\x11\x00\x00\x00\x12\x00\x00\x00\x13\x00\x00\x00\x14\x00\x00\x00\x15\x00\x00\x00\x16\x00\x00\x00\x17\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xef\x00\x00\x00C\x00\x00\x00\x9d\x00\x00\x00\xc6\x00\x00\x00\xee\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00O\x00\r\x00\r\x00\x00\x00\xf0\x00\x00\x00\xc4\x00\x00\x00\xc7\x00\x00\x00\x18\x00\x00\x00\x19\x00\x00\x00\x1a\x00\x00\x00\x1b\x00\x00\x00\x1c\x00\x00\x00\x1d\x00\x00\x00\x1e\x00\x00\x00\x1f\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\x9c\x00\x00\x00C\x00\x00\x00\xef\x00\x00\x00\xcb\x00\x00\x00\xf0\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xf2\x00\x00\x00C\x00\x00\x00\x9b\x00\x00\x00\xce\x00\x00\x00\xee\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\x9a\x00\x00\x00C\x00\x00\x00\xf2\x00\x00\x00\xd0\x00\x00\x00\xf0\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xf4\x00\x00\x00C\x00\x00\x00\x99\x00\x00\x00\xd2\x00\x00\x00\xee\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\x98\x00\x00\x00C\x00\x00\x00\xf4\x00\x00\x00\xd4\x00\x00\x00\xf0\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\xf6\x00\x00\x00C\x00\x00\x00\x97\x00\x00\x00\xd6\x00\x00\x00\xee\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x009\x00\x0c\x00@\x00\x00\x00\x96\x00\x00\x00C\x00\x00\x00\xf6\x00\x00\x00\xd8\x00\x00\x00\xf0\x00\x00\x00\xc9\x00\x00\x00\xc9\x00\x00\x00y\x00\x00\x00y\x00\x00\x00\xbc\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00\xf8\x00\x00\x00\xb8\x00\x00\x00\x8a\x00\x00\x00R\x00\x06\x00\x7f\x00\x00\x00\x93\x00\x00\x00\xf8\x00\x00\x00\x94\x00\x00\x00\x00\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00\xfa\x00\x00\x00\xbf\x00\x00\x00\x8a\x00\x00\x00R\x00\x06\x00\x7f\x00\x00\x00\x91\x00\x00\x00\xfa\x00\x00\x00\x92\x00\x00\x00\x01\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00\xfc\x00\x00\x00\xc3\x00\x00\x00\x8a\x00\x00\x00R\x00\x06\x00\x7f\x00\x00\x00\x8f\x00\x00\x00\xfc\x00\x00\x00\x90\x00\x00\x00\x01\x00\x00\x00\x80\x00\x05\x00\x03\x00\x00\x00\xb6\x00\x00\x00\xb7\x00\x00\x00\x8a\x00\x00\x00\xb0\x00\x05\x00\x0c\x00\x00\x00\x00\x01\x00\x00\xb7\x00\x00\x00\xff\x00\x00\x00\xfa\x00\x04\x00\x00\x01\x00\x00W\x00\x00\x00X\x00\x00\x00\xf8\x00\x02\x00X\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x01\x01\x00\x00~\x00\x00\x00y\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x02\x01\x00\x00~\x00\x00\x00o\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x04\x01\x00\x00~\x00\x00\x00\x03\x01\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x05\x01\x00\x00\x87\x00\x00\x00o\x00\x00\x00\xc5\x00\x05\x00\x03\x00\x00\x00\x07\x01\x00\x00\x87\x00\x00\x00\x06\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x08\x01\x00\x00\xb4\x00\x00\x00u\x00\x04\x00\x02\x00\x00\x00\t\x01\x00\x00R\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x0b\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x87\x00\x00\x00~\x00\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x08\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x0c\x01\x00\x00\xb2\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\r\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x87\x00\x00\x00\x01\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x0c\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x0e\x01\x00\x00\xb0\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x0f\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x87\x00\x00\x00\x02\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x0e\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x10\x01\x00\x00\xae\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x11\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x87\x00\x00\x00\x04\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x10\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x12\x01\x00\x00\xac\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x13\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x05\x01\x00\x00~\x00\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x12\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x14\x01\x00\x00\xaa\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x15\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x05\x01\x00\x00\x01\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x14\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x16\x01\x00\x00\xa8\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x17\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x05\x01\x00\x00\x02\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x16\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x18\x01\x00\x00\xa6\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x19\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x05\x01\x00\x00\x04\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x18\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x1a\x01\x00\x00\xa4\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x1b\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x8b\x00\x00\x00~\x00\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x1a\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x1c\x01\x00\x00\xa2\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x1d\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x8b\x00\x00\x00\x01\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x1c\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00\x1e\x01\x00\x00\xa0\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\x1f\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x8b\x00\x00\x00\x02\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00\x1e\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00 \x01\x00\x00\x9e\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00!\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x8b\x00\x00\x00\x04\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00 \x01\x00\x00|\x00\x04\x00\r\x00\x00\x00"\x01\x00\x00\x9c\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00#\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x07\x01\x00\x00~\x00\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00"\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00$\x01\x00\x00\x9a\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00%\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x07\x01\x00\x00\x01\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00$\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00&\x01\x00\x00\x98\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00\'\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x07\x01\x00\x00\x02\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00&\x01\x00\x00|\x00\x04\x00\r\x00\x00\x00(\x01\x00\x00\x96\x00\x00\x009\x00\x12\x00\x0b\x00\x00\x00)\x01\x00\x00\x0f\x00\x00\x00\t\x01\x00\x00\n\x01\x00\x00\xbb\x00\x00\x00\n\x01\x00\x00\x07\x01\x00\x00\x04\x01\x00\x00\x8a\x00\x00\x00o\x00\x00\x00y\x00\x00\x00\\\x00\x00\x00\xbc\x00\x00\x00\xbc\x00\x00\x00Y\x00\x00\x00(\x01\x00\x00\xfd\x00\x01\x008\x00\x01\x006\x00\x05\x00\x0b\x00\x00\x00*\x01\x00\x00\x00\x00\x00\x00N\x00\x00\x007\x00\x03\x00M\x00\x00\x00+\x01\x00\x007\x00\x03\x00M\x00\x00\x00,\x01\x00\x007\x00\x03\x00M\x00\x00\x00-\x01\x00\x007\x00\x03\x00\x03\x00\x00\x00.\x01\x00\x007\x00\x03\x00\x03\x00\x00\x00/\x01\x00\x007\x00\x03\x00\x03\x00\x00\x000\x01\x00\x00\xf8\x00\x02\x001\x01\x00\x009\x00\n\x00\x0b\x00\x00\x002\x01\x00\x00O\x00\x00\x00+\x01\x00\x00,\x01\x00\x00-\x01\x00\x00.\x01\x00\x00/\x01\x00\x000\x01\x00\x00\xfd\x00\x01\x008\x00\x01\x00'create kernel:matmul_kernel_with_block_pointers_0d1d2d3d4d5d
compiled kernel ptr: 0xf2a64a0
total kernels:1
  kernel:matmul_kernel_with_block_pointers_0d1d2d3d4d5d @0xf2a64a0
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
2024-03-09 05:43:27,650 - numexpr.utils - INFO - Note: detected 224 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-03-09 05:43:27,651 - numexpr.utils - INFO - Note: NumExpr detected 224 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-09 05:43:27,651 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512
threads_per_warp16
num_warps32
global_range_x131072
global_range_y1
global_range_z1
local_range_x512

 Triton and Torch match
matmul-performance:
        M       N       K      oneDNN      Triton
0  4096.0  4096.0  4096.0  185.476433  112.392426

***** first dot info *****
***** chain ops of dotA *****
***** chain ops end *********

***** chain ops of dotB *****
***** chain ops end *********

***** chain ops of dotC *****
***** chain ops end *********
make tensor ptr 
make tensor ptr 
make tensor ptr 
lsc 
lsc 
lsc 
cast 
extract 
cast 
cast 
extract 
cast 
extract 
cast 
extract 
cast 
glue 
extract 
cast 
extract 
cast 
glue 
extract 
cast 
extract 
cast 
glue 
extract 
cast 
extract 
cast 
glue 
extract 
cast 
extract 
cast 
glue 
glue 
glue 
glue 
cast 
extract 
cast 
extract 
cast 
glue 
glue 
glue 
glue 
extract 
cast 
extract 
cast 
glue 
glue 
glue 
glue 
advance 
advance 
advance 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
make tensor ptr 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
lsc 
